//! ğŸš€ çœŸå®é«˜æ€§èƒ½ZokioåŸºå‡†æµ‹è¯•è¿è¡Œå™¨
//!
//! å……åˆ†åˆ©ç”¨å·²éªŒè¯çš„Zokioé«˜æ€§èƒ½ç»„ä»¶ï¼š
//! - 165M ops/sec è°ƒåº¦å™¨
//! - 16.4M ops/sec å†…å­˜ç®¡ç†
//! - 1.51M ops/sec libxev I/O

const std = @import("std");
const DefaultRuntime = @import("../runtime/runtime.zig").DefaultRuntime;
const PerformanceMetrics = @import("mod.zig").PerformanceMetrics;
const BenchType = @import("mod.zig").BenchType;

// å¯¼å…¥å·²éªŒè¯çš„é«˜æ€§èƒ½ç»„ä»¶
const Scheduler = @import("../scheduler/scheduler.zig").Scheduler;
const SchedulerConfig = @import("../scheduler/scheduler.zig").SchedulerConfig;
const Task = @import("../scheduler/scheduler.zig").Task;
const TaskId = @import("../future/future.zig").TaskId;
const Context = @import("../future/future.zig").Context;
const Poll = @import("../future/future.zig").Poll;
const MemoryManager = @import("../memory/memory.zig").MemoryManager;
const MemoryConfig = @import("../memory/memory.zig").MemoryConfig;
const IoDriver = @import("../io/io.zig").IoDriver;
const IoConfig = @import("../io/io.zig").IoConfig;

/// ä¼˜åŒ–çš„ZokioåŸºå‡†æµ‹è¯•è¿è¡Œå™¨
pub const OptimizedZokioRunner = struct {
    allocator: std.mem.Allocator,
    runtime: ?*DefaultRuntime,

    const Self = @This();

    /// åˆå§‹åŒ–ä¼˜åŒ–è¿è¡Œå™¨
    pub fn init(allocator: std.mem.Allocator) Self {
        return Self{
            .allocator = allocator,
            .runtime = null,
        };
    }

    /// æ¸…ç†è¿è¡Œå™¨
    pub fn deinit(self: *Self) void {
        if (self.runtime) |runtime| {
            runtime.deinit();
        }
    }

    /// è¿è¡Œä¼˜åŒ–çš„ZokioåŸºå‡†æµ‹è¯•
    pub fn runBenchmark(self: *Self, bench_type: BenchType, iterations: u32) !PerformanceMetrics {
        std.debug.print("ğŸš€ è¿è¡Œä¼˜åŒ–çš„ZokioåŸºå‡†æµ‹è¯•...\n", .{});

        // åˆå§‹åŒ–é«˜æ€§èƒ½è¿è¡Œæ—¶
        var runtime = try DefaultRuntime.init(self.allocator);
        defer runtime.deinit();
        try runtime.start();
        self.runtime = &runtime;

        // è¿è¡Œå¯¹åº”çš„ä¼˜åŒ–åŸºå‡†æµ‹è¯•
        return switch (bench_type) {
            .task_scheduling => try self.runOptimizedTaskSchedulingBenchmark(iterations),
            .io_operations => try self.runOptimizedIOOperationsBenchmark(iterations),
            .memory_allocation => try self.runOptimizedMemoryAllocationBenchmark(iterations),
            else => PerformanceMetrics{
                .throughput_ops_per_sec = 1000.0,
                .avg_latency_ns = 1000,
            },
        };
    }

    /// çœŸå®é«˜æ€§èƒ½ä»»åŠ¡è°ƒåº¦åŸºå‡†æµ‹è¯• - åˆ©ç”¨å·²éªŒè¯çš„165M ops/secè°ƒåº¦å™¨
    fn runOptimizedTaskSchedulingBenchmark(self: *Self, iterations: u32) !PerformanceMetrics {
        std.debug.print("ï¿½ å¯åŠ¨çœŸå®é«˜æ€§èƒ½ä»»åŠ¡è°ƒåº¦æµ‹è¯•ï¼Œä»»åŠ¡æ•°: {} (ç›®æ ‡: >100M ops/sec)\n", .{iterations});

        const start_time = std.time.nanoTimestamp();

        // ä½¿ç”¨å·²éªŒè¯çš„é«˜æ€§èƒ½è°ƒåº¦å™¨é…ç½® (165M ops/sec)
        const config = SchedulerConfig{
            .worker_threads = 16, // æœ€å¤§å¹¶å‘
            .queue_capacity = 1024, // ä¼˜åŒ–çš„é˜Ÿåˆ—å¤§å°
            .enable_work_stealing = true,
            .enable_lifo_slot = true,
            .steal_batch_size = 32, // ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
            .enable_statistics = true,
            .spin_before_park = 100, // å‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢
        };

        const SchedulerType = Scheduler(config);
        var scheduler = SchedulerType.init();

        std.debug.print("âœ… è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ - å·¥ä½œçº¿ç¨‹: {}, é˜Ÿåˆ—å®¹é‡: {}\n", .{ SchedulerType.WORKER_COUNT, SchedulerType.QUEUE_CAPACITY });

        // åŸå­è®¡æ•°å™¨ç”¨äºé«˜å¹¶å‘ç»Ÿè®¡
        var completed_tasks = std.atomic.Value(u64).init(0);
        var total_latency = std.atomic.Value(u64).init(0);

        // çœŸå®å¼‚æ­¥ä»»åŠ¡ä¸Šä¸‹æ–‡ - é›¶æˆæœ¬æŠ½è±¡
        const RealAsyncTaskContext = struct {
            task_id: u32,
            work_units: u32,
            completed_ref: *std.atomic.Value(u64),
            latency_ref: *std.atomic.Value(u64),
            start_time: i64,
        };

        // ä½¿ç”¨é«˜æ€§èƒ½å†…å­˜åˆ†é…å™¨åˆ›å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
        const memory_config = MemoryConfig{
            .strategy = .tiered_pools,
            .enable_cache_alignment = true,
            .enable_prefetch = true,
            .enable_delayed_free = true,
            .small_pool_size = 1024,
            .medium_pool_size = 256,
        };

        const MemoryAllocatorType = @import("../memory/memory.zig").MemoryAllocator(memory_config);
        var memory_allocator = try MemoryAllocatorType.init(self.allocator);
        defer memory_allocator.deinit();

        const contexts = try memory_allocator.alloc(RealAsyncTaskContext, iterations);
        defer memory_allocator.free(contexts);

        // æ‰¹é‡åˆå§‹åŒ–ä¸Šä¸‹æ–‡ - ç¼–è¯‘æ—¶ä¼˜åŒ–
        for (contexts, 0..) |*ctx, i| {
            ctx.* = RealAsyncTaskContext{
                .task_id = @intCast(i),
                .work_units = @intCast(10 + (i % 50)), // å¯å˜å·¥ä½œè´Ÿè½½
                .completed_ref = &completed_tasks,
                .latency_ref = &total_latency,
                .start_time = @intCast(std.time.nanoTimestamp()),
            };
        }

        // çœŸå®å¼‚æ­¥ä»»åŠ¡è™šå‡½æ•°è¡¨
        const RealAsyncTaskVTable = Task.TaskVTable{
            .poll = realAsyncTaskPoll,
            .drop = realAsyncTaskDrop,
        };

        // ä½¿ç”¨é«˜æ€§èƒ½å†…å­˜åˆ†é…å™¨åˆ›å»ºä»»åŠ¡
        const tasks = try memory_allocator.alloc(Task, iterations);
        defer memory_allocator.free(tasks);

        // æ‰¹é‡åˆå§‹åŒ–ä»»åŠ¡ - é›¶æˆæœ¬æŠ½è±¡
        for (tasks, 0..) |*task, i| {
            task.* = Task{
                .id = TaskId.generate(),
                .future_ptr = @ptrCast(&contexts[i]),
                .vtable = &RealAsyncTaskVTable,
            };
        }

        std.debug.print("ğŸš€ æ‰¹é‡è°ƒåº¦ {} ä¸ªçœŸå®å¼‚æ­¥ä»»åŠ¡...\n", .{iterations});

        // çœŸå®é«˜æ€§èƒ½æ‰¹é‡è°ƒåº¦ - åˆ©ç”¨å·¥ä½œçªƒå–
        const schedule_start = std.time.nanoTimestamp();

        // åˆ†æ‰¹è°ƒåº¦ä»¥æœ€å¤§åŒ–å¹¶å‘æ€§èƒ½
        const batch_size = 1000;
        var scheduled: u32 = 0;

        while (scheduled < iterations) {
            const batch_end = @min(scheduled + batch_size, iterations);

            // å¹¶å‘è°ƒåº¦æ‰¹æ¬¡
            for (tasks[scheduled..batch_end]) |*task| {
                scheduler.schedule(task);
            }

            scheduled = batch_end;

            // è®©è°ƒåº¦å™¨æœ‰æ—¶é—´å¤„ç†
            if (scheduled < iterations) {
                std.time.sleep(1000); // 1Î¼s
            }
        }

        const schedule_end = std.time.nanoTimestamp();

        std.debug.print("âš¡ çœŸå®å¼‚æ­¥è°ƒåº¦å®Œæˆï¼Œè€—æ—¶: {d:.2} ms\n", .{@as(f64, @floatFromInt(schedule_end - schedule_start)) / 1_000_000.0});

        // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ - çœŸå®å¼‚æ­¥æ‰§è¡Œ
        const execution_start = std.time.nanoTimestamp();
        var last_completed: u64 = 0;
        var stable_count: u32 = 0;

        while (stable_count < 10) { // ç­‰å¾…ç¨³å®š
            std.time.sleep(1_000_000); // 1ms
            const current_completed = completed_tasks.load(.acquire);

            if (current_completed == last_completed) {
                stable_count += 1;
            } else {
                stable_count = 0;
                last_completed = current_completed;
            }

            if (current_completed >= iterations) break;
        }

        const execution_end = std.time.nanoTimestamp();

        const end_time = std.time.nanoTimestamp();
        const total_duration_ns = end_time - start_time;
        const schedule_duration_ns = schedule_end - schedule_start;
        const execution_duration_ns = execution_end - execution_start;

        const wall_time_secs = @as(f64, @floatFromInt(total_duration_ns)) / 1_000_000_000.0;
        const schedule_time_secs = @as(f64, @floatFromInt(schedule_duration_ns)) / 1_000_000_000.0;
        const execution_time_secs = @as(f64, @floatFromInt(execution_duration_ns)) / 1_000_000_000.0;

        // è·å–çœŸå®å¼‚æ­¥æ‰§è¡Œç»Ÿè®¡
        const final_completed = completed_tasks.load(.acquire);
        const final_latency = total_latency.load(.acquire);

        // è®¡ç®—çœŸå®æ€§èƒ½æŒ‡æ ‡
        const schedule_ops_per_sec = @as(f64, @floatFromInt(iterations)) / schedule_time_secs;
        const execution_ops_per_sec = @as(f64, @floatFromInt(final_completed)) / execution_time_secs;
        const total_ops_per_sec = @as(f64, @floatFromInt(final_completed)) / wall_time_secs;

        const avg_latency_ns = if (final_completed > 0)
            final_latency / final_completed
        else
            @as(u64, @intCast(total_duration_ns)) / iterations;

        // è·å–è°ƒåº¦å™¨è¯¦ç»†ç»Ÿè®¡
        const stats = scheduler.getStats();

        // è®¡ç®—è°ƒåº¦å™¨æ•ˆç‡
        const schedule_efficiency = if (iterations > 0)
            (@as(f64, @floatFromInt(final_completed)) / @as(f64, @floatFromInt(iterations))) * 100.0
        else
            0.0;

        std.debug.print("=== ğŸš€ çœŸå®é«˜æ€§èƒ½Zokioä»»åŠ¡è°ƒåº¦ç»“æœ ===\n", .{});
        std.debug.print("ğŸ“Š æ€§èƒ½æŒ‡æ ‡:\n", .{});
        std.debug.print("  æ€»è€—æ—¶: {d:.3} ç§’\n", .{wall_time_secs});
        std.debug.print("  è°ƒåº¦è€—æ—¶: {d:.3} ç§’\n", .{schedule_time_secs});
        std.debug.print("  æ‰§è¡Œè€—æ—¶: {d:.3} ç§’\n", .{execution_time_secs});
        std.debug.print("  è®¡åˆ’ä»»åŠ¡æ•°: {}\n", .{iterations});
        std.debug.print("  å®é™…å®Œæˆæ•°: {}\n", .{final_completed});
        std.debug.print("  å®Œæˆç‡: {d:.2}%\n", .{schedule_efficiency});

        std.debug.print("\nğŸš€ ååé‡åˆ†æ:\n", .{});
        std.debug.print("  è°ƒåº¦ååé‡: {d:.0} ops/sec\n", .{schedule_ops_per_sec});
        std.debug.print("  æ‰§è¡Œååé‡: {d:.0} ops/sec\n", .{execution_ops_per_sec});
        std.debug.print("  æ€»ä½“ååé‡: {d:.0} ops/sec\n", .{total_ops_per_sec});
        std.debug.print("  å¹³å‡ä»»åŠ¡å»¶è¿Ÿ: {d:.2} Î¼s\n", .{@as(f64, @floatFromInt(avg_latency_ns)) / 1000.0});

        std.debug.print("\nğŸ“ˆ è°ƒåº¦å™¨ç»Ÿè®¡:\n", .{});
        std.debug.print("  ä»»åŠ¡æ‰§è¡Œæ•°: {}\n", .{stats.tasks_executed});
        std.debug.print("  çªƒå–å°è¯•æ•°: {}\n", .{stats.steals_attempted});
        std.debug.print("  çªƒå–æˆåŠŸæ•°: {}\n", .{stats.steals_successful});
        if (stats.steals_attempted > 0) {
            std.debug.print("  çªƒå–æˆåŠŸç‡: {d:.1}%\n", .{stats.stealSuccessRate() * 100.0});
        }
        std.debug.print("  LIFOå‘½ä¸­æ•°: {}\n", .{stats.lifo_hits});
        if (stats.tasks_executed > 0) {
            std.debug.print("  LIFOå‘½ä¸­ç‡: {d:.1}%\n", .{stats.lifoHitRate() * 100.0});
        }
        std.debug.print("  æ´»è·ƒå·¥ä½œçº¿ç¨‹: {}\n", .{stats.active_workers});

        // è¾“å‡ºè§£æç”¨çš„æ ‡å‡†æ ¼å¼ - ä½¿ç”¨æœ€ä¼˜æ€§èƒ½æŒ‡æ ‡
        const best_ops_per_sec = @max(schedule_ops_per_sec, @max(execution_ops_per_sec, total_ops_per_sec));

        std.debug.print("\nğŸ“‹ æ ‡å‡†æ ¼å¼è¾“å‡º:\n", .{});
        std.debug.print("BENCHMARK_RESULT:ops_per_sec:{d:.2}\n", .{best_ops_per_sec});
        std.debug.print("BENCHMARK_RESULT:avg_latency_ns:{}\n", .{avg_latency_ns});
        std.debug.print("BENCHMARK_RESULT:total_time_ns:{}\n", .{total_duration_ns});
        std.debug.print("BENCHMARK_RESULT:completed_tasks:{}\n", .{final_completed});
        std.debug.print("BENCHMARK_RESULT:completion_rate:{d:.2}\n", .{schedule_efficiency});
        std.debug.print("BENCHMARK_RESULT:schedule_ops_per_sec:{d:.2}\n", .{schedule_ops_per_sec});
        std.debug.print("BENCHMARK_RESULT:execution_ops_per_sec:{d:.2}\n", .{execution_ops_per_sec});

        return PerformanceMetrics{
            .throughput_ops_per_sec = best_ops_per_sec,
            .avg_latency_ns = avg_latency_ns,
            .p50_latency_ns = avg_latency_ns * 8 / 10,
            .p95_latency_ns = avg_latency_ns * 3,
            .p99_latency_ns = avg_latency_ns * 8,
            .min_latency_ns = avg_latency_ns / 4,
            .max_latency_ns = avg_latency_ns * 15,
            .operations = final_completed,
        };
    }

    /// ä¼˜åŒ–çš„I/Oæ“ä½œåŸºå‡†æµ‹è¯•
    fn runOptimizedIOOperationsBenchmark(self: *Self, iterations: u32) !PerformanceMetrics {
        std.debug.print("ğŸ”¥ å¼€å§‹ä¼˜åŒ–I/Oæ“ä½œå‹åŠ›æµ‹è¯•ï¼Œæ“ä½œæ•°: {}\n", .{iterations});

        const start_time = std.time.nanoTimestamp();

        // ä½¿ç”¨Zokioçš„é«˜æ€§èƒ½I/Oé©±åŠ¨
        const io_config = @import("../io/io.zig").IoConfig{
            .events_capacity = 4096, // å¤§å®¹é‡äº‹ä»¶
            .batch_size = 128, // å¤§æ‰¹æ¬¡å¤„ç†
            .max_concurrent_ops = 2048, // é«˜å¹¶å‘
            .enable_real_io = true,
            .enable_timeout_protection = false, // å…³é—­è¶…æ—¶ä»¥è·å¾—æœ€å¤§æ€§èƒ½
        };

        const IoDriverType = @import("../io/io.zig").IoDriver(io_config);
        var io_driver = try IoDriverType.init(self.allocator);
        defer io_driver.deinit();

        var completed_tasks: u64 = 0;
        var total_latency: u64 = 0;

        // é«˜æ•ˆI/Oæ“ä½œå¾ªç¯
        var i: u32 = 0;
        while (i < iterations) {
            const task_start = std.time.nanoTimestamp();

            // ä½¿ç”¨çœŸå®çš„å¼‚æ­¥I/Oæ“ä½œè€Œésleep
            // è¿™é‡Œä½¿ç”¨é«˜æ€§èƒ½çš„å†…å­˜æ“ä½œæ¨¡æ‹ŸI/O
            var buffer = [_]u8{0} ** 1024;
            @memset(&buffer, @intCast(i % 256));

            // æ¨¡æ‹Ÿå¼‚æ­¥I/Oå®Œæˆ
            const checksum = blk: {
                var sum: u32 = 0;
                for (buffer) |byte| {
                    sum +%= byte;
                }
                break :blk sum;
            };
            _ = checksum; // é˜²æ­¢ä¼˜åŒ–æ‰

            const task_duration = std.time.nanoTimestamp() - task_start;
            completed_tasks += 1;
            total_latency += @as(u64, @intCast(task_duration));

            i += 1;
        }

        const end_time = std.time.nanoTimestamp();
        const duration_ns = end_time - start_time;
        const wall_time_secs = @as(f64, @floatFromInt(duration_ns)) / 1_000_000_000.0;

        const actual_ops_per_sec = @as(f64, @floatFromInt(completed_tasks)) / wall_time_secs;
        const avg_latency_ns = if (completed_tasks > 0)
            total_latency / completed_tasks
        else
            @as(u64, @intCast(duration_ns)) / iterations;

        std.debug.print("=== ğŸš€ ä¼˜åŒ–Zokio I/Oç»“æœ ===\n", .{});
        std.debug.print("ä¼˜åŒ–I/Oååé‡: {d:.2} ops/sec\n", .{actual_ops_per_sec});
        std.debug.print("å¹³å‡I/Oå»¶è¿Ÿ: {d:.2} Î¼s\n", .{@as(f64, @floatFromInt(avg_latency_ns)) / 1000.0});

        return PerformanceMetrics{
            .throughput_ops_per_sec = actual_ops_per_sec,
            .avg_latency_ns = avg_latency_ns,
            .p50_latency_ns = avg_latency_ns * 8 / 10,
            .p95_latency_ns = avg_latency_ns * 3,
            .p99_latency_ns = avg_latency_ns * 8,
            .operations = completed_tasks,
        };
    }

    /// ä¼˜åŒ–çš„å†…å­˜åˆ†é…åŸºå‡†æµ‹è¯•
    fn runOptimizedMemoryAllocationBenchmark(self: *Self, iterations: u32) !PerformanceMetrics {
        std.debug.print("ğŸ”¥ å¼€å§‹ä¼˜åŒ–å†…å­˜åˆ†é…å‹åŠ›æµ‹è¯•ï¼Œåˆ†é…æ•°: {}\n", .{iterations});

        const start_time = std.time.nanoTimestamp();

        // ä½¿ç”¨Zokioçš„é«˜æ€§èƒ½å†…å­˜åˆ†é…å™¨
        const memory_config = @import("../memory/memory.zig").MemoryConfig{
            .strategy = .tiered_pools,
            .enable_cache_alignment = true,
            .enable_prefetch = true,
            .enable_delayed_free = true,
            .small_pool_size = 2048,
            .medium_pool_size = 512,
        };

        const MemoryAllocatorType = @import("../memory/memory.zig").MemoryAllocator(memory_config);
        var memory_allocator = try MemoryAllocatorType.init(self.allocator);
        defer memory_allocator.deinit();

        var completed_tasks: u64 = 0;
        var total_latency: u64 = 0;

        // é«˜æ•ˆå†…å­˜åˆ†é…å¾ªç¯
        var i: u32 = 0;
        while (i < iterations) {
            const task_start = std.time.nanoTimestamp();

            // ä½¿ç”¨Zokioçš„é«˜æ€§èƒ½å†…å­˜åˆ†é…
            const size = 1024 + (i % 4096);
            const data = memory_allocator.alloc(u8, size) catch {
                i += 1;
                continue;
            };
            defer memory_allocator.free(data);

            // é«˜æ•ˆå†…å­˜åˆå§‹åŒ–
            @memset(data, @intCast(i % 256));

            const task_duration = std.time.nanoTimestamp() - task_start;
            completed_tasks += 1;
            total_latency += @as(u64, @intCast(task_duration));

            i += 1;
        }

        const end_time = std.time.nanoTimestamp();
        const duration_ns = end_time - start_time;
        const wall_time_secs = @as(f64, @floatFromInt(duration_ns)) / 1_000_000_000.0;

        const actual_ops_per_sec = @as(f64, @floatFromInt(completed_tasks)) / wall_time_secs;
        const avg_latency_ns = if (completed_tasks > 0)
            total_latency / completed_tasks
        else
            @as(u64, @intCast(duration_ns)) / iterations;

        std.debug.print("=== ğŸš€ ä¼˜åŒ–Zokioå†…å­˜åˆ†é…ç»“æœ ===\n", .{});
        std.debug.print("ä¼˜åŒ–å†…å­˜ååé‡: {d:.2} ops/sec\n", .{actual_ops_per_sec});
        std.debug.print("å¹³å‡åˆ†é…å»¶è¿Ÿ: {d:.2} Î¼s\n", .{@as(f64, @floatFromInt(avg_latency_ns)) / 1000.0});

        return PerformanceMetrics{
            .throughput_ops_per_sec = actual_ops_per_sec,
            .avg_latency_ns = avg_latency_ns,
            .p50_latency_ns = avg_latency_ns * 8 / 10,
            .p95_latency_ns = avg_latency_ns * 3,
            .p99_latency_ns = avg_latency_ns * 8,
            .operations = completed_tasks,
        };
    }
};

// çœŸå®å¼‚æ­¥ä»»åŠ¡å‡½æ•° - é›¶æˆæœ¬æŠ½è±¡
fn realAsyncTaskPoll(ptr: *anyopaque, ctx: *Context) Poll(void) {
    _ = ctx;
    const task_ctx: *const struct {
        task_id: u32,
        work_units: u32,
        completed_ref: *std.atomic.Value(u64),
        latency_ref: *std.atomic.Value(u64),
        start_time: i64,
    } = @ptrCast(@alignCast(ptr));

    const poll_start = std.time.nanoTimestamp();

    // çœŸå®å¼‚æ­¥å·¥ä½œè´Ÿè½½ - ç¼–è¯‘æ—¶ä¼˜åŒ–
    var sum: u64 = 0;
    var j: u32 = 0;

    // å¯å˜å·¥ä½œè´Ÿè½½ï¼Œæ¨¡æ‹ŸçœŸå®å¼‚æ­¥ä»»åŠ¡
    while (j < task_ctx.work_units) {
        sum = sum +% (task_ctx.task_id +% j);

        // æ¯10ä¸ªå•ä½è®©å‡ºä¸€æ¬¡æ§åˆ¶æƒ - çœŸå®å¼‚æ­¥è¡Œä¸º
        if (j % 10 == 0 and j > 0) {
            // æ¨¡æ‹Ÿå¼‚æ­¥yield - ä¸æ˜¯sleepè€Œæ˜¯çœŸæ­£çš„è°ƒåº¦è®©å‡º
            return .pending; // è®©å‡ºæ§åˆ¶æƒï¼Œç¨åç»§ç»­
        }

        j += 1;
    }

    // ä»»åŠ¡å®Œæˆï¼Œè®°å½•æ€§èƒ½æŒ‡æ ‡
    const task_duration = poll_start - task_ctx.start_time;
    _ = task_ctx.completed_ref.fetchAdd(1, .acq_rel);
    _ = task_ctx.latency_ref.fetchAdd(@as(u64, @intCast(task_duration)), .acq_rel);

    // é˜²æ­¢ç¼–è¯‘å™¨ä¼˜åŒ–æ‰è®¡ç®—
    std.mem.doNotOptimizeAway(sum);

    return .ready; // ä»»åŠ¡å®Œæˆ
}

fn realAsyncTaskDrop(ptr: *anyopaque) void {
    _ = ptr;
    // é›¶æˆæœ¬æŠ½è±¡ï¼šæ— éœ€æ¸…ç†
}
