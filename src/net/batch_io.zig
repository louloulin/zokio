//! ğŸš€ Zokio æ‰¹é‡ç½‘ç»œI/Oä¼˜åŒ–
//!
//! å……åˆ†åˆ©ç”¨libxevçš„æ‰¹é‡å¤„ç†èƒ½åŠ›ï¼Œå®ç°é«˜æ€§èƒ½ç½‘ç»œI/Oï¼š
//! 1. æ‰¹é‡acceptè¿æ¥
//! 2. æ‰¹é‡read/writeæ“ä½œ
//! 3. æ™ºèƒ½ç¼“å†²åŒºç®¡ç†
//! 4. è¿æ¥æ± ä¼˜åŒ–

const std = @import("std");
const libxev = @import("libxev");
const utils = @import("../utils/utils.zig");
const CompletionBridge = @import("../runtime/completion_bridge.zig").CompletionBridge;

/// ğŸ”§ æ‰¹é‡I/Oé…ç½®
pub const BatchIoConfig = struct {
    /// æ‰¹é‡å¤§å°
    batch_size: u32 = 128,

    /// æœ€å¤§å¹¶å‘è¿æ¥æ•°
    max_concurrent_connections: u32 = 10000,

    /// ç¼“å†²åŒºå¤§å°
    buffer_size: u32 = 64 * 1024, // 64KB

    /// ç¼“å†²åŒºæ± å¤§å°
    buffer_pool_size: u32 = 1024,

    /// å¯ç”¨Nagleç®—æ³•ä¼˜åŒ–
    enable_nagle_optimization: bool = true,

    /// å¯ç”¨é›¶æ‹·è´
    enable_zero_copy: bool = true,

    /// æ‰¹é‡è¶…æ—¶ï¼ˆå¾®ç§’ï¼‰
    batch_timeout_us: u32 = 1000, // 1ms
};

/// ğŸš€ æ‰¹é‡I/Oç®¡ç†å™¨
pub const BatchIoManager = struct {
    const Self = @This();

    /// é…ç½®
    config: BatchIoConfig,

    /// å†…å­˜åˆ†é…å™¨
    allocator: std.mem.Allocator,

    /// ç¼“å†²åŒºæ± 
    buffer_pool: BufferPool,

    /// æ‰¹é‡æ“ä½œé˜Ÿåˆ—
    batch_queue: BatchQueue,

    /// è¿æ¥ç®¡ç†å™¨
    connection_manager: ConnectionManager,

    /// ç»Ÿè®¡ä¿¡æ¯
    stats: BatchIoStats,

    /// åˆå§‹åŒ–æ‰¹é‡I/Oç®¡ç†å™¨
    pub fn init(allocator: std.mem.Allocator, config: BatchIoConfig) !Self {
        return Self{
            .config = config,
            .allocator = allocator,
            .buffer_pool = try BufferPool.init(allocator, config.buffer_pool_size, config.buffer_size),
            .batch_queue = try BatchQueue.init(allocator, config.batch_size),
            .connection_manager = try ConnectionManager.init(allocator, config.max_concurrent_connections),
            .stats = BatchIoStats{},
        };
    }

    /// æ¸…ç†èµ„æº
    pub fn deinit(self: *Self) void {
        self.buffer_pool.deinit();
        self.batch_queue.deinit();
        self.connection_manager.deinit();
    }

    /// ğŸš€ æ‰¹é‡acceptè¿æ¥
    pub fn batchAccept(
        self: *Self,
        loop: *libxev.Loop,
        listener_fd: std.posix.fd_t,
        max_accepts: u32,
    ) ![]AcceptResult {
        const start_time = std.time.nanoTimestamp();

        // å‡†å¤‡æ‰¹é‡acceptæ“ä½œ
        const accept_count = @min(max_accepts, self.config.batch_size);
        const results = try self.allocator.alloc(AcceptResult, accept_count);
        errdefer self.allocator.free(results);

        const bridges = try self.allocator.alloc(CompletionBridge, accept_count);
        defer self.allocator.free(bridges);

        // æäº¤æ‰¹é‡acceptæ“ä½œ
        for (bridges, 0..) |*bridge, i| {
            bridge.* = CompletionBridge.init();
            try bridge.submitAccept(loop, listener_fd);
            results[i] = AcceptResult{ .bridge = bridge, .status = .pending };
        }

        // ç­‰å¾…å®Œæˆ
        var completed_count: u32 = 0;
        const timeout_ns = self.config.batch_timeout_us * 1000;

        while (completed_count < accept_count) {
            const current_time = std.time.nanoTimestamp();
            if (current_time - start_time > timeout_ns) break;

            // æ£€æŸ¥å®ŒæˆçŠ¶æ€
            for (results, 0..) |*result, i| {
                if (result.status == .pending) {
                    const bridge = bridges[i];
                    if (bridge.isReady()) {
                        result.status = .completed;
                        result.client_fd = bridge.getAcceptResult() catch 0;
                        completed_count += 1;
                    }
                }
            }

            // çŸ­æš‚ä¼‘çœ é¿å…å¿™ç­‰å¾…
            std.time.sleep(1000); // 1Î¼s
        }

        const end_time = std.time.nanoTimestamp();
        self.stats.updateBatchAccept(completed_count, end_time - start_time);

        return results;
    }

    /// ğŸš€ æ‰¹é‡è¯»å–æ“ä½œ
    pub fn batchRead(
        self: *Self,
        loop: *libxev.Loop,
        connections: []ConnectionHandle,
    ) ![]ReadResult {
        const start_time = std.time.nanoTimestamp();

        const results = try self.allocator.alloc(ReadResult, connections.len);
        errdefer self.allocator.free(results);

        const bridges = try self.allocator.alloc(CompletionBridge, connections.len);
        defer self.allocator.free(bridges);

        // æäº¤æ‰¹é‡è¯»å–æ“ä½œ
        for (connections, bridges, results) |conn, *bridge, *result| {
            const buffer = try self.buffer_pool.acquire();

            bridge.* = CompletionBridge.init();
            try bridge.submitRead(loop, conn.fd, buffer, null);

            result.* = ReadResult{
                .bridge = bridge,
                .buffer = buffer,
                .status = .pending,
                .connection_id = conn.id,
            };
        }

        // ç­‰å¾…å®Œæˆ
        var completed_count: u32 = 0;
        const timeout_ns = self.config.batch_timeout_us * 1000;

        while (completed_count < connections.len) {
            const current_time = std.time.nanoTimestamp();
            if (current_time - start_time > timeout_ns) break;

            for (results, bridges) |*result, bridge| {
                if (result.status == .pending and bridge.isReady()) {
                    result.status = .completed;
                    result.bytes_read = bridge.getReadResult() catch 0;
                    completed_count += 1;
                }
            }

            std.time.sleep(1000); // 1Î¼s
        }

        const end_time = std.time.nanoTimestamp();
        self.stats.updateBatchRead(completed_count, end_time - start_time);

        return results;
    }

    /// ğŸš€ æ‰¹é‡å†™å…¥æ“ä½œ
    pub fn batchWrite(
        self: *Self,
        loop: *libxev.Loop,
        write_requests: []WriteRequest,
    ) ![]WriteResult {
        const start_time = std.time.nanoTimestamp();

        const results = try self.allocator.alloc(WriteResult, write_requests.len);
        errdefer self.allocator.free(results);

        const bridges = try self.allocator.alloc(CompletionBridge, write_requests.len);
        defer self.allocator.free(bridges);

        // æäº¤æ‰¹é‡å†™å…¥æ“ä½œ
        for (write_requests, bridges, results) |req, *bridge, *result| {
            bridge.* = CompletionBridge.init();
            try bridge.submitWrite(loop, req.fd, req.data, null);

            result.* = WriteResult{
                .bridge = bridge,
                .status = .pending,
                .connection_id = req.connection_id,
            };
        }

        // ç­‰å¾…å®Œæˆ
        var completed_count: u32 = 0;
        const timeout_ns = self.config.batch_timeout_us * 1000;

        while (completed_count < write_requests.len) {
            const current_time = std.time.nanoTimestamp();
            if (current_time - start_time > timeout_ns) break;

            for (results, bridges) |*result, bridge| {
                if (result.status == .pending and bridge.isReady()) {
                    result.status = .completed;
                    result.bytes_written = bridge.getWriteResult() catch 0;
                    completed_count += 1;
                }
            }

            std.time.sleep(1000); // 1Î¼s
        }

        const end_time = std.time.nanoTimestamp();
        self.stats.updateBatchWrite(completed_count, end_time - start_time);

        return results;
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn getStats(self: *const Self) BatchIoStats {
        return self.stats;
    }
};

/// ğŸ”„ ç¼“å†²åŒºæ± 
const BufferPool = struct {
    buffers: std.ArrayList([]u8),
    available: std.ArrayList(usize),
    buffer_size: u32,
    allocator: std.mem.Allocator,

    pub fn init(allocator: std.mem.Allocator, pool_size: u32, buffer_size: u32) !BufferPool {
        var pool = BufferPool{
            .buffers = std.ArrayList([]u8).init(allocator),
            .available = std.ArrayList(usize).init(allocator),
            .buffer_size = buffer_size,
            .allocator = allocator,
        };

        for (0..pool_size) |i| {
            const buffer = try allocator.alloc(u8, buffer_size);
            try pool.buffers.append(buffer);
            try pool.available.append(i);
        }

        return pool;
    }

    pub fn deinit(self: *BufferPool) void {
        for (self.buffers.items) |buffer| {
            self.allocator.free(buffer);
        }
        self.buffers.deinit();
        self.available.deinit();
    }

    pub fn acquire(self: *BufferPool) ![]u8 {
        if (self.available.items.len == 0) {
            return error.NoBuffersAvailable;
        }

        const index = self.available.swapRemove(self.available.items.len - 1);
        return self.buffers.items[index];
    }

    pub fn release(self: *BufferPool, buffer: []u8) void {
        for (self.buffers.items, 0..) |pool_buffer, i| {
            if (pool_buffer.ptr == buffer.ptr) {
                self.available.append(i) catch {};
                return;
            }
        }
    }
};

/// ğŸ“¦ æ‰¹é‡æ“ä½œé˜Ÿåˆ—
const BatchQueue = struct {
    operations: std.ArrayList(BatchOperation),
    max_size: u32,

    pub fn init(allocator: std.mem.Allocator, max_size: u32) !BatchQueue {
        return BatchQueue{
            .operations = std.ArrayList(BatchOperation).init(allocator),
            .max_size = max_size,
        };
    }

    pub fn deinit(self: *BatchQueue) void {
        self.operations.deinit();
    }
};

/// ğŸ”— è¿æ¥ç®¡ç†å™¨
const ConnectionManager = struct {
    connections: std.ArrayList(ConnectionHandle),
    max_connections: u32,
    next_id: u64,

    pub fn init(allocator: std.mem.Allocator, max_connections: u32) !ConnectionManager {
        return ConnectionManager{
            .connections = std.ArrayList(ConnectionHandle).init(allocator),
            .max_connections = max_connections,
            .next_id = 1,
        };
    }

    pub fn deinit(self: *ConnectionManager) void {
        self.connections.deinit();
    }
};

/// ğŸ“Š æ‰¹é‡I/Oç»Ÿè®¡ä¿¡æ¯
pub const BatchIoStats = struct {
    /// æ‰¹é‡acceptæ“ä½œæ¬¡æ•°
    batch_accept_operations: u64 = 0,

    /// æ‰¹é‡acceptæ€»è¿æ¥æ•°
    batch_accept_connections: u64 = 0,

    /// æ‰¹é‡acceptæ€»è€—æ—¶ï¼ˆçº³ç§’ï¼‰
    batch_accept_total_time_ns: u64 = 0,

    /// æ‰¹é‡è¯»å–æ“ä½œæ¬¡æ•°
    batch_read_operations: u64 = 0,

    /// æ‰¹é‡è¯»å–æ€»å­—èŠ‚æ•°
    batch_read_bytes: u64 = 0,

    /// æ‰¹é‡è¯»å–æ€»è€—æ—¶ï¼ˆçº³ç§’ï¼‰
    batch_read_total_time_ns: u64 = 0,

    /// æ‰¹é‡å†™å…¥æ“ä½œæ¬¡æ•°
    batch_write_operations: u64 = 0,

    /// æ‰¹é‡å†™å…¥æ€»å­—èŠ‚æ•°
    batch_write_bytes: u64 = 0,

    /// æ‰¹é‡å†™å…¥æ€»è€—æ—¶ï¼ˆçº³ç§’ï¼‰
    batch_write_total_time_ns: u64 = 0,

    pub fn updateBatchAccept(self: *BatchIoStats, connections: u32, time_ns: u64) void {
        self.batch_accept_operations += 1;
        self.batch_accept_connections += connections;
        self.batch_accept_total_time_ns += time_ns;
    }

    pub fn updateBatchRead(self: *BatchIoStats, operations: u32, time_ns: u64) void {
        self.batch_read_operations += 1;
        self.batch_read_total_time_ns += time_ns;
        _ = operations;
    }

    pub fn updateBatchWrite(self: *BatchIoStats, operations: u32, time_ns: u64) void {
        self.batch_write_operations += 1;
        self.batch_write_total_time_ns += time_ns;
        _ = operations;
    }
};

/// ğŸ¯ æ•°æ®ç»“æ„å®šä¹‰
pub const AcceptResult = struct {
    bridge: *CompletionBridge,
    status: OperationStatus,
    client_fd: std.posix.fd_t = 0,
};

pub const ReadResult = struct {
    bridge: *CompletionBridge,
    buffer: []u8,
    status: OperationStatus,
    connection_id: u64,
    bytes_read: usize = 0,
};

pub const WriteResult = struct {
    bridge: *CompletionBridge,
    status: OperationStatus,
    connection_id: u64,
    bytes_written: usize = 0,
};

pub const WriteRequest = struct {
    fd: std.posix.fd_t,
    data: []const u8,
    connection_id: u64,
};

pub const ConnectionHandle = struct {
    id: u64,
    fd: std.posix.fd_t,
};

pub const BatchOperation = struct {
    op_type: enum { accept, read, write },
    fd: std.posix.fd_t,
    data: ?[]u8 = null,
};

pub const OperationStatus = enum {
    pending,
    completed,
    timeout,
    error_occurred,
};
