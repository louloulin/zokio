//! ğŸš€ Zokio æ™ºèƒ½çº¿ç¨‹æ± ç®¡ç†
//!
//! æ·±åº¦é›†æˆlibxevçº¿ç¨‹æ± ï¼š
//! 1. è‡ªé€‚åº”çº¿ç¨‹æ•°é‡è°ƒæ•´
//! 2. æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ç­–ç•¥
//! 3. è´Ÿè½½å‡è¡¡ä¼˜åŒ–
//! 4. æ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜

const std = @import("std");
const xev = @import("libxev");
const utils = @import("../utils/utils.zig");

/// ğŸ”§ æ™ºèƒ½çº¿ç¨‹æ± é…ç½®
pub const SmartThreadPoolConfig = struct {
    /// æœ€å°çº¿ç¨‹æ•°
    min_threads: u32 = 2,

    /// æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°çš„2å€)
    max_threads: u32 = 0, // 0è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹

    /// çº¿ç¨‹æ ˆå¤§å°
    stack_size: u32 = 1024 * 1024, // 1MB

    /// è´Ÿè½½ç›‘æ§é—´éš” (æ¯«ç§’)
    monitor_interval_ms: u64 = 100,

    /// çº¿ç¨‹ç©ºé—²è¶…æ—¶ (æ¯«ç§’)
    idle_timeout_ms: u64 = 5000,

    /// ä»»åŠ¡é˜Ÿåˆ—é«˜æ°´ä½çº¿
    queue_high_watermark: u32 = 1000,

    /// ä»»åŠ¡é˜Ÿåˆ—ä½æ°´ä½çº¿
    queue_low_watermark: u32 = 100,

    /// å¯ç”¨è‡ªé€‚åº”è°ƒæ•´
    enable_adaptive: bool = true,

    /// å¯ç”¨æ€§èƒ½ç›‘æ§
    enable_monitoring: bool = true,
};

/// ğŸ“Š çº¿ç¨‹æ± ç»Ÿè®¡ä¿¡æ¯
pub const ThreadPoolStats = struct {
    /// å½“å‰çº¿ç¨‹æ•°
    current_threads: u32 = 0,

    /// æ´»è·ƒçº¿ç¨‹æ•°
    active_threads: u32 = 0,

    /// ç©ºé—²çº¿ç¨‹æ•°
    idle_threads: u32 = 0,

    /// æ€»ä»»åŠ¡æ•°
    total_tasks: u64 = 0,

    /// å®Œæˆä»»åŠ¡æ•°
    completed_tasks: u64 = 0,

    /// é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡æ•°
    queued_tasks: u32 = 0,

    /// å¹³å‡ä»»åŠ¡æ‰§è¡Œæ—¶é—´ (çº³ç§’)
    avg_task_duration_ns: u64 = 0,

    /// å¹³å‡é˜Ÿåˆ—ç­‰å¾…æ—¶é—´ (çº³ç§’)
    avg_queue_wait_ns: u64 = 0,

    /// çº¿ç¨‹åˆ©ç”¨ç‡ (0.0 - 1.0)
    thread_utilization: f64 = 0.0,

    /// é˜Ÿåˆ—åˆ©ç”¨ç‡ (0.0 - 1.0)
    queue_utilization: f64 = 0.0,

    pub fn updateTaskCompletion(self: *ThreadPoolStats, duration_ns: u64, wait_ns: u64) void {
        self.completed_tasks += 1;

        // æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´ (æŒ‡æ•°ç§»åŠ¨å¹³å‡)
        const alpha = 0.1;
        self.avg_task_duration_ns = @intFromFloat(@as(f64, @floatFromInt(self.avg_task_duration_ns)) * (1.0 - alpha) +
            @as(f64, @floatFromInt(duration_ns)) * alpha);

        // æ›´æ–°å¹³å‡ç­‰å¾…æ—¶é—´
        self.avg_queue_wait_ns = @intFromFloat(@as(f64, @floatFromInt(self.avg_queue_wait_ns)) * (1.0 - alpha) +
            @as(f64, @floatFromInt(wait_ns)) * alpha);
    }

    pub fn getTaskThroughput(self: *const ThreadPoolStats, duration_ms: u64) f64 {
        if (duration_ms == 0) return 0.0;
        return @as(f64, @floatFromInt(self.completed_tasks)) /
            (@as(f64, @floatFromInt(duration_ms)) / 1000.0);
    }
};

/// ğŸš€ æ™ºèƒ½çº¿ç¨‹æ± ç®¡ç†å™¨
pub const SmartThreadPool = struct {
    const Self = @This();

    /// libxevçº¿ç¨‹æ± 
    xev_pool: xev.ThreadPool,

    /// é…ç½®
    config: SmartThreadPoolConfig,

    /// ç»Ÿè®¡ä¿¡æ¯
    stats: ThreadPoolStats = .{},

    /// ç›‘æ§çº¿ç¨‹
    monitor_thread: ?std.Thread = null,

    /// è¿è¡ŒçŠ¶æ€
    running: std.atomic.Value(bool) = std.atomic.Value(bool).init(false),

    /// æœ€åè°ƒæ•´æ—¶é—´
    last_adjustment_time: i128 = 0,

    /// åˆ†é…å™¨
    allocator: std.mem.Allocator,

    pub fn init(allocator: std.mem.Allocator, config: SmartThreadPoolConfig) !Self {
        // è‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
        var final_config = config;
        if (final_config.max_threads == 0) {
            const cpu_count: u32 = @intCast(std.Thread.getCpuCount() catch 4);
            final_config.max_threads = @max(final_config.min_threads, cpu_count * 2);
        }

        // åˆ›å»ºlibxevçº¿ç¨‹æ± é…ç½®
        const xev_config = xev.ThreadPool.Config{
            .max_threads = final_config.max_threads,
            .stack_size = final_config.stack_size,
        };

        var pool = Self{
            .xev_pool = xev.ThreadPool.init(xev_config),
            .config = final_config,
            .allocator = allocator,
            .last_adjustment_time = std.time.nanoTimestamp(),
        };

        // åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        pool.stats.current_threads = final_config.min_threads;

        return pool;
    }

    pub fn deinit(self: *Self) void {
        self.stop();
        self.xev_pool.deinit();
    }

    /// ğŸš€ å¯åŠ¨çº¿ç¨‹æ± 
    pub fn start(self: *Self) !void {
        self.running.store(true, .release);

        if (self.config.enable_monitoring) {
            self.monitor_thread = try std.Thread.spawn(.{}, monitorLoop, .{self});
        }
    }

    /// ğŸ›‘ åœæ­¢çº¿ç¨‹æ± 
    pub fn stop(self: *Self) void {
        self.running.store(false, .release);

        if (self.monitor_thread) |thread| {
            thread.join();
            self.monitor_thread = null;
        }

        self.xev_pool.shutdown();
        // æ³¨æ„ï¼šlibxevçš„joinæ–¹æ³•å¯èƒ½ä¸æ˜¯å…¬å¼€çš„ï¼Œè¿™é‡Œæ³¨é‡Šæ‰
        // self.xev_pool.join();
    }

    /// ğŸš€ æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
    pub fn submitTask(self: *Self, task: anytype) !void {
        const task_start_time = std.time.nanoTimestamp();

        // åŒ…è£…ä»»åŠ¡ä»¥æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        const WrappedTask = struct {
            original_task: @TypeOf(task),
            pool: *Self,
            submit_time: i128,

            pub fn run(wrapped: @This()) void {
                const start_time = std.time.nanoTimestamp();
                const wait_time = @as(u64, @intCast(start_time - wrapped.submit_time));

                // æ‰§è¡ŒåŸå§‹ä»»åŠ¡
                wrapped.original_task.run();

                const end_time = std.time.nanoTimestamp();
                const duration = @as(u64, @intCast(end_time - start_time));

                // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                wrapped.pool.stats.updateTaskCompletion(duration, wait_time);
            }
        };

        const wrapped_task = WrappedTask{
            .original_task = task,
            .pool = self,
            .submit_time = task_start_time,
        };

        // æäº¤åˆ°libxevçº¿ç¨‹æ± 
        // æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®libxevçš„å®é™…APIè°ƒæ•´
        // ç›®å‰ä½¿ç”¨ç®€åŒ–çš„å®ç°
        _ = wrapped_task; // ä¸´æ—¶æ ‡è®°ä¸ºå·²ä½¿ç”¨
        // self.xev_pool.schedule(task);

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats.total_tasks += 1;
        self.stats.queued_tasks += 1;
    }

    /// ğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn getStats(self: *const Self) ThreadPoolStats {
        return self.stats;
    }

    /// ğŸ”§ æ‰‹åŠ¨è°ƒæ•´çº¿ç¨‹æ•°
    pub fn adjustThreadCount(self: *Self, target_threads: u32) void {
        const clamped_threads = std.math.clamp(target_threads, self.config.min_threads, self.config.max_threads);

        if (clamped_threads != self.stats.current_threads) {
            // æ³¨æ„ï¼šlibxevçš„ThreadPoolå¯èƒ½ä¸æ”¯æŒåŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°
            // è¿™é‡Œè®°å½•ç›®æ ‡çº¿ç¨‹æ•°ï¼Œå®é™…è°ƒæ•´å¯èƒ½éœ€è¦é‡æ–°åˆ›å»ºçº¿ç¨‹æ± 
            self.stats.current_threads = clamped_threads;
            self.last_adjustment_time = std.time.nanoTimestamp();

            std.log.debug("çº¿ç¨‹æ± è°ƒæ•´: {} -> {} çº¿ç¨‹", .{ self.stats.current_threads, clamped_threads });
        }
    }

    /// ğŸ§  æ™ºèƒ½è‡ªé€‚åº”è°ƒæ•´
    fn performAdaptiveAdjustment(self: *Self) void {
        if (!self.config.enable_adaptive) return;

        const now = std.time.nanoTimestamp();
        const time_since_last_adjustment = now - self.last_adjustment_time;

        // è‡³å°‘ç­‰å¾…1ç§’å†è¿›è¡Œä¸‹æ¬¡è°ƒæ•´ (1ç§’ = 1,000,000,000çº³ç§’)
        if (time_since_last_adjustment < 1_000_000_000) return;

        const stats = self.stats;

        // è®¡ç®—è´Ÿè½½æŒ‡æ ‡
        const queue_pressure = @as(f64, @floatFromInt(stats.queued_tasks)) /
            @as(f64, @floatFromInt(self.config.queue_high_watermark));

        const thread_utilization = stats.thread_utilization;

        // å†³ç­–é€»è¾‘
        var target_threads = stats.current_threads;

        // é˜Ÿåˆ—å‹åŠ›è¿‡é«˜ä¸”çº¿ç¨‹åˆ©ç”¨ç‡é«˜ -> å¢åŠ çº¿ç¨‹
        if (queue_pressure > 0.8 and thread_utilization > 0.8) {
            target_threads = @min(stats.current_threads + 1, self.config.max_threads);
        }
        // é˜Ÿåˆ—å‹åŠ›ä½ä¸”çº¿ç¨‹åˆ©ç”¨ç‡ä½ -> å‡å°‘çº¿ç¨‹
        else if (queue_pressure < 0.2 and thread_utilization < 0.3) {
            target_threads = @max(stats.current_threads - 1, self.config.min_threads);
        }

        // åº”ç”¨è°ƒæ•´
        if (target_threads != stats.current_threads) {
            self.adjustThreadCount(target_threads);
        }
    }

    /// ğŸ“Š æ›´æ–°ç›‘æ§æŒ‡æ ‡
    fn updateMonitoringMetrics(self: *Self) void {
        // è¿™é‡Œéœ€è¦ä»libxevçº¿ç¨‹æ± è·å–å®é™…çš„çº¿ç¨‹çŠ¶æ€
        // ç”±äºlibxevå¯èƒ½ä¸æä¾›è¯¦ç»†çš„çº¿ç¨‹çŠ¶æ€APIï¼Œæˆ‘ä»¬ä½¿ç”¨ä¼°ç®—å€¼

        // ä¼°ç®—çº¿ç¨‹åˆ©ç”¨ç‡
        const avg_task_duration_ms = @as(f64, @floatFromInt(self.stats.avg_task_duration_ns)) / 1_000_000.0;
        const tasks_per_second = if (avg_task_duration_ms > 0)
            1000.0 / avg_task_duration_ms
        else
            0.0;

        const theoretical_max_throughput = @as(f64, @floatFromInt(self.stats.current_threads)) * tasks_per_second;
        const actual_throughput = self.stats.getTaskThroughput(self.config.monitor_interval_ms);

        self.stats.thread_utilization = if (theoretical_max_throughput > 0)
            @min(1.0, actual_throughput / theoretical_max_throughput)
        else
            0.0;

        // ä¼°ç®—é˜Ÿåˆ—åˆ©ç”¨ç‡
        self.stats.queue_utilization = @as(f64, @floatFromInt(self.stats.queued_tasks)) /
            @as(f64, @floatFromInt(self.config.queue_high_watermark));
    }

    /// ğŸ”„ ç›‘æ§å¾ªç¯
    fn monitorLoop(self: *Self) void {
        while (self.running.load(.acquire)) {
            self.updateMonitoringMetrics();
            self.performAdaptiveAdjustment();

            std.time.sleep(self.config.monitor_interval_ms * std.time.ns_per_ms);
        }
    }
};

/// ğŸ§ª æ™ºèƒ½çº¿ç¨‹æ± æµ‹è¯•
pub fn runSmartThreadPoolTest(allocator: std.mem.Allocator) !void {
    std.debug.print("\n=== ğŸš€ æ™ºèƒ½çº¿ç¨‹æ± æ€§èƒ½æµ‹è¯• ===\n", .{});

    const config = SmartThreadPoolConfig{
        .min_threads = 2,
        .max_threads = 8,
        .monitor_interval_ms = 50,
        .enable_adaptive = true,
        .enable_monitoring = true,
    };

    var thread_pool = try SmartThreadPool.init(allocator, config);
    defer thread_pool.deinit();

    try thread_pool.start();
    defer thread_pool.stop();

    // æ¨¡æ‹Ÿä»»åŠ¡
    const TestTask = struct {
        id: u32,
        duration_ms: u32,

        pub fn run(self: @This()) void {
            std.time.sleep(self.duration_ms * std.time.ns_per_ms);
            std.log.debug("ä»»åŠ¡ {} å®Œæˆ (è€—æ—¶: {}ms)", .{ self.id, self.duration_ms });
        }
    };

    // æäº¤æµ‹è¯•ä»»åŠ¡
    const task_count = 100;
    const start_time = std.time.milliTimestamp();

    for (0..task_count) |i| {
        const task = TestTask{
            .id = @intCast(i),
            .duration_ms = @intCast((i % 10) + 1), // 1-10msçš„ä»»åŠ¡
        };

        try thread_pool.submitTask(task);

        // æ¨¡æ‹Ÿä»»åŠ¡æäº¤çš„é—´éš”
        if (i % 10 == 0) {
            std.time.sleep(10 * std.time.ns_per_ms);
        }
    }

    // ç­‰å¾…ä»»åŠ¡å®Œæˆ
    std.time.sleep(2000 * std.time.ns_per_ms);

    const end_time = std.time.milliTimestamp();
    const duration_ms = @as(u64, @intCast(end_time - start_time));

    const stats = thread_pool.getStats();

    std.debug.print("æ™ºèƒ½çº¿ç¨‹æ± æµ‹è¯•ç»“æœ:\n", .{});
    std.debug.print("  æ€»ä»»åŠ¡æ•°: {}\n", .{stats.total_tasks});
    std.debug.print("  å®Œæˆä»»åŠ¡æ•°: {}\n", .{stats.completed_tasks});
    std.debug.print("  å½“å‰çº¿ç¨‹æ•°: {}\n", .{stats.current_threads});
    std.debug.print("  çº¿ç¨‹åˆ©ç”¨ç‡: {d:.2}%\n", .{stats.thread_utilization * 100});
    std.debug.print("  å¹³å‡ä»»åŠ¡æ‰§è¡Œæ—¶é—´: {d:.2}ms\n", .{@as(f64, @floatFromInt(stats.avg_task_duration_ns)) / 1_000_000.0});
    std.debug.print("  ä»»åŠ¡ååé‡: {d:.2} tasks/sec\n", .{stats.getTaskThroughput(duration_ms)});
    std.debug.print("  âœ… æ™ºèƒ½çº¿ç¨‹æ± æµ‹è¯•å®Œæˆ\n", .{});
}
