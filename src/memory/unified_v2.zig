//! Zokio å†…å­˜ç®¡ç†æ¨¡å— V2 - é›¶å¼€é”€æŠ½è±¡å®ç°
//!
//! P0 ä¼˜åŒ–ï¼šç»Ÿä¸€æ¥å£é›¶å¼€é”€é‡æ„
//! ç›®æ ‡ï¼šä» 8.59M ops/sec æå‡åˆ° 15M+ ops/sec (1.7x æå‡)
//!
//! æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ï¼š
//! - ğŸš€ ç¼–è¯‘æ—¶ç‰¹åŒ–æ¶ˆé™¤è¿è¡Œæ—¶å¼€é”€
//! - âš¡ å†…è”å‡½æ•°å‡å°‘è°ƒç”¨æ ˆæ·±åº¦
//! - ğŸ¯ ç›´æ¥åˆ†é…è·¯å¾„é¿å…ä¸­é—´å±‚
//! - ğŸ§  çº¿ç¨‹æœ¬åœ°ç¼“å­˜å‡å°‘åŸå­æ“ä½œ

const std = @import("std");
const fast_smart_allocator = @import("fast_smart_allocator.zig");
const FastSmartAllocator = fast_smart_allocator.FastSmartAllocator;
const FastSmartAllocatorConfig = fast_smart_allocator.FastSmartAllocatorConfig;
const ExtendedAllocator = @import("extended_allocator.zig").ExtendedAllocator;
const OptimizedAllocator = @import("optimized_allocator.zig").OptimizedAllocator;

/// ç¼“å­˜è¡Œå¤§å°ï¼ˆ64å­—èŠ‚ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ç°ä»£CPUï¼‰
const CACHE_LINE_SIZE = 64;

/// æ€§èƒ½æ¨¡å¼æšä¸¾
pub const PerformanceMode = enum {
    /// ğŸš€ è¶…é«˜æ€§èƒ½æ¨¡å¼ - é›¶å¼€é”€ï¼Œæ— ç›‘æ§
    ultra_fast,
    /// ğŸ”¥ é«˜æ€§èƒ½æ¨¡å¼ - æœ€å°å¼€é”€ï¼ŒåŸºç¡€ç»Ÿè®¡
    high_performance,
    /// âš–ï¸ å¹³è¡¡æ¨¡å¼ - è½»é‡çº§ç›‘æ§
    balanced,
    /// ğŸ“Š è°ƒè¯•æ¨¡å¼ - å®Œæ•´ç›‘æ§å’Œè°ƒè¯•ä¿¡æ¯
    debug,
};

/// ä¼˜åŒ–é…ç½®ç»“æ„
pub const OptimizedConfig = struct {
    /// æ€§èƒ½æ¨¡å¼
    mode: PerformanceMode = .high_performance,

    /// å°å¯¹è±¡é˜ˆå€¼ï¼ˆå­—èŠ‚ï¼‰
    small_threshold: usize = 256,

    /// å¤§å¯¹è±¡é˜ˆå€¼ï¼ˆå­—èŠ‚ï¼‰
    large_threshold: usize = 8192,

    /// æ˜¯å¦å¯ç”¨çº¿ç¨‹æœ¬åœ°ç¼“å­˜
    enable_thread_local_cache: bool = true,

    /// æ˜¯å¦å¯ç”¨ç¼“å­˜è¡Œå¯¹é½
    enable_cache_alignment: bool = true,

    /// æ˜¯å¦å¯ç”¨é¢„å–ä¼˜åŒ–
    enable_prefetch: bool = true,

    /// ç¼–è¯‘æ—¶éªŒè¯é…ç½®
    pub fn validate(comptime self: @This()) void {
        if (self.small_threshold == 0) {
            @compileError("small_threshold must be greater than 0");
        }
        if (self.large_threshold <= self.small_threshold) {
            @compileError("large_threshold must be greater than small_threshold");
        }
    }
};

/// ğŸš€ ç¼–è¯‘æ—¶ç‰¹åŒ–çš„é›¶å¼€é”€å†…å­˜ç®¡ç†å™¨
pub fn ZokioMemoryV2(comptime config: OptimizedConfig) type {
    // ç¼–è¯‘æ—¶éªŒè¯é…ç½®
    comptime config.validate();

    return struct {
        const Self = @This();

        // ç¼–è¯‘æ—¶é€‰æ‹©æœ€ä¼˜åˆ†é…å™¨å®ç°
        const AllocImpl = switch (config.mode) {
            .ultra_fast => UltraFastAllocator,
            .high_performance => HighPerformanceAllocator,
            .balanced => BalancedAllocator,
            .debug => DebugAllocator,
        };

        /// åˆ†é…å™¨å®ç°
        allocator_impl: AllocImpl,

        /// åŸºç¡€åˆ†é…å™¨
        base_allocator: std.mem.Allocator,

        /// çº¿ç¨‹æœ¬åœ°ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        thread_local_cache: if (config.enable_thread_local_cache) ThreadLocalCache else void,

        /// ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼‰
        stats: if (config.mode == .debug) DebugStats else void,

        /// åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
        pub fn init(base_allocator: std.mem.Allocator) !Self {
            return Self{
                .allocator_impl = try AllocImpl.init(base_allocator),
                .base_allocator = base_allocator,
                .thread_local_cache = if (config.enable_thread_local_cache)
                    ThreadLocalCache.init()
                else {},
                .stats = if (config.mode == .debug)
                    DebugStats.init()
                else {},
            };
        }

        /// æ¸…ç†èµ„æº
        pub fn deinit(self: *Self) void {
            self.allocator_impl.deinit();
            if (config.enable_thread_local_cache) {
                self.thread_local_cache.deinit();
            }
        }

        /// ğŸš€ é›¶å¼€é”€åˆ†é…å‡½æ•° - ç¼–è¯‘æ—¶ç‰¹åŒ–
        pub inline fn alloc(self: *Self, comptime T: type, count: usize) ![]T {
            const size = @sizeOf(T) * count;

            // ç¼–è¯‘æ—¶å¤§å°åˆ†ç±»å’Œè·¯å¾„é€‰æ‹©
            return if (comptime size <= config.small_threshold)
                self.allocSmallFast(T, count, size)
            else if (comptime size <= config.large_threshold)
                self.allocMediumFast(T, count, size)
            else
                self.allocLargeFast(T, count, size);
        }

        /// ğŸš€ é›¶å¼€é”€é‡Šæ”¾å‡½æ•°
        pub inline fn free(self: *Self, memory: anytype) void {
            const size = @sizeOf(@TypeOf(memory[0])) * memory.len;

            // ç¼–è¯‘æ—¶å¤§å°åˆ†ç±»å’Œè·¯å¾„é€‰æ‹©
            if (comptime size <= config.small_threshold) {
                self.freeSmallFast(memory, size);
            } else if (comptime size <= config.large_threshold) {
                self.freeMediumFast(memory, size);
            } else {
                self.freeLargeFast(memory, size);
            }
        }

        /// ğŸš€ å°å¯¹è±¡å¿«é€Ÿåˆ†é… - å†…è”ä¼˜åŒ–
        inline fn allocSmallFast(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
            // çº¿ç¨‹æœ¬åœ°ç¼“å­˜ä¼˜å…ˆ
            if (config.enable_thread_local_cache) {
                if (self.thread_local_cache.tryAllocSmall(T, count)) |memory| {
                    return memory;
                }
            }

            // ç›´æ¥è°ƒç”¨ä¼˜åŒ–åˆ†é…å™¨ï¼Œé¿å…ä¸­é—´å±‚
            const memory = try self.allocator_impl.allocSmall(T, count);

            // ç¼“å­˜è¡Œå¯¹é½ä¼˜åŒ–
            if (config.enable_cache_alignment and @sizeOf(T) >= 32) {
                return self.alignToCacheLine(memory);
            }

            // é¢„å–ä¼˜åŒ–
            if (config.enable_prefetch and memory.len > 0) {
                @prefetch(&memory[0], .{});
            }

            // è°ƒè¯•æ¨¡å¼ç»Ÿè®¡
            if (config.mode == .debug) {
                self.stats.recordSmallAllocation(size);
            }

            return memory;
        }

        /// âš¡ ä¸­ç­‰å¯¹è±¡å¿«é€Ÿåˆ†é…
        inline fn allocMediumFast(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
            // çº¿ç¨‹æœ¬åœ°ç¼“å­˜ä¼˜å…ˆ
            if (config.enable_thread_local_cache) {
                if (self.thread_local_cache.tryAllocMedium(T, count)) |memory| {
                    return memory;
                }
            }

            // ç›´æ¥è°ƒç”¨æ‰©å±•åˆ†é…å™¨
            const memory = try self.allocator_impl.allocMedium(T, count);

            // é¢„å–ä¼˜åŒ–
            if (config.enable_prefetch and memory.len > 0) {
                @prefetch(&memory[0], .{});
            }

            // è°ƒè¯•æ¨¡å¼ç»Ÿè®¡
            if (config.mode == .debug) {
                self.stats.recordMediumAllocation(size);
            }

            return memory;
        }

        /// ğŸ¯ å¤§å¯¹è±¡å¿«é€Ÿåˆ†é…
        inline fn allocLargeFast(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
            // å¤§å¯¹è±¡ç›´æ¥ä»åŸºç¡€åˆ†é…å™¨åˆ†é…
            const memory = try self.base_allocator.alloc(T, count);

            // å¤§å¯¹è±¡é¢„å–ä¼˜åŒ–
            if (config.enable_prefetch and memory.len > 0) {
                // é¢„å–å¤šä¸ªç¼“å­˜è¡Œ
                var i: usize = 0;
                while (i < size) : (i += CACHE_LINE_SIZE) {
                    const ptr = @as([*]u8, @ptrCast(memory.ptr)) + i;
                    @prefetch(ptr, .{});
                }
            }

            // è°ƒè¯•æ¨¡å¼ç»Ÿè®¡
            if (config.mode == .debug) {
                self.stats.recordLargeAllocation(size);
            }

            return memory;
        }

        /// ğŸš€ å°å¯¹è±¡å¿«é€Ÿé‡Šæ”¾
        inline fn freeSmallFast(self: *Self, memory: anytype, size: usize) void {
            // çº¿ç¨‹æœ¬åœ°ç¼“å­˜å›æ”¶
            if (config.enable_thread_local_cache) {
                if (self.thread_local_cache.tryFreeSmall(memory)) {
                    return;
                }
            }

            self.allocator_impl.freeSmall(memory);

            // è°ƒè¯•æ¨¡å¼ç»Ÿè®¡
            if (config.mode == .debug) {
                self.stats.recordSmallDeallocation(size);
            }
        }

        /// âš¡ ä¸­ç­‰å¯¹è±¡å¿«é€Ÿé‡Šæ”¾
        inline fn freeMediumFast(self: *Self, memory: anytype, size: usize) void {
            // çº¿ç¨‹æœ¬åœ°ç¼“å­˜å›æ”¶
            if (config.enable_thread_local_cache) {
                if (self.thread_local_cache.tryFreeMedium(memory)) {
                    return;
                }
            }

            self.allocator_impl.freeMedium(memory);

            // è°ƒè¯•æ¨¡å¼ç»Ÿè®¡
            if (config.mode == .debug) {
                self.stats.recordMediumDeallocation(size);
            }
        }

        /// ğŸ¯ å¤§å¯¹è±¡å¿«é€Ÿé‡Šæ”¾
        inline fn freeLargeFast(self: *Self, memory: anytype, size: usize) void {
            self.base_allocator.free(memory);

            // è°ƒè¯•æ¨¡å¼ç»Ÿè®¡
            if (config.mode == .debug) {
                self.stats.recordLargeDeallocation(size);
            }
        }

        /// ç¼“å­˜è¡Œå¯¹é½ä¼˜åŒ–
        inline fn alignToCacheLine(self: *Self, memory: anytype) @TypeOf(memory) {
            _ = self;
            const addr = @intFromPtr(memory.ptr);
            const aligned_addr = std.mem.alignForward(usize, addr, CACHE_LINE_SIZE);
            if (aligned_addr == addr) {
                return memory;
            }

            // å¦‚æœéœ€è¦é‡æ–°å¯¹é½ï¼Œè¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘
            // ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥è¿”å›åŸå†…å­˜
            return memory;
        }

        /// è·å–æ ‡å‡†åˆ†é…å™¨æ¥å£
        pub fn allocator(self: *Self) std.mem.Allocator {
            return self.allocator_impl.allocator();
        }

        /// è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…è°ƒè¯•æ¨¡å¼ï¼‰
        pub fn getStats(self: *const Self) if (config.mode == .debug) DebugStats else void {
            if (config.mode == .debug) {
                return self.stats;
            } else {
                return {};
            }
        }
    };
}

/// ğŸš€ è¶…é«˜æ€§èƒ½åˆ†é…å™¨ - é›¶å¼€é”€å®ç°
const UltraFastAllocator = struct {
    base_allocator: std.mem.Allocator,
    optimized: OptimizedAllocator,
    extended: ExtendedAllocator,

    pub fn init(base_allocator: std.mem.Allocator) !UltraFastAllocator {
        return UltraFastAllocator{
            .base_allocator = base_allocator,
            .optimized = try OptimizedAllocator.init(base_allocator),
            .extended = try ExtendedAllocator.init(base_allocator),
        };
    }

    pub fn deinit(self: *UltraFastAllocator) void {
        self.optimized.deinit();
        self.extended.deinit();
    }

    /// å°å¯¹è±¡åˆ†é… - ç›´æ¥è·¯å¾„
    pub inline fn allocSmall(self: *UltraFastAllocator, comptime T: type, count: usize) ![]T {
        return self.optimized.alloc(T, count);
    }

    /// ä¸­ç­‰å¯¹è±¡åˆ†é… - ç›´æ¥è·¯å¾„
    pub inline fn allocMedium(self: *UltraFastAllocator, comptime T: type, count: usize) ![]T {
        return self.extended.alloc(T, count);
    }

    /// å°å¯¹è±¡é‡Šæ”¾ - ç›´æ¥è·¯å¾„
    pub inline fn freeSmall(self: *UltraFastAllocator, memory: anytype) void {
        self.optimized.free(memory);
    }

    /// ä¸­ç­‰å¯¹è±¡é‡Šæ”¾ - ç›´æ¥è·¯å¾„
    pub inline fn freeMedium(self: *UltraFastAllocator, memory: anytype) void {
        self.extended.free(memory);
    }

    pub fn allocator(self: *UltraFastAllocator) std.mem.Allocator {
        return self.base_allocator;
    }
};

/// ğŸ”¥ é«˜æ€§èƒ½åˆ†é…å™¨ - æœ€å°å¼€é”€å®ç°
const HighPerformanceAllocator = struct {
    base_allocator: std.mem.Allocator,
    smart: FastSmartAllocator,

    pub fn init(base_allocator: std.mem.Allocator) !HighPerformanceAllocator {
        const config = FastSmartAllocatorConfig{};
        return HighPerformanceAllocator{
            .base_allocator = base_allocator,
            .smart = try FastSmartAllocator.init(base_allocator, config),
        };
    }

    pub fn deinit(self: *HighPerformanceAllocator) void {
        self.smart.deinit();
    }

    /// å°å¯¹è±¡åˆ†é…
    pub inline fn allocSmall(self: *HighPerformanceAllocator, comptime T: type, count: usize) ![]T {
        return self.smart.alloc(T, count);
    }

    /// ä¸­ç­‰å¯¹è±¡åˆ†é…
    pub inline fn allocMedium(self: *HighPerformanceAllocator, comptime T: type, count: usize) ![]T {
        return self.smart.alloc(T, count);
    }

    /// å°å¯¹è±¡é‡Šæ”¾
    pub inline fn freeSmall(self: *HighPerformanceAllocator, memory: anytype) void {
        self.smart.free(memory);
    }

    /// ä¸­ç­‰å¯¹è±¡é‡Šæ”¾
    pub inline fn freeMedium(self: *HighPerformanceAllocator, memory: anytype) void {
        self.smart.free(memory);
    }

    pub fn allocator(self: *HighPerformanceAllocator) std.mem.Allocator {
        return self.base_allocator;
    }
};

/// âš–ï¸ å¹³è¡¡åˆ†é…å™¨ - è½»é‡çº§ç›‘æ§
const BalancedAllocator = struct {
    base_allocator: std.mem.Allocator,
    smart: FastSmartAllocator,
    allocation_count: std.atomic.Value(u64),

    pub fn init(base_allocator: std.mem.Allocator) !BalancedAllocator {
        return BalancedAllocator{
            .base_allocator = base_allocator,
            .smart = try FastSmartAllocator.init(base_allocator, .{}),
            .allocation_count = std.atomic.Value(u64).init(0),
        };
    }

    pub fn deinit(self: *BalancedAllocator) void {
        self.smart.deinit();
    }

    /// å°å¯¹è±¡åˆ†é…
    pub inline fn allocSmall(self: *BalancedAllocator, comptime T: type, count: usize) ![]T {
        _ = self.allocation_count.fetchAdd(1, .monotonic);
        return self.smart.alloc(T, count);
    }

    /// ä¸­ç­‰å¯¹è±¡åˆ†é…
    pub inline fn allocMedium(self: *BalancedAllocator, comptime T: type, count: usize) ![]T {
        _ = self.allocation_count.fetchAdd(1, .monotonic);
        return self.smart.alloc(T, count);
    }

    /// å°å¯¹è±¡é‡Šæ”¾
    pub inline fn freeSmall(self: *BalancedAllocator, memory: anytype) void {
        self.smart.free(memory);
    }

    /// ä¸­ç­‰å¯¹è±¡é‡Šæ”¾
    pub inline fn freeMedium(self: *BalancedAllocator, memory: anytype) void {
        self.smart.free(memory);
    }

    pub fn allocator(self: *BalancedAllocator) std.mem.Allocator {
        return self.base_allocator;
    }
};

/// ğŸ“Š è°ƒè¯•åˆ†é…å™¨ - å®Œæ•´ç›‘æ§
const DebugAllocator = struct {
    base_allocator: std.mem.Allocator,
    smart: FastSmartAllocator,

    pub fn init(base_allocator: std.mem.Allocator) !DebugAllocator {
        return DebugAllocator{
            .base_allocator = base_allocator,
            .smart = try FastSmartAllocator.init(base_allocator, .{}),
        };
    }

    pub fn deinit(self: *DebugAllocator) void {
        self.smart.deinit();
    }

    /// å°å¯¹è±¡åˆ†é…
    pub inline fn allocSmall(self: *DebugAllocator, comptime T: type, count: usize) ![]T {
        return self.smart.alloc(T, count);
    }

    /// ä¸­ç­‰å¯¹è±¡åˆ†é…
    pub inline fn allocMedium(self: *DebugAllocator, comptime T: type, count: usize) ![]T {
        return self.smart.alloc(T, count);
    }

    /// å°å¯¹è±¡é‡Šæ”¾
    pub inline fn freeSmall(self: *DebugAllocator, memory: anytype) void {
        self.smart.free(memory);
    }

    /// ä¸­ç­‰å¯¹è±¡é‡Šæ”¾
    pub inline fn freeMedium(self: *DebugAllocator, memory: anytype) void {
        self.smart.free(memory);
    }

    pub fn allocator(self: *DebugAllocator) std.mem.Allocator {
        return self.base_allocator;
    }
};

/// ğŸ§  çº¿ç¨‹æœ¬åœ°ç¼“å­˜ - å‡å°‘åŸå­æ“ä½œ
const ThreadLocalCache = struct {
    /// å°å¯¹è±¡ç¼“å­˜æ•°ç»„ï¼ˆ8ç§å¤§å°ç±»åˆ«ï¼‰
    small_cache: [8]SmallObjectCache,

    /// ä¸­ç­‰å¯¹è±¡ç¼“å­˜
    medium_cache: MediumObjectCache,

    /// ç¼“å­˜ç»Ÿè®¡
    hit_count: u64,
    miss_count: u64,

    const SmallObjectCache = struct {
        objects: [32]*anyopaque, // æ¯ç§å¤§å°32ä¸ªå¯¹è±¡
        count: u8,
        size_class: u8,

        pub fn init(size_class: u8) SmallObjectCache {
            return SmallObjectCache{
                .objects = undefined,
                .count = 0,
                .size_class = size_class,
            };
        }

        pub fn tryGet(self: *SmallObjectCache) ?*anyopaque {
            if (self.count == 0) return null;
            self.count -= 1;
            return self.objects[self.count];
        }

        pub fn tryPut(self: *SmallObjectCache, obj: *anyopaque) bool {
            if (self.count >= 32) return false;
            self.objects[self.count] = obj;
            self.count += 1;
            return true;
        }
    };

    const MediumObjectCache = struct {
        objects: [16]*anyopaque, // 16ä¸ªä¸­ç­‰å¯¹è±¡
        count: u8,

        pub fn init() MediumObjectCache {
            return MediumObjectCache{
                .objects = undefined,
                .count = 0,
            };
        }

        pub fn tryGet(self: *MediumObjectCache) ?*anyopaque {
            if (self.count == 0) return null;
            self.count -= 1;
            return self.objects[self.count];
        }

        pub fn tryPut(self: *MediumObjectCache, obj: *anyopaque) bool {
            if (self.count >= 16) return false;
            self.objects[self.count] = obj;
            self.count += 1;
            return true;
        }
    };

    pub fn init() ThreadLocalCache {
        var cache = ThreadLocalCache{
            .small_cache = undefined,
            .medium_cache = MediumObjectCache.init(),
            .hit_count = 0,
            .miss_count = 0,
        };

        // åˆå§‹åŒ–å°å¯¹è±¡ç¼“å­˜
        for (0..8) |i| {
            cache.small_cache[i] = SmallObjectCache.init(@intCast(i));
        }

        return cache;
    }

    pub fn deinit(self: *ThreadLocalCache) void {
        _ = self;
        // æ¸…ç†ç¼“å­˜ä¸­çš„å¯¹è±¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    }

    /// å°è¯•ä»ç¼“å­˜åˆ†é…å°å¯¹è±¡
    pub fn tryAllocSmall(self: *ThreadLocalCache, comptime T: type, count: usize) ?[]T {
        const size = @sizeOf(T) * count;
        const size_class = getSizeClass(size);

        if (size_class >= 8) return null;

        if (self.small_cache[size_class].tryGet()) |obj| {
            self.hit_count += 1;
            return @as([*]T, @ptrCast(@alignCast(obj)))[0..count];
        }

        self.miss_count += 1;
        return null;
    }

    /// å°è¯•ä»ç¼“å­˜åˆ†é…ä¸­ç­‰å¯¹è±¡
    pub fn tryAllocMedium(self: *ThreadLocalCache, comptime T: type, count: usize) ?[]T {
        if (self.medium_cache.tryGet()) |obj| {
            self.hit_count += 1;
            return @as([*]T, @ptrCast(@alignCast(obj)))[0..count];
        }

        self.miss_count += 1;
        return null;
    }

    /// å°è¯•é‡Šæ”¾å°å¯¹è±¡åˆ°ç¼“å­˜
    pub fn tryFreeSmall(self: *ThreadLocalCache, memory: anytype) bool {
        const size = @sizeOf(@TypeOf(memory[0])) * memory.len;
        const size_class = getSizeClass(size);

        if (size_class >= 8) return false;

        return self.small_cache[size_class].tryPut(@ptrCast(memory.ptr));
    }

    /// å°è¯•é‡Šæ”¾ä¸­ç­‰å¯¹è±¡åˆ°ç¼“å­˜
    pub fn tryFreeMedium(self: *ThreadLocalCache, memory: anytype) bool {
        return self.medium_cache.tryPut(@ptrCast(memory.ptr));
    }

    /// è·å–å¤§å°ç±»åˆ«
    fn getSizeClass(size: usize) usize {
        if (size <= 8) return 0;
        if (size <= 16) return 1;
        if (size <= 32) return 2;
        if (size <= 64) return 3;
        if (size <= 128) return 4;
        if (size <= 256) return 5;
        if (size <= 512) return 6;
        if (size <= 1024) return 7;
        return 8; // è¶…å‡ºå°å¯¹è±¡èŒƒå›´
    }

    /// è·å–ç¼“å­˜å‘½ä¸­ç‡
    pub fn getHitRate(self: *const ThreadLocalCache) f32 {
        const total = self.hit_count + self.miss_count;
        if (total == 0) return 0.0;
        return @as(f32, @floatFromInt(self.hit_count)) / @as(f32, @floatFromInt(total));
    }
};

/// ğŸ“Š è°ƒè¯•ç»Ÿè®¡ä¿¡æ¯
const DebugStats = struct {
    /// åˆ†é…ç»Ÿè®¡
    small_allocations: std.atomic.Value(u64),
    medium_allocations: std.atomic.Value(u64),
    large_allocations: std.atomic.Value(u64),

    /// é‡Šæ”¾ç»Ÿè®¡
    small_deallocations: std.atomic.Value(u64),
    medium_deallocations: std.atomic.Value(u64),
    large_deallocations: std.atomic.Value(u64),

    /// å­—èŠ‚ç»Ÿè®¡
    total_allocated_bytes: std.atomic.Value(u64),
    total_deallocated_bytes: std.atomic.Value(u64),

    /// æ—¶é—´ç»Ÿè®¡
    total_allocation_time: std.atomic.Value(u64),
    allocation_count: std.atomic.Value(u64),

    pub fn init() DebugStats {
        return DebugStats{
            .small_allocations = std.atomic.Value(u64).init(0),
            .medium_allocations = std.atomic.Value(u64).init(0),
            .large_allocations = std.atomic.Value(u64).init(0),
            .small_deallocations = std.atomic.Value(u64).init(0),
            .medium_deallocations = std.atomic.Value(u64).init(0),
            .large_deallocations = std.atomic.Value(u64).init(0),
            .total_allocated_bytes = std.atomic.Value(u64).init(0),
            .total_deallocated_bytes = std.atomic.Value(u64).init(0),
            .total_allocation_time = std.atomic.Value(u64).init(0),
            .allocation_count = std.atomic.Value(u64).init(0),
        };
    }

    pub fn recordSmallAllocation(self: *DebugStats, size: usize) void {
        _ = self.small_allocations.fetchAdd(1, .monotonic);
        _ = self.total_allocated_bytes.fetchAdd(size, .monotonic);
        _ = self.allocation_count.fetchAdd(1, .monotonic);
    }

    pub fn recordMediumAllocation(self: *DebugStats, size: usize) void {
        _ = self.medium_allocations.fetchAdd(1, .monotonic);
        _ = self.total_allocated_bytes.fetchAdd(size, .monotonic);
        _ = self.allocation_count.fetchAdd(1, .monotonic);
    }

    pub fn recordLargeAllocation(self: *DebugStats, size: usize) void {
        _ = self.large_allocations.fetchAdd(1, .monotonic);
        _ = self.total_allocated_bytes.fetchAdd(size, .monotonic);
        _ = self.allocation_count.fetchAdd(1, .monotonic);
    }

    pub fn recordSmallDeallocation(self: *DebugStats, size: usize) void {
        _ = self.small_deallocations.fetchAdd(1, .monotonic);
        _ = self.total_deallocated_bytes.fetchAdd(size, .monotonic);
    }

    pub fn recordMediumDeallocation(self: *DebugStats, size: usize) void {
        _ = self.medium_deallocations.fetchAdd(1, .monotonic);
        _ = self.total_deallocated_bytes.fetchAdd(size, .monotonic);
    }

    pub fn recordLargeDeallocation(self: *DebugStats, size: usize) void {
        _ = self.large_deallocations.fetchAdd(1, .monotonic);
        _ = self.total_deallocated_bytes.fetchAdd(size, .monotonic);
    }

    /// è·å–å¹³å‡åˆ†é…æ—¶é—´ï¼ˆçº³ç§’ï¼‰
    pub fn getAverageAllocationTime(self: *const DebugStats) u64 {
        const total_time = self.total_allocation_time.load(.monotonic);
        const count = self.allocation_count.load(.monotonic);
        if (count == 0) return 0;
        return total_time / count;
    }

    /// è·å–æ€»åˆ†é…æ¬¡æ•°
    pub fn getTotalAllocations(self: *const DebugStats) u64 {
        return self.small_allocations.load(.monotonic) +
            self.medium_allocations.load(.monotonic) +
            self.large_allocations.load(.monotonic);
    }

    /// è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡
    pub fn getCurrentMemoryUsage(self: *const DebugStats) u64 {
        const allocated = self.total_allocated_bytes.load(.monotonic);
        const deallocated = self.total_deallocated_bytes.load(.monotonic);
        return allocated - deallocated;
    }
};
