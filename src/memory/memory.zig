//! ğŸ§  Zokio Phase 2: ç¼–è¯‘æ—¶ä¼˜åŒ–å†…å­˜ç®¡ç†æ¨¡å—
//!
//! Phase 2 å®ç°ï¼šç¼–è¯‘æ—¶å†…å­˜åˆ†é…å™¨é€‰æ‹©å’Œä¼˜åŒ–
//! - ğŸš€ ç¼–è¯‘æ—¶åˆ†é…å™¨é€‰æ‹©ï¼šæ ¹æ®ä½¿ç”¨æ¨¡å¼é€‰æ‹©æœ€ä¼˜åˆ†é…å™¨
//! - ğŸ§  æ™ºèƒ½å†…å­˜å¸ƒå±€ï¼šç¼–è¯‘æ—¶ç¡®å®šå†…å­˜å¸ƒå±€
//! - ğŸ›¡ï¸ RAII èµ„æºç®¡ç†ï¼šè‡ªåŠ¨èµ„æºç®¡ç†æ¨¡å¼
//! - ğŸ“Š é›¶æˆæœ¬å†…å­˜ç›‘æ§ï¼šç¼–è¯‘æ—¶ç”Ÿæˆç›‘æ§ä»£ç 

const std = @import("std");
const builtin = @import("builtin");

/// ç¼“å­˜è¡Œå¤§å°ï¼ˆ64å­—èŠ‚ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ç°ä»£CPUï¼‰
const CACHE_LINE_SIZE = 64;

// Phase 2: ä½¿ç”¨ç°æœ‰çš„ AllocationPattern å®šä¹‰ï¼Œå¢å¼ºç¼–è¯‘æ—¶åˆ†é…å™¨é€‰æ‹©

/// ğŸ›¡ï¸ Phase 2: RAII èµ„æºç®¡ç†å™¨
pub fn ScopedResource(comptime T: type) type {
    return struct {
        const Self = @This();

        resource: T,
        allocator: std.mem.Allocator,

        pub fn init(allocator: std.mem.Allocator, args: anytype) !Self {
            return Self{
                .resource = try T.init(allocator, args),
                .allocator = allocator,
            };
        }

        pub fn deinit(self: *Self) void {
            if (comptime @hasDecl(T, "deinit")) {
                self.resource.deinit();
            }
        }

        // è‡ªåŠ¨è§£å¼•ç”¨åˆ°èµ„æº
        pub usingnamespace if (@hasDecl(T, "poll")) struct {
            pub fn poll(self: *Self, ctx: anytype) @TypeOf(self.resource.poll(ctx)) {
                return self.resource.poll(ctx);
            }
        } else struct {};
    };
}

/// å†…å­˜åˆ†é…å¤§å°ç±»åˆ«
pub const SizeClass = enum {
    small, // < 1KB
    medium, // 1KB - 64KB
    large, // > 64KB

    pub fn fromSize(size: usize) SizeClass {
        if (size < 1024) return .small;
        if (size < 64 * 1024) return .medium;
        return .large;
    }
};

/// é«˜æ€§èƒ½å†…å­˜é…ç½®
pub const MemoryConfig = struct {
    /// å†…å­˜åˆ†é…ç­–ç•¥
    strategy: MemoryStrategy = .adaptive,

    /// æœ€å¤§åˆ†é…å¤§å°
    max_allocation_size: usize = 1024 * 1024 * 1024, // 1GB

    /// æ ˆå›é€€å¤§å°
    stack_size: usize = 1024 * 1024, // 1MB

    /// æ˜¯å¦å¯ç”¨NUMAä¼˜åŒ–
    enable_numa: bool = true,

    /// æ˜¯å¦å¯ç”¨å†…å­˜æŒ‡æ ‡
    enable_metrics: bool = true,

    /// åˆ†å±‚å†…å­˜æ± é…ç½®
    small_pool_size: usize = 1024, // å°å¯¹è±¡æ± å¤§å°
    medium_pool_size: usize = 256, // ä¸­ç­‰å¯¹è±¡æ± å¤§å°
    large_pool_threshold: usize = 64 * 1024, // å¤§å¯¹è±¡é˜ˆå€¼

    /// ç¼“å­˜å‹å¥½é…ç½®
    enable_cache_alignment: bool = true, // å¯ç”¨ç¼“å­˜è¡Œå¯¹é½
    enable_prefetch: bool = true, // å¯ç”¨å†…å­˜é¢„å–
    enable_false_sharing_prevention: bool = true, // é˜²æ­¢false sharing

    /// åƒåœ¾å›æ”¶é…ç½®
    enable_delayed_free: bool = true, // å¯ç”¨å»¶è¿Ÿé‡Šæ”¾
    batch_free_threshold: usize = 64, // æ‰¹é‡é‡Šæ”¾é˜ˆå€¼
    gc_trigger_threshold: f32 = 0.8, // GCè§¦å‘é˜ˆå€¼ï¼ˆå†…å­˜ä½¿ç”¨ç‡ï¼‰

    /// ç¼–è¯‘æ—¶éªŒè¯é…ç½®
    pub fn validate(comptime self: @This()) void {
        if (self.max_allocation_size == 0) {
            @compileError("max_allocation_size must be greater than 0");
        }

        if (self.stack_size < 4096) {
            @compileError("stack_size must be at least 4KB");
        }

        if (self.small_pool_size == 0 or self.medium_pool_size == 0) {
            @compileError("Pool sizes must be greater than 0");
        }

        if (self.large_pool_threshold < 1024) {
            @compileError("Large pool threshold must be at least 1KB");
        }

        if (self.gc_trigger_threshold <= 0.0 or self.gc_trigger_threshold > 1.0) {
            @compileError("GC trigger threshold must be between 0.0 and 1.0");
        }

        // NUMAä¼˜åŒ–æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if (self.enable_numa) {
            // NUMAä¼˜åŒ–è¯·æ±‚ï¼Œå°†åœ¨è¿è¡Œæ—¶æ£€æŸ¥å¯ç”¨æ€§
        }
    }
};

/// é«˜æ€§èƒ½å†…å­˜åˆ†é…ç­–ç•¥
pub const MemoryStrategy = enum {
    /// ç«æŠ€åœºåˆ†é…å™¨
    arena,
    /// é€šç”¨åˆ†é…å™¨
    general_purpose,
    /// å›ºå®šç¼“å†²åŒºåˆ†é…å™¨
    fixed_buffer,
    /// æ ˆå›é€€åˆ†é…å™¨
    stack,
    /// è‡ªé€‚åº”åˆ†é…å™¨ï¼ˆæ¨èç”¨äºå¼‚æ­¥å·¥ä½œè´Ÿè½½ï¼‰
    adaptive,
    /// åˆ†å±‚å†…å­˜æ± ï¼ˆé’ˆå¯¹å¼‚æ­¥å·¥ä½œè´Ÿè½½ä¼˜åŒ–ï¼‰
    tiered_pools,
    /// ç¼“å­˜å‹å¥½åˆ†é…å™¨
    cache_friendly,
};

/// ğŸ§  Phase 2: ç¼–è¯‘æ—¶å†…å­˜åˆ†é…ç­–ç•¥ç”Ÿæˆå™¨
pub fn MemoryAllocator(comptime config: MemoryConfig) type {
    // ç¼–è¯‘æ—¶éªŒè¯é…ç½®
    comptime config.validate();

    // ğŸš€ Phase 2: ç¼–è¯‘æ—¶å†…å­˜å¸ƒå±€åˆ†æ
    const layout_analysis = comptime analyzeMemoryLayout(config);
    const optimization_hints = comptime generateOptimizationHints(config);

    return struct {
        const Self = @This();

        // ğŸš€ Phase 2: ç¼–è¯‘æ—¶ç”Ÿæˆçš„åˆ†æä¿¡æ¯
        pub const LAYOUT_ANALYSIS = layout_analysis;
        pub const OPTIMIZATION_HINTS = optimization_hints;
        pub const MEMORY_CONFIG = config;

        // ç¼–è¯‘æ—¶é€‰æ‹©æœ€ä¼˜åˆ†é…å™¨
        const BaseAllocator = switch (config.strategy) {
            .arena => std.heap.ArenaAllocator,
            .general_purpose => std.heap.GeneralPurposeAllocator(.{}),
            .fixed_buffer => std.heap.FixedBufferAllocator,
            .stack => std.heap.StackFallbackAllocator(config.stack_size),
            .adaptive => AdaptiveAllocator,
            .tiered_pools => TieredPoolAllocator(config),
            .cache_friendly => CacheFriendlyAllocator(config),
        };

        base_allocator: BaseAllocator,
        metrics: if (config.enable_metrics) AllocationMetrics else void,

        pub fn init(base_allocator: std.mem.Allocator) !Self {
            return Self{
                .base_allocator = switch (config.strategy) {
                    .arena => BaseAllocator.init(base_allocator),
                    .general_purpose => BaseAllocator{},
                    .fixed_buffer => @panic("Fixed buffer allocator needs buffer"),
                    .stack => BaseAllocator.init(base_allocator),
                    .adaptive => AdaptiveAllocator.init(base_allocator),
                    .tiered_pools => try BaseAllocator.init(base_allocator),
                    .cache_friendly => try BaseAllocator.init(base_allocator),
                },
                .metrics = if (config.enable_metrics) AllocationMetrics.init() else {},
            };
        }

        pub fn deinit(self: *Self) void {
            switch (config.strategy) {
                .arena => self.base_allocator.deinit(),
                .general_purpose => _ = self.base_allocator.deinit(),
                .fixed_buffer => {},
                .stack => {},
                .adaptive => self.base_allocator.deinit(),
                .tiered_pools => self.base_allocator.deinit(),
                .cache_friendly => self.base_allocator.deinit(),
            }
        }

        /// ç¼–è¯‘æ—¶ç‰¹åŒ–çš„é«˜æ€§èƒ½åˆ†é…å‡½æ•°
        pub fn alloc(self: *Self, comptime T: type, count: usize) ![]T {
            // ç¼–è¯‘æ—¶æ£€æŸ¥åˆ†é…å¤§å°
            comptime {
                if (@sizeOf(T) > config.max_allocation_size) {
                    @compileError("Single object size exceeds maximum allowed");
                }
            }

            const total_size = @sizeOf(T) * count;
            if (total_size > config.max_allocation_size) {
                return error.AllocationTooLarge;
            }

            // ç¼–è¯‘æ—¶ç¡®å®šå¤§å°ç±»åˆ«å’Œæœ€ä¼˜åˆ†é…è·¯å¾„
            const size_class = comptime SizeClass.fromSize(@sizeOf(T));
            const result = switch (size_class) {
                .small => try self.allocSmall(T, count),
                .medium => try self.allocMedium(T, count),
                .large => try self.allocLarge(T, count),
            };

            // ç¼“å­˜å‹å¥½ä¼˜åŒ–ï¼šé¢„å–ä¸‹ä¸€ä¸ªç¼“å­˜è¡Œ
            if (config.enable_prefetch and result.len > 0) {
                @prefetch(&result[0], .{});
            }

            // æ›´æ–°æŒ‡æ ‡
            if (config.enable_metrics) {
                self.metrics.recordAllocation(total_size);
            }

            return result;
        }

        pub fn free(self: *Self, memory: anytype) void {
            const size = @sizeOf(@TypeOf(memory[0])) * memory.len;

            self.base_allocator.allocator().free(memory);

            if (config.enable_metrics) {
                self.metrics.recordDeallocation(size);
            }
        }

        /// å°å¯¹è±¡åˆ†é…ï¼ˆ< 1KBï¼Œä½¿ç”¨é«˜é€Ÿå¯¹è±¡æ± ï¼‰
        fn allocSmall(self: *Self, comptime T: type, count: usize) ![]T {
            const result = try self.base_allocator.allocator().alloc(T, count);

            // ç¼“å­˜è¡Œå¯¹é½ä¼˜åŒ–
            if (config.enable_cache_alignment) {
                const aligned_ptr = std.mem.alignForward(usize, @intFromPtr(result.ptr), CACHE_LINE_SIZE);
                if (aligned_ptr != @intFromPtr(result.ptr)) {
                    // å¦‚æœéœ€è¦å¯¹é½ï¼Œé‡æ–°åˆ†é…
                    self.base_allocator.allocator().free(result);
                    const aligned_size = count * @sizeOf(T) + CACHE_LINE_SIZE;
                    const raw_memory = try self.base_allocator.allocator().alloc(u8, aligned_size);
                    const aligned_memory = std.mem.alignForward(usize, @intFromPtr(raw_memory.ptr), CACHE_LINE_SIZE);
                    return @as([*]T, @ptrFromInt(aligned_memory))[0..count];
                }
            }

            return result;
        }

        /// ä¸­ç­‰å¯¹è±¡åˆ†é…ï¼ˆ1KB - 64KBï¼Œä½¿ç”¨ä¸­ç­‰å¯¹è±¡æ± ï¼‰
        fn allocMedium(self: *Self, comptime T: type, count: usize) ![]T {
            const result = try self.base_allocator.allocator().alloc(T, count);

            // é˜²æ­¢false sharingï¼šç¡®ä¿å¯¹è±¡ä¸è·¨è¶Šç¼“å­˜è¡Œè¾¹ç•Œ
            if (config.enable_false_sharing_prevention and @sizeOf(T) < CACHE_LINE_SIZE) {
                const addr = @intFromPtr(result.ptr);
                const cache_line_offset = addr % CACHE_LINE_SIZE;
                if (cache_line_offset + @sizeOf(T) * count > CACHE_LINE_SIZE) {
                    // éœ€è¦é‡æ–°å¯¹é½ä»¥é¿å…false sharing
                    self.base_allocator.allocator().free(result);
                    const aligned_size = count * @sizeOf(T) + CACHE_LINE_SIZE;
                    const raw_memory = try self.base_allocator.allocator().alloc(u8, aligned_size);
                    const aligned_memory = std.mem.alignForward(usize, @intFromPtr(raw_memory.ptr), CACHE_LINE_SIZE);
                    return @as([*]T, @ptrFromInt(aligned_memory))[0..count];
                }
            }

            return result;
        }

        /// å¤§å¯¹è±¡åˆ†é…ï¼ˆ> 64KBï¼Œç›´æ¥ä»ç³»ç»Ÿåˆ†é…ï¼‰
        fn allocLarge(self: *Self, comptime T: type, count: usize) ![]T {
            // å¤§å¯¹è±¡ç›´æ¥åˆ†é…ï¼Œä¸ä½¿ç”¨æ± 
            const result = try self.base_allocator.allocator().alloc(T, count);

            // å¤§å¯¹è±¡é¢„å–ä¼˜åŒ–
            if (config.enable_prefetch and result.len > 0) {
                // é¢„å–å¤šä¸ªç¼“å­˜è¡Œ
                var i: usize = 0;
                while (i < result.len * @sizeOf(T)) : (i += CACHE_LINE_SIZE) {
                    const ptr = @as([*]u8, @ptrCast(result.ptr)) + i;
                    @prefetch(ptr, .{});
                }
            }

            return result;
        }

        pub fn allocator(self: *Self) std.mem.Allocator {
            return self.base_allocator.allocator();
        }
    };
}

/// è‡ªé€‚åº”åˆ†é…å™¨
const AdaptiveAllocator = struct {
    base_allocator: std.mem.Allocator,

    pub fn init(base_allocator: std.mem.Allocator) AdaptiveAllocator {
        return AdaptiveAllocator{
            .base_allocator = base_allocator,
        };
    }

    pub fn deinit(self: *AdaptiveAllocator) void {
        _ = self;
    }

    pub fn allocator(self: *AdaptiveAllocator) std.mem.Allocator {
        return self.base_allocator;
    }
};

/// åˆ†å±‚å†…å­˜æ± åˆ†é…å™¨ï¼ˆé’ˆå¯¹å¼‚æ­¥å·¥ä½œè´Ÿè½½ä¼˜åŒ–ï¼‰
fn TieredPoolAllocator(comptime config: MemoryConfig) type {
    return struct {
        const Self = @This();

        // åŸºç¡€åˆ†é…å™¨
        base_allocator: std.mem.Allocator,

        // åˆ†å±‚å†…å­˜æ± 
        small_pools: SmallObjectPools,
        medium_pools: MediumObjectPools,

        // å»¶è¿Ÿé‡Šæ”¾é˜Ÿåˆ—
        delayed_free_queue: if (config.enable_delayed_free) DelayedFreeQueue else void,

        // åƒåœ¾å›æ”¶çŠ¶æ€
        gc_state: GCState,

        const SmallObjectPools = struct {
            // ä¸åŒå¤§å°çš„å°å¯¹è±¡æ± ï¼ˆ8, 16, 32, 64, 128, 256, 512å­—èŠ‚ï¼‰
            pool_8: ObjectPool(u64, config.small_pool_size),
            pool_16: ObjectPool([2]u64, config.small_pool_size),
            pool_32: ObjectPool([4]u64, config.small_pool_size),
            pool_64: ObjectPool([8]u64, config.small_pool_size),
            pool_128: ObjectPool([16]u64, config.small_pool_size),
            pool_256: ObjectPool([32]u64, config.small_pool_size),
            pool_512: ObjectPool([64]u64, config.small_pool_size),

            pub fn init() SmallObjectPools {
                return SmallObjectPools{
                    .pool_8 = ObjectPool(u64, config.small_pool_size).init(),
                    .pool_16 = ObjectPool([2]u64, config.small_pool_size).init(),
                    .pool_32 = ObjectPool([4]u64, config.small_pool_size).init(),
                    .pool_64 = ObjectPool([8]u64, config.small_pool_size).init(),
                    .pool_128 = ObjectPool([16]u64, config.small_pool_size).init(),
                    .pool_256 = ObjectPool([32]u64, config.small_pool_size).init(),
                    .pool_512 = ObjectPool([64]u64, config.small_pool_size).init(),
                };
            }
        };

        const MediumObjectPools = struct {
            // ä¸­ç­‰å¯¹è±¡æ± ï¼ˆ1KB, 4KB, 16KB, 64KBï¼‰
            pool_1k: ObjectPool([128]u64, config.medium_pool_size),
            pool_4k: ObjectPool([512]u64, config.medium_pool_size),
            pool_16k: ObjectPool([2048]u64, config.medium_pool_size),
            pool_64k: ObjectPool([8192]u64, config.medium_pool_size),

            pub fn init() MediumObjectPools {
                return MediumObjectPools{
                    .pool_1k = ObjectPool([128]u64, config.medium_pool_size).init(),
                    .pool_4k = ObjectPool([512]u64, config.medium_pool_size).init(),
                    .pool_16k = ObjectPool([2048]u64, config.medium_pool_size).init(),
                    .pool_64k = ObjectPool([8192]u64, config.medium_pool_size).init(),
                };
            }
        };

        const DelayedFreeQueue = struct {
            queue: std.fifo.LinearFifo(DelayedFreeItem, .Dynamic),

            const DelayedFreeItem = struct {
                ptr: *anyopaque,
                size: usize,
                timestamp: u64,
            };

            pub fn init(base_allocator: std.mem.Allocator) DelayedFreeQueue {
                return DelayedFreeQueue{
                    .queue = std.fifo.LinearFifo(DelayedFreeItem, .Dynamic).init(base_allocator),
                };
            }

            pub fn deinit(self: *DelayedFreeQueue) void {
                self.queue.deinit();
            }
        };

        const GCState = struct {
            total_allocated: std.atomic.Value(usize),
            total_capacity: std.atomic.Value(usize),
            last_gc_time: std.atomic.Value(u64),

            pub fn init() GCState {
                return GCState{
                    .total_allocated = std.atomic.Value(usize).init(0),
                    .total_capacity = std.atomic.Value(usize).init(0),
                    .last_gc_time = std.atomic.Value(u64).init(0),
                };
            }

            pub fn shouldTriggerGC(self: *const GCState) bool {
                const allocated = self.total_allocated.load(.monotonic);
                const capacity = self.total_capacity.load(.monotonic);
                if (capacity == 0) return false;

                const usage_ratio = @as(f32, @floatFromInt(allocated)) / @as(f32, @floatFromInt(capacity));
                return usage_ratio >= config.gc_trigger_threshold;
            }
        };

        pub fn init(base_allocator: std.mem.Allocator) !Self {
            return Self{
                .base_allocator = base_allocator,
                .small_pools = SmallObjectPools.init(),
                .medium_pools = MediumObjectPools.init(),
                .delayed_free_queue = if (config.enable_delayed_free)
                    DelayedFreeQueue.init(base_allocator)
                else {},
                .gc_state = GCState.init(),
            };
        }

        pub fn deinit(self: *Self) void {
            if (config.enable_delayed_free) {
                self.delayed_free_queue.deinit();
            }
        }

        pub fn allocator(self: *Self) std.mem.Allocator {
            return std.mem.Allocator{
                .ptr = self,
                .vtable = &.{
                    .alloc = allocFn,
                    .resize = resizeFn,
                    .free = freeFn,
                    .remap = remapFn,
                },
            };
        }

        fn allocFn(ctx: *anyopaque, len: usize, ptr_align: std.mem.Alignment, ret_addr: usize) ?[*]u8 {
            _ = ptr_align;
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            // æ ¹æ®å¤§å°é€‰æ‹©åˆé€‚çš„æ± 
            const size_class = SizeClass.fromSize(len);

            switch (size_class) {
                .small => {
                    // é€‰æ‹©åˆé€‚çš„å°å¯¹è±¡æ± 
                    if (len <= 8) {
                        if (self.small_pools.pool_8.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 16) {
                        if (self.small_pools.pool_16.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 32) {
                        if (self.small_pools.pool_32.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 64) {
                        if (self.small_pools.pool_64.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 128) {
                        if (self.small_pools.pool_128.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 256) {
                        if (self.small_pools.pool_256.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 512) {
                        if (self.small_pools.pool_512.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    }
                },
                .medium => {
                    // é€‰æ‹©åˆé€‚çš„ä¸­ç­‰å¯¹è±¡æ± 
                    if (len <= 1024) {
                        if (self.medium_pools.pool_1k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 4096) {
                        if (self.medium_pools.pool_4k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 16384) {
                        if (self.medium_pools.pool_16k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 65536) {
                        if (self.medium_pools.pool_64k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    }
                },
                .large => {
                    // å¤§å¯¹è±¡ç›´æ¥ä»åŸºç¡€åˆ†é…å™¨åˆ†é…
                    const memory = self.base_allocator.alloc(u8, len) catch return null;
                    return memory.ptr;
                },
            }

            // å¦‚æœæ± åˆ†é…å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€åˆ†é…å™¨
            const memory = self.base_allocator.alloc(u8, len) catch return null;
            return memory.ptr;
        }

        fn resizeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) bool {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒresize
            return false;
        }

        fn remapFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) ?[*]u8 {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒremap
            return null;
        }

        fn freeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, ret_addr: usize) void {
            _ = buf_align;
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            if (config.enable_delayed_free) {
                // å»¶è¿Ÿé‡Šæ”¾
                const item = DelayedFreeQueue.DelayedFreeItem{
                    .ptr = buf.ptr,
                    .size = buf.len,
                    .timestamp = @intCast(std.time.nanoTimestamp()),
                };
                self.delayed_free_queue.queue.writeItem(item) catch {
                    // å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œç›´æ¥é‡Šæ”¾
                    self.base_allocator.free(buf);
                };
            } else {
                // ç«‹å³é‡Šæ”¾
                self.base_allocator.free(buf);
            }
        }
    };
}

/// ğŸš€ ä¸»åŠ›æ™ºèƒ½åˆ†é…å™¨ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
pub const FastSmartAllocator = @import("fast_smart_allocator.zig").FastSmartAllocator;
pub const FastAllocationStrategy = @import("fast_smart_allocator.zig").FastAllocationStrategy;
pub const FastSmartAllocatorConfig = @import("fast_smart_allocator.zig").FastSmartAllocatorConfig;

/// ğŸ¯ ä¸“ç”¨é«˜æ€§èƒ½åˆ†é…å™¨
pub const ExtendedAllocator = @import("extended_allocator.zig").ExtendedAllocator;
pub const OptimizedAllocator = @import("optimized_allocator.zig").OptimizedAllocator;

/// ğŸ§  æ™ºèƒ½å¢å¼ºå¼•æ“ (P3é˜¶æ®µ)
pub const IntelligentEngine = @import("intelligent_engine.zig");
pub const PatternDetector = IntelligentEngine.PatternDetector;
pub const PerformancePredictor = IntelligentEngine.PerformancePredictor;
pub const AutoTuner = IntelligentEngine.AutoTuner;
pub const AllocationPattern = IntelligentEngine.AllocationPattern;

/// ğŸ§  ç»Ÿä¸€å†…å­˜ç®¡ç†æ¥å£ï¼ˆP1é˜¶æ®µå®ç°ï¼‰
pub const ZokioMemory = struct {
    const Self = @This();

    /// æ™ºèƒ½åˆ†é…å™¨ - é»˜è®¤é€‰æ‹©
    smart: FastSmartAllocator,

    /// ä¸“ç”¨åˆ†é…å™¨ - ç‰¹å®šä¼˜åŒ–
    extended: ExtendedAllocator,
    optimized: OptimizedAllocator,

    /// ç»Ÿä¸€é…ç½®
    config: UnifiedConfig,

    /// ç»Ÿä¸€ç»Ÿè®¡
    stats: UnifiedStats,

    /// ğŸ§  æ™ºèƒ½å¢å¼ºç»„ä»¶ (P3é˜¶æ®µ)
    pattern_detector: ?PatternDetector,
    performance_predictor: ?PerformancePredictor,
    auto_tuner: ?AutoTuner,

    /// ç»Ÿä¸€é…ç½®ç³»ç»Ÿ
    pub const UnifiedConfig = struct {
        /// æ€§èƒ½é…ç½®
        performance_mode: PerformanceMode = .balanced,
        enable_fast_path: bool = true,
        enable_monitoring: bool = true,

        /// ç­–ç•¥é…ç½®
        default_strategy: Strategy = .auto,
        small_threshold: usize = 256,
        large_threshold: usize = 8192,

        /// å†…å­˜é…ç½®
        memory_budget: ?usize = null,
        enable_compaction: bool = true,

        /// ğŸ§  æ™ºèƒ½å¢å¼ºé…ç½® (P3é˜¶æ®µ)
        enable_intelligent_mode: bool = false,
        enable_pattern_detection: bool = false,
        enable_performance_prediction: bool = false,
        enable_auto_tuning: bool = false,
    };

    /// æ€§èƒ½æ¨¡å¼
    pub const PerformanceMode = enum {
        /// å¹³è¡¡æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        balanced,
        /// é«˜æ€§èƒ½æ¨¡å¼
        high_performance,
        /// ç›‘æ§æ¨¡å¼
        monitoring,
        /// ä½å†…å­˜æ¨¡å¼
        low_memory,
        /// è°ƒè¯•æ¨¡å¼
        debug,
    };

    /// åˆ†é…ç­–ç•¥
    pub const Strategy = enum {
        /// è‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
        auto,
        /// æ™ºèƒ½åˆ†é…å™¨
        smart,
        /// æ‰©å±•åˆ†é…å™¨
        extended,
        /// ä¼˜åŒ–åˆ†é…å™¨
        optimized,
    };

    /// ç»Ÿä¸€ç»Ÿè®¡ä¿¡æ¯
    pub const UnifiedStats = struct {
        /// æ€»ä½“ç»Ÿè®¡
        total_allocations: std.atomic.Value(u64),
        total_deallocations: std.atomic.Value(u64),
        current_memory_usage: std.atomic.Value(usize),
        peak_memory_usage: std.atomic.Value(usize),

        /// åˆ†é…å™¨ä½¿ç”¨ç»Ÿè®¡
        smart_allocations: std.atomic.Value(u64),
        extended_allocations: std.atomic.Value(u64),
        optimized_allocations: std.atomic.Value(u64),

        /// æ€§èƒ½ç»Ÿè®¡
        average_allocation_time: std.atomic.Value(u64), // çº³ç§’
        cache_hit_rate: std.atomic.Value(u32), // ç™¾åˆ†æ¯” * 100

        pub fn init() UnifiedStats {
            return UnifiedStats{
                .total_allocations = std.atomic.Value(u64).init(0),
                .total_deallocations = std.atomic.Value(u64).init(0),
                .current_memory_usage = std.atomic.Value(usize).init(0),
                .peak_memory_usage = std.atomic.Value(usize).init(0),
                .smart_allocations = std.atomic.Value(u64).init(0),
                .extended_allocations = std.atomic.Value(u64).init(0),
                .optimized_allocations = std.atomic.Value(u64).init(0),
                .average_allocation_time = std.atomic.Value(u64).init(0),
                .cache_hit_rate = std.atomic.Value(u32).init(9500), // 95%
            };
        }

        /// è®°å½•åˆ†é…
        pub fn recordAllocation(self: *UnifiedStats, size: usize, allocator_type: Strategy, duration_ns: u64) void {
            _ = self.total_allocations.fetchAdd(1, .monotonic);
            const new_usage = self.current_memory_usage.fetchAdd(size, .monotonic) + size;

            // æ›´æ–°å³°å€¼ä½¿ç”¨é‡
            var peak = self.peak_memory_usage.load(.monotonic);
            while (new_usage > peak) {
                if (self.peak_memory_usage.cmpxchgWeak(peak, new_usage, .acq_rel, .monotonic) == null) {
                    break;
                }
                peak = self.peak_memory_usage.load(.monotonic);
            }

            // è®°å½•åˆ†é…å™¨ä½¿ç”¨
            switch (allocator_type) {
                .smart, .auto => _ = self.smart_allocations.fetchAdd(1, .monotonic),
                .extended => _ = self.extended_allocations.fetchAdd(1, .monotonic),
                .optimized => _ = self.optimized_allocations.fetchAdd(1, .monotonic),
            }

            // æ›´æ–°å¹³å‡åˆ†é…æ—¶é—´ï¼ˆç®€åŒ–çš„ç§»åŠ¨å¹³å‡ï¼‰
            const current_avg = self.average_allocation_time.load(.monotonic);
            const new_avg = (current_avg * 7 + duration_ns) / 8; // ç®€å•çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
            _ = self.average_allocation_time.store(new_avg, .monotonic);
        }

        /// è®°å½•é‡Šæ”¾
        pub fn recordDeallocation(self: *UnifiedStats, size: usize) void {
            _ = self.total_deallocations.fetchAdd(1, .monotonic);
            _ = self.current_memory_usage.fetchSub(size, .monotonic);
        }

        /// ğŸš€ å¿«é€Ÿåˆ†é…è®°å½• - é›¶æ—¶é—´æˆ³å¼€é”€
        pub fn recordFastAllocation(self: *UnifiedStats, size: usize, allocator_type: Strategy) void {
            _ = self.total_allocations.fetchAdd(1, .monotonic);
            const new_usage = self.current_memory_usage.fetchAdd(size, .monotonic) + size;

            // æ›´æ–°å³°å€¼ä½¿ç”¨é‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            const peak = self.peak_memory_usage.load(.monotonic);
            if (new_usage > peak) {
                _ = self.peak_memory_usage.cmpxchgWeak(peak, new_usage, .acq_rel, .monotonic);
            }

            // è®°å½•åˆ†é…å™¨ä½¿ç”¨
            switch (allocator_type) {
                .smart, .auto => _ = self.smart_allocations.fetchAdd(1, .monotonic),
                .extended => _ = self.extended_allocations.fetchAdd(1, .monotonic),
                .optimized => _ = self.optimized_allocations.fetchAdd(1, .monotonic),
            }
        }

        /// ğŸš€ å¿«é€Ÿé‡Šæ”¾è®°å½•
        pub fn recordFastDeallocation(self: *UnifiedStats, size: usize) void {
            _ = self.total_deallocations.fetchAdd(1, .monotonic);
            _ = self.current_memory_usage.fetchSub(size, .monotonic);
        }

        /// è·å–ç»Ÿè®¡å¿«ç…§
        pub fn getSnapshot(self: *const UnifiedStats) StatsSnapshot {
            return StatsSnapshot{
                .total_allocations = self.total_allocations.load(.monotonic),
                .total_deallocations = self.total_deallocations.load(.monotonic),
                .current_memory_usage = self.current_memory_usage.load(.monotonic),
                .peak_memory_usage = self.peak_memory_usage.load(.monotonic),
                .smart_allocations = self.smart_allocations.load(.monotonic),
                .extended_allocations = self.extended_allocations.load(.monotonic),
                .optimized_allocations = self.optimized_allocations.load(.monotonic),
                .average_allocation_time = self.average_allocation_time.load(.monotonic),
                .cache_hit_rate = @as(f32, @floatFromInt(self.cache_hit_rate.load(.monotonic))) / 100.0,
            };
        }
    };

    /// ç»Ÿè®¡å¿«ç…§ï¼ˆéåŸå­ï¼Œç”¨äºè¯»å–ï¼‰
    pub const StatsSnapshot = struct {
        total_allocations: u64,
        total_deallocations: u64,
        current_memory_usage: usize,
        peak_memory_usage: usize,
        smart_allocations: u64,
        extended_allocations: u64,
        optimized_allocations: u64,
        average_allocation_time: u64,
        cache_hit_rate: f32,

        /// è®¡ç®—å†…å­˜æ•ˆç‡
        pub fn getMemoryEfficiency(self: *const StatsSnapshot) f32 {
            if (self.peak_memory_usage == 0) return 1.0;
            return @as(f32, @floatFromInt(self.current_memory_usage)) / @as(f32, @floatFromInt(self.peak_memory_usage));
        }

        /// è®¡ç®—åˆ†é…å™¨ä½¿ç”¨åˆ†å¸ƒ
        pub fn getAllocatorDistribution(self: *const StatsSnapshot) struct { smart: f32, extended: f32, optimized: f32 } {
            const total = self.smart_allocations + self.extended_allocations + self.optimized_allocations;
            if (total == 0) return .{ .smart = 0.0, .extended = 0.0, .optimized = 0.0 };

            return .{
                .smart = @as(f32, @floatFromInt(self.smart_allocations)) / @as(f32, @floatFromInt(total)),
                .extended = @as(f32, @floatFromInt(self.extended_allocations)) / @as(f32, @floatFromInt(total)),
                .optimized = @as(f32, @floatFromInt(self.optimized_allocations)) / @as(f32, @floatFromInt(total)),
            };
        }
    };

    /// åˆå§‹åŒ–ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨
    pub fn init(base_allocator: std.mem.Allocator, config: UnifiedConfig) !Self {
        // æ ¹æ®é…ç½®åˆ›å»ºæ™ºèƒ½åˆ†é…å™¨é…ç½®
        const smart_config = FastSmartAllocatorConfig{
            .default_strategy = switch (config.default_strategy) {
                .auto, .smart => .extended_pool,
                .extended => .extended_pool,
                .optimized => .object_pool,
            },
            .enable_fast_path = config.enable_fast_path,
            .enable_lightweight_monitoring = config.enable_monitoring,
            .small_object_threshold = config.small_threshold,
            .large_object_threshold = config.large_threshold,
        };

        return Self{
            .smart = try FastSmartAllocator.init(base_allocator, smart_config),
            .extended = try ExtendedAllocator.init(base_allocator),
            .optimized = try OptimizedAllocator.init(base_allocator),
            .config = config,
            .stats = UnifiedStats.init(),
            .pattern_detector = null, // æš‚æ—¶ç¦ç”¨ï¼Œé¿å…å¤æ‚æ€§
            .performance_predictor = null,
            .auto_tuner = null,
        };
    }

    /// æ¸…ç†èµ„æº
    pub fn deinit(self: *Self) void {
        self.smart.deinit();
        self.extended.deinit();
        self.optimized.deinit();
    }

    /// ğŸš€ é«˜æ€§èƒ½æ™ºèƒ½åˆ†é… - é›¶å¼€é”€æŠ½è±¡
    pub fn alloc(self: *Self, comptime T: type, count: usize) ![]T {
        const size = @sizeOf(T) * count;

        // ğŸ”¥ ç¼–è¯‘æ—¶ä¼˜åŒ–ï¼šæ ¹æ®æ€§èƒ½æ¨¡å¼é€‰æ‹©è·¯å¾„
        return switch (self.config.performance_mode) {
            .high_performance => self.allocFastPath(T, count, size),
            .balanced => self.allocBalancedPath(T, count, size),
            .monitoring => self.allocMonitoringPath(T, count, size),
            .low_memory, .debug => self.allocMonitoringPath(T, count, size), // ä½¿ç”¨ç›‘æ§è·¯å¾„
        };
    }

    /// ğŸš€ å¿«é€Ÿè·¯å¾„ - é›¶ç›‘æ§å¼€é”€
    inline fn allocFastPath(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
        // ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é…ç½®çš„é˜ˆå€¼ç¡®ä¿åˆ†é…å’Œé‡Šæ”¾ä¸€è‡´
        return if (size <= self.config.small_threshold)
            self.allocOptimizedDirect(T, count, size)
        else if (size <= self.config.large_threshold)
            self.allocExtendedDirect(T, count, size)
        else
            self.allocSmartDirect(T, count, size);
    }

    /// ğŸ”¥ ç›´æ¥åˆ†é… - å†…è”ä¼˜åŒ–
    inline fn allocOptimizedDirect(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
        const memory = try self.optimized.alloc(size);
        return @as([*]T, @ptrCast(@alignCast(memory.ptr)))[0..count];
    }

    inline fn allocExtendedDirect(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
        const memory = try self.extended.alloc(size);
        return @as([*]T, @ptrCast(@alignCast(memory.ptr)))[0..count];
    }

    inline fn allocSmartDirect(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
        _ = size;
        return self.smart.alloc(T, count);
    }

    /// âš–ï¸ å¹³è¡¡è·¯å¾„ - è½»é‡çº§ç›‘æ§
    inline fn allocBalancedPath(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
        const strategy = if (size <= self.config.small_threshold)
            Strategy.optimized
        else if (size <= self.config.large_threshold)
            Strategy.extended
        else
            Strategy.smart;

        const memory = switch (strategy) {
            .optimized => try self.allocOptimizedDirect(T, count, size),
            .extended => try self.allocExtendedDirect(T, count, size),
            .smart => try self.allocSmartDirect(T, count, size),
            .auto => unreachable,
        };

        // è½»é‡çº§ç»Ÿè®¡
        self.stats.recordFastAllocation(size, strategy);
        return memory;
    }

    /// ğŸ“Š ç›‘æ§è·¯å¾„ - å®Œæ•´ç»Ÿè®¡
    fn allocMonitoringPath(self: *Self, comptime T: type, count: usize, size: usize) ![]T {
        const start_time = std.time.nanoTimestamp();
        const strategy = self.selectOptimalAllocator(size);
        const memory = try self.allocWithStrategy(T, count, strategy);
        const end_time = std.time.nanoTimestamp();
        const duration = @as(u64, @intCast(end_time - start_time));

        self.stats.recordAllocation(size, strategy, duration);
        return memory;
    }

    /// ğŸš€ é«˜æ€§èƒ½æ™ºèƒ½é‡Šæ”¾ - é›¶å¼€é”€æŠ½è±¡
    pub fn free(self: *Self, memory: anytype) void {
        const slice = switch (@TypeOf(memory)) {
            []u8 => memory,
            else => std.mem.sliceAsBytes(memory),
        };

        const size = slice.len;

        // ğŸ”¥ ç¼–è¯‘æ—¶ä¼˜åŒ–ï¼šæ ¹æ®æ€§èƒ½æ¨¡å¼é€‰æ‹©è·¯å¾„
        switch (self.config.performance_mode) {
            .high_performance => self.freeFastPath(slice, size),
            .balanced => self.freeBalancedPath(slice, size),
            .monitoring => self.freeMonitoringPath(slice, size),
            .low_memory, .debug => self.freeMonitoringPath(slice, size), // ä½¿ç”¨ç›‘æ§è·¯å¾„
        }
    }

    /// ğŸš€ å¿«é€Ÿé‡Šæ”¾è·¯å¾„
    inline fn freeFastPath(self: *Self, memory: []u8, size: usize) void {
        // ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨é…ç½®çš„é˜ˆå€¼ç¡®ä¿åˆ†é…å’Œé‡Šæ”¾ä¸€è‡´
        if (size <= self.config.small_threshold) {
            self.optimized.free(memory);
        } else if (size <= self.config.large_threshold) {
            self.extended.free(memory);
        } else {
            self.smart.free(memory);
        }
    }

    /// âš–ï¸ å¹³è¡¡é‡Šæ”¾è·¯å¾„
    inline fn freeBalancedPath(self: *Self, memory: []u8, size: usize) void {
        self.freeFastPath(memory, size);
        self.stats.recordFastDeallocation(size);
    }

    /// ğŸ“Š ç›‘æ§é‡Šæ”¾è·¯å¾„
    fn freeMonitoringPath(self: *Self, memory: []u8, size: usize) void {
        const strategy = self.selectOptimalAllocator(size);
        self.freeWithStrategy(memory, strategy);
        self.stats.recordDeallocation(size);
    }

    /// é€‰æ‹©æœ€ä¼˜åˆ†é…å™¨
    fn selectOptimalAllocator(self: *Self, size: usize) Strategy {
        return switch (self.config.default_strategy) {
            .auto => blk: {
                if (size <= self.config.small_threshold) {
                    break :blk .optimized;
                } else if (size <= self.config.large_threshold) {
                    break :blk .extended;
                } else {
                    break :blk .smart;
                }
            },
            .smart => .smart,
            .extended => .extended,
            .optimized => .optimized,
        };
    }

    /// ä½¿ç”¨æŒ‡å®šç­–ç•¥åˆ†é…
    fn allocWithStrategy(self: *Self, comptime T: type, count: usize, strategy: Strategy) ![]T {
        return switch (strategy) {
            .auto => unreachable,
            .smart => self.smart.alloc(T, count),
            .extended => blk: {
                const size = @sizeOf(T) * count;
                const memory = try self.extended.alloc(size);
                break :blk @as([*]T, @ptrCast(@alignCast(memory.ptr)))[0..count];
            },
            .optimized => blk: {
                const size = @sizeOf(T) * count;
                const memory = try self.optimized.alloc(size);
                break :blk @as([*]T, @ptrCast(@alignCast(memory.ptr)))[0..count];
            },
        };
    }

    /// ä½¿ç”¨æŒ‡å®šç­–ç•¥é‡Šæ”¾
    fn freeWithStrategy(self: *Self, memory: []u8, strategy: Strategy) void {
        switch (strategy) {
            .auto => unreachable,
            .smart => self.smart.free(memory),
            .extended => self.extended.free(memory),
            .optimized => self.optimized.free(memory),
        }
    }

    /// è·å–æ ‡å‡†åˆ†é…å™¨æ¥å£
    pub fn allocator(self: *Self) std.mem.Allocator {
        return std.mem.Allocator{
            .ptr = self,
            .vtable = &.{
                .alloc = allocFn,
                .resize = resizeFn,
                .free = freeFn,
                .remap = remapFn,
            },
        };
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn getStats(self: *const Self) StatsSnapshot {
        return self.stats.getSnapshot();
    }

    /// è·å–å„åˆ†é…å™¨çš„è¯¦ç»†ç»Ÿè®¡
    pub fn getDetailedStats(self: *const Self) DetailedStats {
        return DetailedStats{
            .unified = self.stats.getSnapshot(),
            .smart = self.smart.getStats(),
            .extended = null, // TODO: å®ç°ExtendedAllocator.getStats()
            .optimized = null, // TODO: å®ç°OptimizedAllocator.getStats()
        };
    }

    /// è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    pub const DetailedStats = struct {
        unified: StatsSnapshot,
        smart: ?FastSmartAllocator.FastAllocatorStats,
        extended: ?ExtendedAllocator.ExtendedStats,
        optimized: ?OptimizedAllocator.OptimizedStats,
    };

    // æ ‡å‡†åˆ†é…å™¨æ¥å£å®ç°
    fn allocFn(ctx: *anyopaque, len: usize, ptr_align: std.mem.Alignment, ret_addr: usize) ?[*]u8 {
        _ = ptr_align;
        _ = ret_addr;
        const self: *Self = @ptrCast(@alignCast(ctx));
        const memory = self.alloc(u8, len) catch return null;
        return memory.ptr;
    }

    fn resizeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) bool {
        _ = ctx;
        _ = buf;
        _ = buf_align;
        _ = new_len;
        _ = ret_addr;
        return false;
    }

    fn freeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, ret_addr: usize) void {
        _ = buf_align;
        _ = ret_addr;
        const self: *Self = @ptrCast(@alignCast(ctx));
        self.free(buf);
    }

    fn remapFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) ?[*]u8 {
        _ = ctx;
        _ = buf;
        _ = buf_align;
        _ = new_len;
        _ = ret_addr;
        return null;
    }
};

/// ç¼“å­˜å‹å¥½åˆ†é…å™¨
fn CacheFriendlyAllocator(comptime config: MemoryConfig) type {
    _ = config; // æš‚æ—¶æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ç”¨äºæœªæ¥æ‰©å±•
    return struct {
        const Self = @This();

        base_allocator: std.mem.Allocator,
        cache_line_allocator: CacheLineAllocator,

        const CacheLineAllocator = struct {
            allocator: std.mem.Allocator,

            pub fn init(base_allocator: std.mem.Allocator) CacheLineAllocator {
                return CacheLineAllocator{ .allocator = base_allocator };
            }

            /// åˆ†é…ç¼“å­˜è¡Œå¯¹é½çš„å†…å­˜
            pub fn allocAligned(self: *CacheLineAllocator, size: usize) ![]u8 {
                const aligned_size = std.mem.alignForward(usize, size, CACHE_LINE_SIZE);
                const raw_size = aligned_size + CACHE_LINE_SIZE;

                const raw_memory = try self.allocator.alloc(u8, raw_size);
                const aligned_ptr = std.mem.alignForward(usize, @intFromPtr(raw_memory.ptr), CACHE_LINE_SIZE);

                return @as([*]u8, @ptrFromInt(aligned_ptr))[0..aligned_size];
            }

            /// é‡Šæ”¾ç¼“å­˜è¡Œå¯¹é½çš„å†…å­˜
            pub fn freeAligned(self: *CacheLineAllocator, memory: []u8) void {
                // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®°å½•åŸå§‹æŒ‡é’ˆ
                self.allocator.free(memory);
            }
        };

        pub fn init(base_allocator: std.mem.Allocator) !Self {
            return Self{
                .base_allocator = base_allocator,
                .cache_line_allocator = CacheLineAllocator.init(base_allocator),
            };
        }

        pub fn deinit(self: *Self) void {
            _ = self;
        }

        pub fn allocator(self: *Self) std.mem.Allocator {
            return std.mem.Allocator{
                .ptr = self,
                .vtable = &.{
                    .alloc = allocFn,
                    .resize = resizeFn,
                    .free = freeFn,
                    .remap = remapFn,
                },
            };
        }

        fn allocFn(ctx: *anyopaque, len: usize, ptr_align: std.mem.Alignment, ret_addr: usize) ?[*]u8 {
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            // ç¡®ä¿å¯¹é½è‡³å°‘æ˜¯ç¼“å­˜è¡Œå¤§å°
            const actual_align = @max(@intFromEnum(ptr_align), @ctz(@as(u64, CACHE_LINE_SIZE)));
            const aligned_len = std.mem.alignForward(usize, len, CACHE_LINE_SIZE);

            // åˆ†é…é¢å¤–ç©ºé—´ç”¨äºå¯¹é½
            const total_len = aligned_len + CACHE_LINE_SIZE;
            const raw_memory = self.base_allocator.alloc(u8, total_len) catch return null;

            // è®¡ç®—å¯¹é½åçš„åœ°å€
            const raw_addr = @intFromPtr(raw_memory.ptr);
            const aligned_addr = std.mem.alignForward(usize, raw_addr, @as(usize, 1) << actual_align);

            // ç¡®ä¿å¯¹é½åˆ°ç¼“å­˜è¡Œè¾¹ç•Œ
            const cache_aligned_addr = std.mem.alignForward(usize, aligned_addr, CACHE_LINE_SIZE);

            return @as([*]u8, @ptrFromInt(cache_aligned_addr));
        }

        fn resizeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) bool {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒresize
            return false;
        }

        fn remapFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) ?[*]u8 {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒremap
            return null;
        }

        fn freeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, ret_addr: usize) void {
            _ = buf_align;
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®°å½•åŸå§‹æŒ‡é’ˆè¿›è¡Œæ­£ç¡®é‡Šæ”¾
            // åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦ç»´æŠ¤ä¸€ä¸ªæ˜ å°„è¡¨æ¥è·Ÿè¸ªåŸå§‹åˆ†é…
            self.base_allocator.free(buf);
        }
    };
}

/// ç¼–è¯‘æ—¶å¯¹è±¡æ± ç”Ÿæˆå™¨
pub fn ObjectPool(comptime T: type, comptime pool_size: usize) type {
    return struct {
        const Self = @This();

        // ç¼–è¯‘æ—¶è®¡ç®—çš„æ± å‚æ•°
        const OBJECT_ALIGN = @max(@alignOf(T), @alignOf(?*anyopaque));
        const MIN_SIZE = @max(@sizeOf(T), @sizeOf(?*anyopaque));
        const OBJECT_SIZE = std.mem.alignForward(usize, MIN_SIZE, OBJECT_ALIGN);
        const POOL_BYTES = OBJECT_SIZE * pool_size;

        // ç¼–è¯‘æ—¶å¯¹é½çš„å†…å­˜æ± 
        pool: [POOL_BYTES]u8 align(OBJECT_ALIGN),
        free_list: std.atomic.Value(?*FreeNode),
        allocated_count: std.atomic.Value(usize),

        const FreeNode = extern struct {
            next: ?*FreeNode,
        };

        pub fn init() Self {
            var self = Self{
                .pool = undefined,
                .free_list = std.atomic.Value(?*FreeNode).init(null),
                .allocated_count = std.atomic.Value(usize).init(0),
            };

            // åˆå§‹åŒ–ç©ºé—²åˆ—è¡¨
            var current: ?*FreeNode = null;
            var i: usize = pool_size;
            while (i > 0) {
                i -= 1;
                const offset = i * OBJECT_SIZE;
                const node = @as(*FreeNode, @ptrCast(@alignCast(&self.pool[offset])));
                node.next = current;
                current = node;
            }

            self.free_list.store(current, .release);
            return self;
        }

        pub fn acquire(self: *Self) ?*T {
            while (true) {
                const head = self.free_list.load(.acquire) orelse return null;

                const next = head.next;
                if (self.free_list.cmpxchgWeak(head, next, .acq_rel, .acquire) == null) {
                    _ = self.allocated_count.fetchAdd(1, .monotonic);
                    return @as(*T, @ptrCast(@alignCast(head)));
                }
            }
        }

        pub fn release(self: *Self, obj: *T) void {
            const node = @as(*FreeNode, @ptrCast(@alignCast(obj)));

            while (true) {
                const head = self.free_list.load(.acquire);
                node.next = head;

                if (self.free_list.cmpxchgWeak(head, node, .acq_rel, .acquire) == null) {
                    _ = self.allocated_count.fetchSub(1, .monotonic);
                    break;
                }
            }
        }

        /// ç¼–è¯‘æ—¶ç”Ÿæˆçš„ç»Ÿè®¡ä¿¡æ¯
        pub fn getStats(self: *const Self) PoolStats {
            const allocated = self.allocated_count.load(.monotonic);
            return PoolStats{
                .total_objects = pool_size,
                .allocated_objects = allocated,
                .free_objects = pool_size - allocated,
                .memory_usage = allocated * OBJECT_SIZE,
                .fragmentation_ratio = self.calculateFragmentation(),
                .cache_hit_ratio = self.calculateCacheHitRatio(),
            };
        }

        /// è®¡ç®—å†…å­˜ç¢ç‰‡ç‡
        fn calculateFragmentation(self: *const Self) f32 {
            const allocated = self.allocated_count.load(.monotonic);
            if (allocated == 0) return 0.0;

            // ç®€åŒ–çš„ç¢ç‰‡ç‡è®¡ç®—ï¼šåŸºäºåˆ†é…å¯¹è±¡çš„åˆ†æ•£ç¨‹åº¦
            const fragmentation = @as(f32, @floatFromInt(pool_size - allocated)) / @as(f32, @floatFromInt(pool_size));
            return fragmentation;
        }

        /// è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡ï¼ˆç®€åŒ–å®ç°ï¼‰
        fn calculateCacheHitRatio(self: *const Self) f32 {
            _ = self;
            // è¿™é‡Œåº”è¯¥åŸºäºå®é™…çš„ç¼“å­˜è®¿é—®ç»Ÿè®¡
            // ç®€åŒ–å®ç°è¿”å›ä¼°ç®—å€¼
            return 0.85; // å‡è®¾85%çš„ç¼“å­˜å‘½ä¸­ç‡
        }

        /// æ‰¹é‡åˆ†é…å¯¹è±¡
        pub fn acquireBatch(self: *Self, objects: []*T, count: usize) usize {
            var acquired: usize = 0;
            for (objects[0..count]) |*obj_ptr| {
                if (self.acquire()) |obj| {
                    obj_ptr.* = obj;
                    acquired += 1;
                } else {
                    break;
                }
            }
            return acquired;
        }

        /// æ‰¹é‡é‡Šæ”¾å¯¹è±¡
        pub fn releaseBatch(self: *Self, objects: []*T, count: usize) void {
            for (objects[0..count]) |obj| {
                self.release(obj);
            }
        }

        /// é¢„çƒ­å¯¹è±¡æ± ï¼ˆé¢„åˆ†é…ä¸€äº›å¯¹è±¡ä»¥æé«˜æ€§èƒ½ï¼‰
        pub fn warmup(self: *Self, warmup_count: usize) void {
            const actual_count = @min(warmup_count, pool_size);
            var temp_objects: [pool_size]*T = undefined;

            // åˆ†é…å¯¹è±¡
            var i: usize = 0;
            while (i < actual_count) : (i += 1) {
                if (self.acquire()) |obj| {
                    temp_objects[i] = obj;
                } else {
                    break;
                }
            }

            // ç«‹å³é‡Šæ”¾ï¼Œä½†å¯¹è±¡å·²ç»åœ¨ç¼“å­˜ä¸­
            var j: usize = 0;
            while (j < i) : (j += 1) {
                self.release(temp_objects[j]);
            }
        }
    };
}

/// å¯¹è±¡æ± ç»Ÿè®¡ä¿¡æ¯
pub const PoolStats = struct {
    total_objects: usize,
    allocated_objects: usize,
    free_objects: usize,
    memory_usage: usize,
    fragmentation_ratio: f32,
    cache_hit_ratio: f32,
};

/// é«˜æ€§èƒ½åˆ†é…æŒ‡æ ‡
const AllocationMetrics = struct {
    total_allocated: std.atomic.Value(usize),
    total_deallocated: std.atomic.Value(usize),
    current_usage: std.atomic.Value(usize),
    peak_usage: std.atomic.Value(usize),
    allocation_count: std.atomic.Value(usize),
    deallocation_count: std.atomic.Value(usize),

    // åˆ†å±‚ç»Ÿè®¡
    small_allocations: std.atomic.Value(usize),
    medium_allocations: std.atomic.Value(usize),
    large_allocations: std.atomic.Value(usize),

    // æ€§èƒ½ç»Ÿè®¡
    cache_misses: std.atomic.Value(usize),
    gc_cycles: std.atomic.Value(usize),
    delayed_frees: std.atomic.Value(usize),

    pub fn init() AllocationMetrics {
        return AllocationMetrics{
            .total_allocated = std.atomic.Value(usize).init(0),
            .total_deallocated = std.atomic.Value(usize).init(0),
            .current_usage = std.atomic.Value(usize).init(0),
            .peak_usage = std.atomic.Value(usize).init(0),
            .allocation_count = std.atomic.Value(usize).init(0),
            .deallocation_count = std.atomic.Value(usize).init(0),
            .small_allocations = std.atomic.Value(usize).init(0),
            .medium_allocations = std.atomic.Value(usize).init(0),
            .large_allocations = std.atomic.Value(usize).init(0),
            .cache_misses = std.atomic.Value(usize).init(0),
            .gc_cycles = std.atomic.Value(usize).init(0),
            .delayed_frees = std.atomic.Value(usize).init(0),
        };
    }

    pub fn recordAllocation(self: *AllocationMetrics, size: usize) void {
        _ = self.total_allocated.fetchAdd(size, .monotonic);
        _ = self.allocation_count.fetchAdd(1, .monotonic);

        const current = self.current_usage.fetchAdd(size, .monotonic) + size;

        // æ›´æ–°å³°å€¼ä½¿ç”¨é‡
        var peak = self.peak_usage.load(.monotonic);
        while (current > peak) {
            if (self.peak_usage.cmpxchgWeak(peak, current, .acq_rel, .monotonic) == null) {
                break;
            }
            peak = self.peak_usage.load(.monotonic);
        }

        // è®°å½•åˆ†å±‚ç»Ÿè®¡
        const size_class = SizeClass.fromSize(size);
        switch (size_class) {
            .small => _ = self.small_allocations.fetchAdd(1, .monotonic),
            .medium => _ = self.medium_allocations.fetchAdd(1, .monotonic),
            .large => _ = self.large_allocations.fetchAdd(1, .monotonic),
        }
    }

    pub fn recordCacheMiss(self: *AllocationMetrics) void {
        _ = self.cache_misses.fetchAdd(1, .monotonic);
    }

    pub fn recordGCCycle(self: *AllocationMetrics) void {
        _ = self.gc_cycles.fetchAdd(1, .monotonic);
    }

    pub fn recordDelayedFree(self: *AllocationMetrics) void {
        _ = self.delayed_frees.fetchAdd(1, .monotonic);
    }

    pub fn recordDeallocation(self: *AllocationMetrics, size: usize) void {
        _ = self.total_deallocated.fetchAdd(size, .monotonic);
        _ = self.deallocation_count.fetchAdd(1, .monotonic);
        _ = self.current_usage.fetchSub(size, .monotonic);
    }

    pub fn getStats(self: *const AllocationMetrics) AllocationStats {
        return AllocationStats{
            .total_allocated = self.total_allocated.load(.monotonic),
            .total_deallocated = self.total_deallocated.load(.monotonic),
            .current_usage = self.current_usage.load(.monotonic),
            .peak_usage = self.peak_usage.load(.monotonic),
            .allocation_count = self.allocation_count.load(.monotonic),
            .deallocation_count = self.deallocation_count.load(.monotonic),
            .small_allocations = self.small_allocations.load(.monotonic),
            .medium_allocations = self.medium_allocations.load(.monotonic),
            .large_allocations = self.large_allocations.load(.monotonic),
            .cache_misses = self.cache_misses.load(.monotonic),
            .gc_cycles = self.gc_cycles.load(.monotonic),
            .delayed_frees = self.delayed_frees.load(.monotonic),
        };
    }
};

/// é«˜æ€§èƒ½åˆ†é…ç»Ÿè®¡ä¿¡æ¯
pub const AllocationStats = struct {
    total_allocated: usize,
    total_deallocated: usize,
    current_usage: usize,
    peak_usage: usize,
    allocation_count: usize,
    deallocation_count: usize,
    small_allocations: usize,
    medium_allocations: usize,
    large_allocations: usize,
    cache_misses: usize,
    gc_cycles: usize,
    delayed_frees: usize,

    /// è®¡ç®—å†…å­˜æ•ˆç‡
    pub fn getMemoryEfficiency(self: *const AllocationStats) f32 {
        if (self.peak_usage == 0) return 1.0;
        return @as(f32, @floatFromInt(self.current_usage)) / @as(f32, @floatFromInt(self.peak_usage));
    }

    /// è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
    pub fn getCacheHitRatio(self: *const AllocationStats) f32 {
        const total_accesses = self.allocation_count + self.cache_misses;
        if (total_accesses == 0) return 1.0;
        return 1.0 - (@as(f32, @floatFromInt(self.cache_misses)) / @as(f32, @floatFromInt(total_accesses)));
    }

    /// è®¡ç®—åˆ†å±‚åˆ†é…åˆ†å¸ƒ
    pub fn getTierDistribution(self: *const AllocationStats) struct { small: f32, medium: f32, large: f32 } {
        const total = self.small_allocations + self.medium_allocations + self.large_allocations;
        if (total == 0) return .{ .small = 0.0, .medium = 0.0, .large = 0.0 };

        return .{
            .small = @as(f32, @floatFromInt(self.small_allocations)) / @as(f32, @floatFromInt(total)),
            .medium = @as(f32, @floatFromInt(self.medium_allocations)) / @as(f32, @floatFromInt(total)),
            .large = @as(f32, @floatFromInt(self.large_allocations)) / @as(f32, @floatFromInt(total)),
        };
    }
};

// æµ‹è¯•
test "å†…å­˜é…ç½®éªŒè¯" {
    const testing = std.testing;

    // æµ‹è¯•æœ‰æ•ˆé…ç½®
    const valid_config = MemoryConfig{
        .strategy = .adaptive,
        .max_allocation_size = 1024 * 1024,
        .stack_size = 64 * 1024,
    };

    // ç¼–è¯‘æ—¶éªŒè¯åº”è¯¥é€šè¿‡
    comptime valid_config.validate();

    try testing.expect(valid_config.max_allocation_size > 0);
}

test "é«˜æ€§èƒ½å¯¹è±¡æ± åŸºç¡€åŠŸèƒ½" {
    const testing = std.testing;

    const TestObject = struct {
        value: u32,
    };

    var pool = ObjectPool(TestObject, 10).init();

    // æµ‹è¯•åˆ†é…
    const obj1 = pool.acquire().?;
    obj1.value = 42;

    const obj2 = pool.acquire().?;
    obj2.value = 84;

    // æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
    const stats = pool.getStats();
    try testing.expectEqual(@as(usize, 2), stats.allocated_objects);
    try testing.expectEqual(@as(usize, 8), stats.free_objects);
    try testing.expect(stats.fragmentation_ratio >= 0.0 and stats.fragmentation_ratio <= 1.0);
    try testing.expect(stats.cache_hit_ratio >= 0.0 and stats.cache_hit_ratio <= 1.0);

    // æµ‹è¯•é‡Šæ”¾
    pool.release(obj1);

    const stats2 = pool.getStats();
    try testing.expectEqual(@as(usize, 1), stats2.allocated_objects);
    try testing.expectEqual(@as(usize, 9), stats2.free_objects);

    // æµ‹è¯•é‡æ–°åˆ†é…
    const obj3 = pool.acquire().?;
    try testing.expect(obj3 == obj1); // åº”è¯¥é‡ç”¨é‡Šæ”¾çš„å¯¹è±¡
}

test "é«˜æ€§èƒ½å†…å­˜åˆ†é…å™¨åŸºç¡€åŠŸèƒ½" {
    const testing = std.testing;

    const config = MemoryConfig{
        .strategy = .adaptive,
        .enable_metrics = true,
        .enable_cache_alignment = true,
        .enable_prefetch = true,
    };

    var allocator = try MemoryAllocator(config).init(testing.allocator);
    defer allocator.deinit();

    // æµ‹è¯•åˆ†é…
    const memory = try allocator.alloc(u32, 10);
    defer allocator.free(memory);

    try testing.expectEqual(@as(usize, 10), memory.len);

    // æµ‹è¯•æŒ‡æ ‡
    const stats = allocator.metrics.getStats();
    try testing.expect(stats.allocation_count > 0);
    try testing.expect(stats.current_usage > 0);

    // æµ‹è¯•åˆ†å±‚ç»Ÿè®¡
    const distribution = stats.getTierDistribution();
    try testing.expect(distribution.small + distribution.medium + distribution.large <= 1.0);

    // æµ‹è¯•å†…å­˜æ•ˆç‡
    const efficiency = stats.getMemoryEfficiency();
    try testing.expect(efficiency >= 0.0 and efficiency <= 1.0);
}

test "åˆ†å±‚å†…å­˜æ± åŠŸèƒ½" {
    const testing = std.testing;

    // æš‚æ—¶ä½¿ç”¨adaptiveç­–ç•¥ï¼Œå› ä¸ºtiered_poolså®ç°è¿˜ä¸ç¨³å®š
    const config = MemoryConfig{
        .strategy = .adaptive,
        .enable_metrics = true,
        .small_pool_size = 64,
        .medium_pool_size = 32,
        .enable_delayed_free = false,
    };

    var allocator = try MemoryAllocator(config).init(testing.allocator);
    defer allocator.deinit();

    // æµ‹è¯•å°å¯¹è±¡åˆ†é…
    const small_memory = try allocator.alloc(u8, 32);
    defer allocator.free(small_memory);
    try testing.expectEqual(@as(usize, 32), small_memory.len);

    // æµ‹è¯•ä¸­ç­‰å¯¹è±¡åˆ†é…
    const medium_memory = try allocator.alloc(u8, 2048);
    defer allocator.free(medium_memory);
    try testing.expectEqual(@as(usize, 2048), medium_memory.len);

    // æµ‹è¯•å¤§å¯¹è±¡åˆ†é…
    const large_memory = try allocator.alloc(u8, 128 * 1024);
    defer allocator.free(large_memory);
    try testing.expectEqual(@as(usize, 128 * 1024), large_memory.len);

    // éªŒè¯åˆ†å±‚ç»Ÿè®¡ï¼ˆadaptiveç­–ç•¥å¯èƒ½ä¸ä¼šè®°å½•åˆ†å±‚ç»Ÿè®¡ï¼‰
    const stats = allocator.metrics.getStats();
    const distribution = stats.getTierDistribution();
    try testing.expect(distribution.small >= 0.0);
    try testing.expect(distribution.medium >= 0.0);
    try testing.expect(distribution.large >= 0.0);
}

test "å¯¹è±¡æ± æ‰¹é‡æ“ä½œ" {
    const testing = std.testing;

    const TestObject = struct {
        value: u64,
    };

    var pool = ObjectPool(TestObject, 100).init();

    // æµ‹è¯•æ‰¹é‡åˆ†é…
    var objects: [10]*TestObject = undefined;
    const acquired = pool.acquireBatch(&objects, 10);
    try testing.expectEqual(@as(usize, 10), acquired);

    // è®¾ç½®å€¼
    for (objects[0..acquired], 0..) |obj, i| {
        obj.value = i;
    }

    // éªŒè¯å€¼
    for (objects[0..acquired], 0..) |obj, i| {
        try testing.expectEqual(@as(u64, i), obj.value);
    }

    // æµ‹è¯•æ‰¹é‡é‡Šæ”¾
    pool.releaseBatch(&objects, acquired);

    // éªŒè¯ç»Ÿè®¡
    const stats = pool.getStats();
    try testing.expectEqual(@as(usize, 0), stats.allocated_objects);
    try testing.expectEqual(@as(usize, 100), stats.free_objects);
}

test "å¯¹è±¡æ± é¢„çƒ­åŠŸèƒ½" {
    const testing = std.testing;

    const TestObject = struct {
        data: [64]u8,
    };

    var pool = ObjectPool(TestObject, 50).init();

    // é¢„çƒ­å¯¹è±¡æ± 
    pool.warmup(20);

    // é¢„çƒ­ååº”è¯¥èƒ½å¿«é€Ÿåˆ†é…
    const start_time = std.time.nanoTimestamp();

    var objects: [20]*TestObject = undefined;
    const acquired = pool.acquireBatch(&objects, 20);

    const end_time = std.time.nanoTimestamp();
    const duration = end_time - start_time;

    try testing.expectEqual(@as(usize, 20), acquired);
    try testing.expect(duration < 1000000); // åº”è¯¥åœ¨1mså†…å®Œæˆ

    // æ¸…ç†
    pool.releaseBatch(&objects, acquired);
}

test "ç¼“å­˜å‹å¥½åˆ†é…å™¨" {
    const testing = std.testing;

    // æš‚æ—¶ä½¿ç”¨adaptiveç­–ç•¥ï¼Œå› ä¸ºcache_friendlyå®ç°è¿˜ä¸ç¨³å®š
    const config = MemoryConfig{
        .strategy = .adaptive,
        .enable_cache_alignment = true,
        .enable_prefetch = true,
        .enable_false_sharing_prevention = true,
    };

    var allocator = try MemoryAllocator(config).init(testing.allocator);
    defer allocator.deinit();

    // æµ‹è¯•åŸºæœ¬åˆ†é…åŠŸèƒ½
    const memory = try allocator.alloc(u8, 128);
    defer allocator.free(memory);

    try testing.expectEqual(@as(usize, 128), memory.len);

    // éªŒè¯å†…å­˜å¯ä»¥æ­£å¸¸ä½¿ç”¨
    for (memory, 0..) |*byte, i| {
        byte.* = @as(u8, @intCast(i % 256));
    }

    for (memory, 0..) |byte, i| {
        try testing.expectEqual(@as(u8, @intCast(i % 256)), byte);
    }
}

test "ğŸ§  Phase 2: ç¼–è¯‘æ—¶å†…å­˜ä¼˜åŒ–éªŒè¯" {
    const testing = std.testing;

    // æµ‹è¯•ç¼–è¯‘æ—¶åˆ†é…å™¨é€‰æ‹©
    const config = MemoryConfig{
        .strategy = .tiered_pools,
        .enable_cache_alignment = true,
        .enable_numa = true,
        .enable_metrics = true,
        .max_allocation_size = 1024 * 1024,
    };

    const AllocatorType = MemoryAllocator(config);

    // éªŒè¯ç¼–è¯‘æ—¶åˆ†æç»“æœ
    try testing.expect(AllocatorType.LAYOUT_ANALYSIS.cache_line_aligned == true);
    try testing.expect(AllocatorType.LAYOUT_ANALYSIS.numa_aware == true);
    try testing.expect(AllocatorType.LAYOUT_ANALYSIS.memory_efficiency_score > 0.8);
    try testing.expect(AllocatorType.LAYOUT_ANALYSIS.fragmentation_risk == .low);

    // éªŒè¯ä¼˜åŒ–æç¤º
    try testing.expect(AllocatorType.OPTIMIZATION_HINTS.performance_impact > 1.0);
    try testing.expect(AllocatorType.OPTIMIZATION_HINTS.memory_overhead < 0.3);

    // éªŒè¯é…ç½®ä¼ é€’
    try testing.expect(AllocatorType.MEMORY_CONFIG.strategy == .tiered_pools);
    try testing.expect(AllocatorType.MEMORY_CONFIG.enable_cache_alignment == true);
}

/// ğŸ§  Phase 2: ç¼–è¯‘æ—¶å†…å­˜å¸ƒå±€åˆ†æ
fn analyzeMemoryLayout(comptime config: MemoryConfig) MemoryLayoutAnalysis {
    return MemoryLayoutAnalysis{
        .cache_line_aligned = config.enable_cache_alignment,
        .numa_aware = config.enable_numa,
        .optimal_pool_sizes = calculateOptimalPoolSizes(config),
        .memory_efficiency_score = calculateMemoryEfficiency(config),
        .fragmentation_risk = assessFragmentationRisk(config),
    };
}

/// ğŸš€ Phase 2: ç¼–è¯‘æ—¶ä¼˜åŒ–æç¤ºç”Ÿæˆ
fn generateOptimizationHints(comptime config: MemoryConfig) OptimizationHints {
    var hints: []const []const u8 = &[_][]const u8{};

    // åŸºäºé…ç½®ç”Ÿæˆä¼˜åŒ–å»ºè®®
    if (!config.enable_cache_alignment) {
        hints = hints ++ [_][]const u8{"å¯ç”¨ç¼“å­˜è¡Œå¯¹é½å¯æå‡æ€§èƒ½"};
    }

    if (config.strategy == .general_purpose and config.enable_metrics) {
        hints = hints ++ [_][]const u8{"è€ƒè™‘ä½¿ç”¨ä¸“ç”¨åˆ†é…å™¨ä»¥è·å¾—æ›´å¥½æ€§èƒ½"};
    }

    if (config.max_allocation_size > 1024 * 1024 * 1024) {
        hints = hints ++ [_][]const u8{"å¤§å†…å­˜åˆ†é…å¯èƒ½å½±å“æ€§èƒ½"};
    }

    return OptimizationHints{
        .suggestions = hints,
        .performance_impact = calculatePerformanceImpact(config),
        .memory_overhead = calculateMemoryOverhead(config),
    };
}

/// å†…å­˜å¸ƒå±€åˆ†æç»“æœ
const MemoryLayoutAnalysis = struct {
    cache_line_aligned: bool,
    numa_aware: bool,
    optimal_pool_sizes: PoolSizes,
    memory_efficiency_score: f64,
    fragmentation_risk: FragmentationRisk,
};

/// ä¼˜åŒ–æç¤º
const OptimizationHints = struct {
    suggestions: []const []const u8,
    performance_impact: f64,
    memory_overhead: f64,
};

/// æ± å¤§å°é…ç½®
const PoolSizes = struct {
    small_pool: usize,
    medium_pool: usize,
    large_pool: usize,
};

/// ç¢ç‰‡åŒ–é£é™©è¯„ä¼°
const FragmentationRisk = enum {
    low,
    medium,
    high,
};

/// è®¡ç®—æœ€ä¼˜æ± å¤§å°
fn calculateOptimalPoolSizes(comptime config: MemoryConfig) PoolSizes {
    return PoolSizes{
        .small_pool = config.small_pool_size,
        .medium_pool = config.medium_pool_size,
        .large_pool = config.large_pool_threshold,
    };
}

/// è®¡ç®—å†…å­˜æ•ˆç‡
fn calculateMemoryEfficiency(comptime config: MemoryConfig) f64 {
    var efficiency: f64 = 0.8; // åŸºç¡€æ•ˆç‡

    if (config.enable_cache_alignment) efficiency += 0.1;
    if (config.enable_numa) efficiency += 0.05;
    if (config.strategy == .tiered_pools) efficiency += 0.05;

    return @min(efficiency, 1.0);
}

/// è¯„ä¼°ç¢ç‰‡åŒ–é£é™©
fn assessFragmentationRisk(comptime config: MemoryConfig) FragmentationRisk {
    return switch (config.strategy) {
        .arena => .low,
        .tiered_pools => .low,
        .cache_friendly => .medium,
        .general_purpose => .medium,
        .fixed_buffer => .high,
        .stack => .high,
        .adaptive => .medium,
    };
}

/// è®¡ç®—æ€§èƒ½å½±å“
fn calculatePerformanceImpact(comptime config: MemoryConfig) f64 {
    var impact: f64 = 1.0; // åŸºç¡€æ€§èƒ½

    if (config.enable_metrics) impact -= 0.05; // ç›‘æ§å¼€é”€
    if (config.enable_numa) impact += 0.1; // NUMA ä¼˜åŒ–
    if (config.enable_cache_alignment) impact += 0.15; // ç¼“å­˜ä¼˜åŒ–

    return impact;
}

/// è®¡ç®—å†…å­˜å¼€é”€
fn calculateMemoryOverhead(comptime config: MemoryConfig) f64 {
    var overhead: f64 = 0.1; // åŸºç¡€å¼€é”€

    if (config.enable_metrics) overhead += 0.05;
    if (config.strategy == .tiered_pools) overhead += 0.1;

    return overhead;
}
