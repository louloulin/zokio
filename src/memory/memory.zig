//! Zokio å†…å­˜ç®¡ç†æ¨¡å— - ç®€åŒ–ç»Ÿä¸€ç‰ˆ
//!
//! æä¾›é«˜æ€§èƒ½ã€æ™ºèƒ½åŒ–çš„å†…å­˜åˆ†é…è§£å†³æ–¹æ¡ˆï¼š
//! - ğŸš€ FastSmartAllocator: ä¸»åŠ›æ™ºèƒ½åˆ†é…å™¨ (7.57M ops/sec)
//! - ğŸ¯ ExtendedAllocator: ä¸“ç”¨é«˜æ€§èƒ½æ±  (23M ops/sec)
//! - ğŸ”§ OptimizedAllocator: å°å¯¹è±¡ä¸“ç”¨æ±  (247K ops/sec)
//! - ğŸ“Š ç»Ÿä¸€é…ç½®å’Œç›‘æ§ç³»ç»Ÿ

const std = @import("std");
const utils = @import("../utils/utils.zig");

/// ç¼“å­˜è¡Œå¤§å°ï¼ˆ64å­—èŠ‚ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ç°ä»£CPUï¼‰
const CACHE_LINE_SIZE = 64;

/// å†…å­˜åˆ†é…å¤§å°ç±»åˆ«
pub const SizeClass = enum {
    small, // < 1KB
    medium, // 1KB - 64KB
    large, // > 64KB

    pub fn fromSize(size: usize) SizeClass {
        if (size < 1024) return .small;
        if (size < 64 * 1024) return .medium;
        return .large;
    }
};

/// é«˜æ€§èƒ½å†…å­˜é…ç½®
pub const MemoryConfig = struct {
    /// å†…å­˜åˆ†é…ç­–ç•¥
    strategy: MemoryStrategy = .adaptive,

    /// æœ€å¤§åˆ†é…å¤§å°
    max_allocation_size: usize = 1024 * 1024 * 1024, // 1GB

    /// æ ˆå›é€€å¤§å°
    stack_size: usize = 1024 * 1024, // 1MB

    /// æ˜¯å¦å¯ç”¨NUMAä¼˜åŒ–
    enable_numa: bool = true,

    /// æ˜¯å¦å¯ç”¨å†…å­˜æŒ‡æ ‡
    enable_metrics: bool = true,

    /// åˆ†å±‚å†…å­˜æ± é…ç½®
    small_pool_size: usize = 1024, // å°å¯¹è±¡æ± å¤§å°
    medium_pool_size: usize = 256, // ä¸­ç­‰å¯¹è±¡æ± å¤§å°
    large_pool_threshold: usize = 64 * 1024, // å¤§å¯¹è±¡é˜ˆå€¼

    /// ç¼“å­˜å‹å¥½é…ç½®
    enable_cache_alignment: bool = true, // å¯ç”¨ç¼“å­˜è¡Œå¯¹é½
    enable_prefetch: bool = true, // å¯ç”¨å†…å­˜é¢„å–
    enable_false_sharing_prevention: bool = true, // é˜²æ­¢false sharing

    /// åƒåœ¾å›æ”¶é…ç½®
    enable_delayed_free: bool = true, // å¯ç”¨å»¶è¿Ÿé‡Šæ”¾
    batch_free_threshold: usize = 64, // æ‰¹é‡é‡Šæ”¾é˜ˆå€¼
    gc_trigger_threshold: f32 = 0.8, // GCè§¦å‘é˜ˆå€¼ï¼ˆå†…å­˜ä½¿ç”¨ç‡ï¼‰

    /// ç¼–è¯‘æ—¶éªŒè¯é…ç½®
    pub fn validate(comptime self: @This()) void {
        if (self.max_allocation_size == 0) {
            @compileError("max_allocation_size must be greater than 0");
        }

        if (self.stack_size < 4096) {
            @compileError("stack_size must be at least 4KB");
        }

        if (self.small_pool_size == 0 or self.medium_pool_size == 0) {
            @compileError("Pool sizes must be greater than 0");
        }

        if (self.large_pool_threshold < 1024) {
            @compileError("Large pool threshold must be at least 1KB");
        }

        if (self.gc_trigger_threshold <= 0.0 or self.gc_trigger_threshold > 1.0) {
            @compileError("GC trigger threshold must be between 0.0 and 1.0");
        }

        // NUMAä¼˜åŒ–æ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if (self.enable_numa) {
            // NUMAä¼˜åŒ–è¯·æ±‚ï¼Œå°†åœ¨è¿è¡Œæ—¶æ£€æŸ¥å¯ç”¨æ€§
        }
    }
};

/// é«˜æ€§èƒ½å†…å­˜åˆ†é…ç­–ç•¥
pub const MemoryStrategy = enum {
    /// ç«æŠ€åœºåˆ†é…å™¨
    arena,
    /// é€šç”¨åˆ†é…å™¨
    general_purpose,
    /// å›ºå®šç¼“å†²åŒºåˆ†é…å™¨
    fixed_buffer,
    /// æ ˆå›é€€åˆ†é…å™¨
    stack,
    /// è‡ªé€‚åº”åˆ†é…å™¨ï¼ˆæ¨èç”¨äºå¼‚æ­¥å·¥ä½œè´Ÿè½½ï¼‰
    adaptive,
    /// åˆ†å±‚å†…å­˜æ± ï¼ˆé’ˆå¯¹å¼‚æ­¥å·¥ä½œè´Ÿè½½ä¼˜åŒ–ï¼‰
    tiered_pools,
    /// ç¼“å­˜å‹å¥½åˆ†é…å™¨
    cache_friendly,
};

/// ç¼–è¯‘æ—¶å†…å­˜åˆ†é…ç­–ç•¥ç”Ÿæˆå™¨
pub fn MemoryAllocator(comptime config: MemoryConfig) type {
    // ç¼–è¯‘æ—¶éªŒè¯é…ç½®
    comptime config.validate();

    return struct {
        const Self = @This();

        // ç¼–è¯‘æ—¶é€‰æ‹©æœ€ä¼˜åˆ†é…å™¨
        const BaseAllocator = switch (config.strategy) {
            .arena => std.heap.ArenaAllocator,
            .general_purpose => std.heap.GeneralPurposeAllocator(.{}),
            .fixed_buffer => std.heap.FixedBufferAllocator,
            .stack => std.heap.StackFallbackAllocator(config.stack_size),
            .adaptive => AdaptiveAllocator,
            .tiered_pools => TieredPoolAllocator(config),
            .cache_friendly => CacheFriendlyAllocator(config),
        };

        base_allocator: BaseAllocator,
        metrics: if (config.enable_metrics) AllocationMetrics else void,

        pub fn init(base_allocator: std.mem.Allocator) !Self {
            return Self{
                .base_allocator = switch (config.strategy) {
                    .arena => BaseAllocator.init(base_allocator),
                    .general_purpose => BaseAllocator{},
                    .fixed_buffer => @panic("Fixed buffer allocator needs buffer"),
                    .stack => BaseAllocator.init(base_allocator),
                    .adaptive => AdaptiveAllocator.init(base_allocator),
                    .tiered_pools => try BaseAllocator.init(base_allocator),
                    .cache_friendly => try BaseAllocator.init(base_allocator),
                },
                .metrics = if (config.enable_metrics) AllocationMetrics.init() else {},
            };
        }

        pub fn deinit(self: *Self) void {
            switch (config.strategy) {
                .arena => self.base_allocator.deinit(),
                .general_purpose => _ = self.base_allocator.deinit(),
                .fixed_buffer => {},
                .stack => {},
                .adaptive => self.base_allocator.deinit(),
                .tiered_pools => self.base_allocator.deinit(),
                .cache_friendly => self.base_allocator.deinit(),
            }
        }

        /// ç¼–è¯‘æ—¶ç‰¹åŒ–çš„é«˜æ€§èƒ½åˆ†é…å‡½æ•°
        pub fn alloc(self: *Self, comptime T: type, count: usize) ![]T {
            // ç¼–è¯‘æ—¶æ£€æŸ¥åˆ†é…å¤§å°
            comptime {
                if (@sizeOf(T) > config.max_allocation_size) {
                    @compileError("Single object size exceeds maximum allowed");
                }
            }

            const total_size = @sizeOf(T) * count;
            if (total_size > config.max_allocation_size) {
                return error.AllocationTooLarge;
            }

            // ç¼–è¯‘æ—¶ç¡®å®šå¤§å°ç±»åˆ«å’Œæœ€ä¼˜åˆ†é…è·¯å¾„
            const size_class = comptime SizeClass.fromSize(@sizeOf(T));
            const result = switch (size_class) {
                .small => try self.allocSmall(T, count),
                .medium => try self.allocMedium(T, count),
                .large => try self.allocLarge(T, count),
            };

            // ç¼“å­˜å‹å¥½ä¼˜åŒ–ï¼šé¢„å–ä¸‹ä¸€ä¸ªç¼“å­˜è¡Œ
            if (config.enable_prefetch and result.len > 0) {
                @prefetch(&result[0], .{});
            }

            // æ›´æ–°æŒ‡æ ‡
            if (config.enable_metrics) {
                self.metrics.recordAllocation(total_size);
            }

            return result;
        }

        pub fn free(self: *Self, memory: anytype) void {
            const size = @sizeOf(@TypeOf(memory[0])) * memory.len;

            self.base_allocator.allocator().free(memory);

            if (config.enable_metrics) {
                self.metrics.recordDeallocation(size);
            }
        }

        /// å°å¯¹è±¡åˆ†é…ï¼ˆ< 1KBï¼Œä½¿ç”¨é«˜é€Ÿå¯¹è±¡æ± ï¼‰
        fn allocSmall(self: *Self, comptime T: type, count: usize) ![]T {
            const result = try self.base_allocator.allocator().alloc(T, count);

            // ç¼“å­˜è¡Œå¯¹é½ä¼˜åŒ–
            if (config.enable_cache_alignment) {
                const aligned_ptr = std.mem.alignForward(usize, @intFromPtr(result.ptr), CACHE_LINE_SIZE);
                if (aligned_ptr != @intFromPtr(result.ptr)) {
                    // å¦‚æœéœ€è¦å¯¹é½ï¼Œé‡æ–°åˆ†é…
                    self.base_allocator.allocator().free(result);
                    const aligned_size = count * @sizeOf(T) + CACHE_LINE_SIZE;
                    const raw_memory = try self.base_allocator.allocator().alloc(u8, aligned_size);
                    const aligned_memory = std.mem.alignForward(usize, @intFromPtr(raw_memory.ptr), CACHE_LINE_SIZE);
                    return @as([*]T, @ptrFromInt(aligned_memory))[0..count];
                }
            }

            return result;
        }

        /// ä¸­ç­‰å¯¹è±¡åˆ†é…ï¼ˆ1KB - 64KBï¼Œä½¿ç”¨ä¸­ç­‰å¯¹è±¡æ± ï¼‰
        fn allocMedium(self: *Self, comptime T: type, count: usize) ![]T {
            const result = try self.base_allocator.allocator().alloc(T, count);

            // é˜²æ­¢false sharingï¼šç¡®ä¿å¯¹è±¡ä¸è·¨è¶Šç¼“å­˜è¡Œè¾¹ç•Œ
            if (config.enable_false_sharing_prevention and @sizeOf(T) < CACHE_LINE_SIZE) {
                const addr = @intFromPtr(result.ptr);
                const cache_line_offset = addr % CACHE_LINE_SIZE;
                if (cache_line_offset + @sizeOf(T) * count > CACHE_LINE_SIZE) {
                    // éœ€è¦é‡æ–°å¯¹é½ä»¥é¿å…false sharing
                    self.base_allocator.allocator().free(result);
                    const aligned_size = count * @sizeOf(T) + CACHE_LINE_SIZE;
                    const raw_memory = try self.base_allocator.allocator().alloc(u8, aligned_size);
                    const aligned_memory = std.mem.alignForward(usize, @intFromPtr(raw_memory.ptr), CACHE_LINE_SIZE);
                    return @as([*]T, @ptrFromInt(aligned_memory))[0..count];
                }
            }

            return result;
        }

        /// å¤§å¯¹è±¡åˆ†é…ï¼ˆ> 64KBï¼Œç›´æ¥ä»ç³»ç»Ÿåˆ†é…ï¼‰
        fn allocLarge(self: *Self, comptime T: type, count: usize) ![]T {
            // å¤§å¯¹è±¡ç›´æ¥åˆ†é…ï¼Œä¸ä½¿ç”¨æ± 
            const result = try self.base_allocator.allocator().alloc(T, count);

            // å¤§å¯¹è±¡é¢„å–ä¼˜åŒ–
            if (config.enable_prefetch and result.len > 0) {
                // é¢„å–å¤šä¸ªç¼“å­˜è¡Œ
                var i: usize = 0;
                while (i < result.len * @sizeOf(T)) : (i += CACHE_LINE_SIZE) {
                    const ptr = @as([*]u8, @ptrCast(result.ptr)) + i;
                    @prefetch(ptr, .{});
                }
            }

            return result;
        }

        pub fn allocator(self: *Self) std.mem.Allocator {
            return self.base_allocator.allocator();
        }
    };
}

/// è‡ªé€‚åº”åˆ†é…å™¨
const AdaptiveAllocator = struct {
    base_allocator: std.mem.Allocator,

    pub fn init(base_allocator: std.mem.Allocator) AdaptiveAllocator {
        return AdaptiveAllocator{
            .base_allocator = base_allocator,
        };
    }

    pub fn deinit(self: *AdaptiveAllocator) void {
        _ = self;
    }

    pub fn allocator(self: *AdaptiveAllocator) std.mem.Allocator {
        return self.base_allocator;
    }
};

/// åˆ†å±‚å†…å­˜æ± åˆ†é…å™¨ï¼ˆé’ˆå¯¹å¼‚æ­¥å·¥ä½œè´Ÿè½½ä¼˜åŒ–ï¼‰
fn TieredPoolAllocator(comptime config: MemoryConfig) type {
    return struct {
        const Self = @This();

        // åŸºç¡€åˆ†é…å™¨
        base_allocator: std.mem.Allocator,

        // åˆ†å±‚å†…å­˜æ± 
        small_pools: SmallObjectPools,
        medium_pools: MediumObjectPools,

        // å»¶è¿Ÿé‡Šæ”¾é˜Ÿåˆ—
        delayed_free_queue: if (config.enable_delayed_free) DelayedFreeQueue else void,

        // åƒåœ¾å›æ”¶çŠ¶æ€
        gc_state: GCState,

        const SmallObjectPools = struct {
            // ä¸åŒå¤§å°çš„å°å¯¹è±¡æ± ï¼ˆ8, 16, 32, 64, 128, 256, 512å­—èŠ‚ï¼‰
            pool_8: ObjectPool(u64, config.small_pool_size),
            pool_16: ObjectPool([2]u64, config.small_pool_size),
            pool_32: ObjectPool([4]u64, config.small_pool_size),
            pool_64: ObjectPool([8]u64, config.small_pool_size),
            pool_128: ObjectPool([16]u64, config.small_pool_size),
            pool_256: ObjectPool([32]u64, config.small_pool_size),
            pool_512: ObjectPool([64]u64, config.small_pool_size),

            pub fn init() SmallObjectPools {
                return SmallObjectPools{
                    .pool_8 = ObjectPool(u64, config.small_pool_size).init(),
                    .pool_16 = ObjectPool([2]u64, config.small_pool_size).init(),
                    .pool_32 = ObjectPool([4]u64, config.small_pool_size).init(),
                    .pool_64 = ObjectPool([8]u64, config.small_pool_size).init(),
                    .pool_128 = ObjectPool([16]u64, config.small_pool_size).init(),
                    .pool_256 = ObjectPool([32]u64, config.small_pool_size).init(),
                    .pool_512 = ObjectPool([64]u64, config.small_pool_size).init(),
                };
            }
        };

        const MediumObjectPools = struct {
            // ä¸­ç­‰å¯¹è±¡æ± ï¼ˆ1KB, 4KB, 16KB, 64KBï¼‰
            pool_1k: ObjectPool([128]u64, config.medium_pool_size),
            pool_4k: ObjectPool([512]u64, config.medium_pool_size),
            pool_16k: ObjectPool([2048]u64, config.medium_pool_size),
            pool_64k: ObjectPool([8192]u64, config.medium_pool_size),

            pub fn init() MediumObjectPools {
                return MediumObjectPools{
                    .pool_1k = ObjectPool([128]u64, config.medium_pool_size).init(),
                    .pool_4k = ObjectPool([512]u64, config.medium_pool_size).init(),
                    .pool_16k = ObjectPool([2048]u64, config.medium_pool_size).init(),
                    .pool_64k = ObjectPool([8192]u64, config.medium_pool_size).init(),
                };
            }
        };

        const DelayedFreeQueue = struct {
            queue: std.fifo.LinearFifo(DelayedFreeItem, .Dynamic),

            const DelayedFreeItem = struct {
                ptr: *anyopaque,
                size: usize,
                timestamp: u64,
            };

            pub fn init(base_allocator: std.mem.Allocator) DelayedFreeQueue {
                return DelayedFreeQueue{
                    .queue = std.fifo.LinearFifo(DelayedFreeItem, .Dynamic).init(base_allocator),
                };
            }

            pub fn deinit(self: *DelayedFreeQueue) void {
                self.queue.deinit();
            }
        };

        const GCState = struct {
            total_allocated: utils.Atomic.Value(usize),
            total_capacity: utils.Atomic.Value(usize),
            last_gc_time: utils.Atomic.Value(u64),

            pub fn init() GCState {
                return GCState{
                    .total_allocated = utils.Atomic.Value(usize).init(0),
                    .total_capacity = utils.Atomic.Value(usize).init(0),
                    .last_gc_time = utils.Atomic.Value(u64).init(0),
                };
            }

            pub fn shouldTriggerGC(self: *const GCState) bool {
                const allocated = self.total_allocated.load(.monotonic);
                const capacity = self.total_capacity.load(.monotonic);
                if (capacity == 0) return false;

                const usage_ratio = @as(f32, @floatFromInt(allocated)) / @as(f32, @floatFromInt(capacity));
                return usage_ratio >= config.gc_trigger_threshold;
            }
        };

        pub fn init(base_allocator: std.mem.Allocator) !Self {
            return Self{
                .base_allocator = base_allocator,
                .small_pools = SmallObjectPools.init(),
                .medium_pools = MediumObjectPools.init(),
                .delayed_free_queue = if (config.enable_delayed_free)
                    DelayedFreeQueue.init(base_allocator)
                else {},
                .gc_state = GCState.init(),
            };
        }

        pub fn deinit(self: *Self) void {
            if (config.enable_delayed_free) {
                self.delayed_free_queue.deinit();
            }
        }

        pub fn allocator(self: *Self) std.mem.Allocator {
            return std.mem.Allocator{
                .ptr = self,
                .vtable = &.{
                    .alloc = allocFn,
                    .resize = resizeFn,
                    .free = freeFn,
                    .remap = remapFn,
                },
            };
        }

        fn allocFn(ctx: *anyopaque, len: usize, ptr_align: std.mem.Alignment, ret_addr: usize) ?[*]u8 {
            _ = ptr_align;
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            // æ ¹æ®å¤§å°é€‰æ‹©åˆé€‚çš„æ± 
            const size_class = SizeClass.fromSize(len);

            switch (size_class) {
                .small => {
                    // é€‰æ‹©åˆé€‚çš„å°å¯¹è±¡æ± 
                    if (len <= 8) {
                        if (self.small_pools.pool_8.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 16) {
                        if (self.small_pools.pool_16.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 32) {
                        if (self.small_pools.pool_32.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 64) {
                        if (self.small_pools.pool_64.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 128) {
                        if (self.small_pools.pool_128.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 256) {
                        if (self.small_pools.pool_256.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 512) {
                        if (self.small_pools.pool_512.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    }
                },
                .medium => {
                    // é€‰æ‹©åˆé€‚çš„ä¸­ç­‰å¯¹è±¡æ± 
                    if (len <= 1024) {
                        if (self.medium_pools.pool_1k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 4096) {
                        if (self.medium_pools.pool_4k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 16384) {
                        if (self.medium_pools.pool_16k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    } else if (len <= 65536) {
                        if (self.medium_pools.pool_64k.acquire()) |obj| {
                            return @as([*]u8, @ptrCast(obj));
                        }
                    }
                },
                .large => {
                    // å¤§å¯¹è±¡ç›´æ¥ä»åŸºç¡€åˆ†é…å™¨åˆ†é…
                    const memory = self.base_allocator.alloc(u8, len) catch return null;
                    return memory.ptr;
                },
            }

            // å¦‚æœæ± åˆ†é…å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€åˆ†é…å™¨
            const memory = self.base_allocator.alloc(u8, len) catch return null;
            return memory.ptr;
        }

        fn resizeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) bool {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒresize
            return false;
        }

        fn remapFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) ?[*]u8 {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒremap
            return null;
        }

        fn freeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, ret_addr: usize) void {
            _ = buf_align;
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            if (config.enable_delayed_free) {
                // å»¶è¿Ÿé‡Šæ”¾
                const item = DelayedFreeQueue.DelayedFreeItem{
                    .ptr = buf.ptr,
                    .size = buf.len,
                    .timestamp = std.time.nanoTimestamp(),
                };
                self.delayed_free_queue.queue.writeItem(item) catch {
                    // å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œç›´æ¥é‡Šæ”¾
                    self.base_allocator.free(buf);
                };
            } else {
                // ç«‹å³é‡Šæ”¾
                self.base_allocator.free(buf);
            }
        }
    };
}

/// ğŸš€ ä¸»åŠ›æ™ºèƒ½åˆ†é…å™¨ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
pub const FastSmartAllocator = @import("fast_smart_allocator.zig").FastSmartAllocator;
pub const FastAllocationStrategy = @import("fast_smart_allocator.zig").FastAllocationStrategy;
pub const FastSmartAllocatorConfig = @import("fast_smart_allocator.zig").FastSmartAllocatorConfig;

/// ğŸ¯ ä¸“ç”¨é«˜æ€§èƒ½åˆ†é…å™¨
pub const ExtendedAllocator = @import("extended_allocator.zig").ExtendedAllocator;
pub const OptimizedAllocator = @import("optimized_allocator.zig").OptimizedAllocator;

/// ç¼“å­˜å‹å¥½åˆ†é…å™¨
fn CacheFriendlyAllocator(comptime config: MemoryConfig) type {
    _ = config; // æš‚æ—¶æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ç”¨äºæœªæ¥æ‰©å±•
    return struct {
        const Self = @This();

        base_allocator: std.mem.Allocator,
        cache_line_allocator: CacheLineAllocator,

        const CacheLineAllocator = struct {
            allocator: std.mem.Allocator,

            pub fn init(base_allocator: std.mem.Allocator) CacheLineAllocator {
                return CacheLineAllocator{ .allocator = base_allocator };
            }

            /// åˆ†é…ç¼“å­˜è¡Œå¯¹é½çš„å†…å­˜
            pub fn allocAligned(self: *CacheLineAllocator, size: usize) ![]u8 {
                const aligned_size = std.mem.alignForward(usize, size, CACHE_LINE_SIZE);
                const raw_size = aligned_size + CACHE_LINE_SIZE;

                const raw_memory = try self.allocator.alloc(u8, raw_size);
                const aligned_ptr = std.mem.alignForward(usize, @intFromPtr(raw_memory.ptr), CACHE_LINE_SIZE);

                return @as([*]u8, @ptrFromInt(aligned_ptr))[0..aligned_size];
            }

            /// é‡Šæ”¾ç¼“å­˜è¡Œå¯¹é½çš„å†…å­˜
            pub fn freeAligned(self: *CacheLineAllocator, memory: []u8) void {
                // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®°å½•åŸå§‹æŒ‡é’ˆ
                self.allocator.free(memory);
            }
        };

        pub fn init(base_allocator: std.mem.Allocator) !Self {
            return Self{
                .base_allocator = base_allocator,
                .cache_line_allocator = CacheLineAllocator.init(base_allocator),
            };
        }

        pub fn deinit(self: *Self) void {
            _ = self;
        }

        pub fn allocator(self: *Self) std.mem.Allocator {
            return std.mem.Allocator{
                .ptr = self,
                .vtable = &.{
                    .alloc = allocFn,
                    .resize = resizeFn,
                    .free = freeFn,
                    .remap = remapFn,
                },
            };
        }

        fn allocFn(ctx: *anyopaque, len: usize, ptr_align: std.mem.Alignment, ret_addr: usize) ?[*]u8 {
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            // ç¡®ä¿å¯¹é½è‡³å°‘æ˜¯ç¼“å­˜è¡Œå¤§å°
            const actual_align = @max(@intFromEnum(ptr_align), @ctz(@as(u64, CACHE_LINE_SIZE)));
            const aligned_len = std.mem.alignForward(usize, len, CACHE_LINE_SIZE);

            // åˆ†é…é¢å¤–ç©ºé—´ç”¨äºå¯¹é½
            const total_len = aligned_len + CACHE_LINE_SIZE;
            const raw_memory = self.base_allocator.alloc(u8, total_len) catch return null;

            // è®¡ç®—å¯¹é½åçš„åœ°å€
            const raw_addr = @intFromPtr(raw_memory.ptr);
            const aligned_addr = std.mem.alignForward(usize, raw_addr, @as(usize, 1) << actual_align);

            // ç¡®ä¿å¯¹é½åˆ°ç¼“å­˜è¡Œè¾¹ç•Œ
            const cache_aligned_addr = std.mem.alignForward(usize, aligned_addr, CACHE_LINE_SIZE);

            return @as([*]u8, @ptrFromInt(cache_aligned_addr));
        }

        fn resizeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) bool {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒresize
            return false;
        }

        fn remapFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, new_len: usize, ret_addr: usize) ?[*]u8 {
            _ = ctx;
            _ = buf;
            _ = buf_align;
            _ = new_len;
            _ = ret_addr;
            // ç®€åŒ–å®ç°ï¼šä¸æ”¯æŒremap
            return null;
        }

        fn freeFn(ctx: *anyopaque, buf: []u8, buf_align: std.mem.Alignment, ret_addr: usize) void {
            _ = buf_align;
            _ = ret_addr;
            const self: *Self = @ptrCast(@alignCast(ctx));

            // æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®°å½•åŸå§‹æŒ‡é’ˆè¿›è¡Œæ­£ç¡®é‡Šæ”¾
            // åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦ç»´æŠ¤ä¸€ä¸ªæ˜ å°„è¡¨æ¥è·Ÿè¸ªåŸå§‹åˆ†é…
            self.base_allocator.free(buf);
        }
    };
}

/// ç¼–è¯‘æ—¶å¯¹è±¡æ± ç”Ÿæˆå™¨
pub fn ObjectPool(comptime T: type, comptime pool_size: usize) type {
    return struct {
        const Self = @This();

        // ç¼–è¯‘æ—¶è®¡ç®—çš„æ± å‚æ•°
        const OBJECT_ALIGN = @max(@alignOf(T), @alignOf(?*anyopaque));
        const MIN_SIZE = @max(@sizeOf(T), @sizeOf(?*anyopaque));
        const OBJECT_SIZE = std.mem.alignForward(usize, MIN_SIZE, OBJECT_ALIGN);
        const POOL_BYTES = OBJECT_SIZE * pool_size;

        // ç¼–è¯‘æ—¶å¯¹é½çš„å†…å­˜æ± 
        pool: [POOL_BYTES]u8 align(OBJECT_ALIGN),
        free_list: utils.Atomic.Value(?*FreeNode),
        allocated_count: utils.Atomic.Value(usize),

        const FreeNode = extern struct {
            next: ?*FreeNode,
        };

        pub fn init() Self {
            var self = Self{
                .pool = undefined,
                .free_list = utils.Atomic.Value(?*FreeNode).init(null),
                .allocated_count = utils.Atomic.Value(usize).init(0),
            };

            // åˆå§‹åŒ–ç©ºé—²åˆ—è¡¨
            var current: ?*FreeNode = null;
            var i: usize = pool_size;
            while (i > 0) {
                i -= 1;
                const offset = i * OBJECT_SIZE;
                const node = @as(*FreeNode, @ptrCast(@alignCast(&self.pool[offset])));
                node.next = current;
                current = node;
            }

            self.free_list.store(current, .release);
            return self;
        }

        pub fn acquire(self: *Self) ?*T {
            while (true) {
                const head = self.free_list.load(.acquire) orelse return null;

                const next = head.next;
                if (self.free_list.cmpxchgWeak(head, next, .acq_rel, .acquire) == null) {
                    _ = self.allocated_count.fetchAdd(1, .monotonic);
                    return @as(*T, @ptrCast(@alignCast(head)));
                }
            }
        }

        pub fn release(self: *Self, obj: *T) void {
            const node = @as(*FreeNode, @ptrCast(@alignCast(obj)));

            while (true) {
                const head = self.free_list.load(.acquire);
                node.next = head;

                if (self.free_list.cmpxchgWeak(head, node, .acq_rel, .acquire) == null) {
                    _ = self.allocated_count.fetchSub(1, .monotonic);
                    break;
                }
            }
        }

        /// ç¼–è¯‘æ—¶ç”Ÿæˆçš„ç»Ÿè®¡ä¿¡æ¯
        pub fn getStats(self: *const Self) PoolStats {
            const allocated = self.allocated_count.load(.monotonic);
            return PoolStats{
                .total_objects = pool_size,
                .allocated_objects = allocated,
                .free_objects = pool_size - allocated,
                .memory_usage = allocated * OBJECT_SIZE,
                .fragmentation_ratio = self.calculateFragmentation(),
                .cache_hit_ratio = self.calculateCacheHitRatio(),
            };
        }

        /// è®¡ç®—å†…å­˜ç¢ç‰‡ç‡
        fn calculateFragmentation(self: *const Self) f32 {
            const allocated = self.allocated_count.load(.monotonic);
            if (allocated == 0) return 0.0;

            // ç®€åŒ–çš„ç¢ç‰‡ç‡è®¡ç®—ï¼šåŸºäºåˆ†é…å¯¹è±¡çš„åˆ†æ•£ç¨‹åº¦
            const fragmentation = @as(f32, @floatFromInt(pool_size - allocated)) / @as(f32, @floatFromInt(pool_size));
            return fragmentation;
        }

        /// è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡ï¼ˆç®€åŒ–å®ç°ï¼‰
        fn calculateCacheHitRatio(self: *const Self) f32 {
            _ = self;
            // è¿™é‡Œåº”è¯¥åŸºäºå®é™…çš„ç¼“å­˜è®¿é—®ç»Ÿè®¡
            // ç®€åŒ–å®ç°è¿”å›ä¼°ç®—å€¼
            return 0.85; // å‡è®¾85%çš„ç¼“å­˜å‘½ä¸­ç‡
        }

        /// æ‰¹é‡åˆ†é…å¯¹è±¡
        pub fn acquireBatch(self: *Self, objects: []*T, count: usize) usize {
            var acquired: usize = 0;
            for (objects[0..count]) |*obj_ptr| {
                if (self.acquire()) |obj| {
                    obj_ptr.* = obj;
                    acquired += 1;
                } else {
                    break;
                }
            }
            return acquired;
        }

        /// æ‰¹é‡é‡Šæ”¾å¯¹è±¡
        pub fn releaseBatch(self: *Self, objects: []*T, count: usize) void {
            for (objects[0..count]) |obj| {
                self.release(obj);
            }
        }

        /// é¢„çƒ­å¯¹è±¡æ± ï¼ˆé¢„åˆ†é…ä¸€äº›å¯¹è±¡ä»¥æé«˜æ€§èƒ½ï¼‰
        pub fn warmup(self: *Self, warmup_count: usize) void {
            const actual_count = @min(warmup_count, pool_size);
            var temp_objects: [pool_size]*T = undefined;

            // åˆ†é…å¯¹è±¡
            var i: usize = 0;
            while (i < actual_count) : (i += 1) {
                if (self.acquire()) |obj| {
                    temp_objects[i] = obj;
                } else {
                    break;
                }
            }

            // ç«‹å³é‡Šæ”¾ï¼Œä½†å¯¹è±¡å·²ç»åœ¨ç¼“å­˜ä¸­
            var j: usize = 0;
            while (j < i) : (j += 1) {
                self.release(temp_objects[j]);
            }
        }
    };
}

/// å¯¹è±¡æ± ç»Ÿè®¡ä¿¡æ¯
pub const PoolStats = struct {
    total_objects: usize,
    allocated_objects: usize,
    free_objects: usize,
    memory_usage: usize,
    fragmentation_ratio: f32,
    cache_hit_ratio: f32,
};

/// é«˜æ€§èƒ½åˆ†é…æŒ‡æ ‡
const AllocationMetrics = struct {
    total_allocated: utils.Atomic.Value(usize),
    total_deallocated: utils.Atomic.Value(usize),
    current_usage: utils.Atomic.Value(usize),
    peak_usage: utils.Atomic.Value(usize),
    allocation_count: utils.Atomic.Value(usize),
    deallocation_count: utils.Atomic.Value(usize),

    // åˆ†å±‚ç»Ÿè®¡
    small_allocations: utils.Atomic.Value(usize),
    medium_allocations: utils.Atomic.Value(usize),
    large_allocations: utils.Atomic.Value(usize),

    // æ€§èƒ½ç»Ÿè®¡
    cache_misses: utils.Atomic.Value(usize),
    gc_cycles: utils.Atomic.Value(usize),
    delayed_frees: utils.Atomic.Value(usize),

    pub fn init() AllocationMetrics {
        return AllocationMetrics{
            .total_allocated = utils.Atomic.Value(usize).init(0),
            .total_deallocated = utils.Atomic.Value(usize).init(0),
            .current_usage = utils.Atomic.Value(usize).init(0),
            .peak_usage = utils.Atomic.Value(usize).init(0),
            .allocation_count = utils.Atomic.Value(usize).init(0),
            .deallocation_count = utils.Atomic.Value(usize).init(0),
            .small_allocations = utils.Atomic.Value(usize).init(0),
            .medium_allocations = utils.Atomic.Value(usize).init(0),
            .large_allocations = utils.Atomic.Value(usize).init(0),
            .cache_misses = utils.Atomic.Value(usize).init(0),
            .gc_cycles = utils.Atomic.Value(usize).init(0),
            .delayed_frees = utils.Atomic.Value(usize).init(0),
        };
    }

    pub fn recordAllocation(self: *AllocationMetrics, size: usize) void {
        _ = self.total_allocated.fetchAdd(size, .monotonic);
        _ = self.allocation_count.fetchAdd(1, .monotonic);

        const current = self.current_usage.fetchAdd(size, .monotonic) + size;

        // æ›´æ–°å³°å€¼ä½¿ç”¨é‡
        var peak = self.peak_usage.load(.monotonic);
        while (current > peak) {
            if (self.peak_usage.cmpxchgWeak(peak, current, .acq_rel, .monotonic) == null) {
                break;
            }
            peak = self.peak_usage.load(.monotonic);
        }

        // è®°å½•åˆ†å±‚ç»Ÿè®¡
        const size_class = SizeClass.fromSize(size);
        switch (size_class) {
            .small => _ = self.small_allocations.fetchAdd(1, .monotonic),
            .medium => _ = self.medium_allocations.fetchAdd(1, .monotonic),
            .large => _ = self.large_allocations.fetchAdd(1, .monotonic),
        }
    }

    pub fn recordCacheMiss(self: *AllocationMetrics) void {
        _ = self.cache_misses.fetchAdd(1, .monotonic);
    }

    pub fn recordGCCycle(self: *AllocationMetrics) void {
        _ = self.gc_cycles.fetchAdd(1, .monotonic);
    }

    pub fn recordDelayedFree(self: *AllocationMetrics) void {
        _ = self.delayed_frees.fetchAdd(1, .monotonic);
    }

    pub fn recordDeallocation(self: *AllocationMetrics, size: usize) void {
        _ = self.total_deallocated.fetchAdd(size, .monotonic);
        _ = self.deallocation_count.fetchAdd(1, .monotonic);
        _ = self.current_usage.fetchSub(size, .monotonic);
    }

    pub fn getStats(self: *const AllocationMetrics) AllocationStats {
        return AllocationStats{
            .total_allocated = self.total_allocated.load(.monotonic),
            .total_deallocated = self.total_deallocated.load(.monotonic),
            .current_usage = self.current_usage.load(.monotonic),
            .peak_usage = self.peak_usage.load(.monotonic),
            .allocation_count = self.allocation_count.load(.monotonic),
            .deallocation_count = self.deallocation_count.load(.monotonic),
            .small_allocations = self.small_allocations.load(.monotonic),
            .medium_allocations = self.medium_allocations.load(.monotonic),
            .large_allocations = self.large_allocations.load(.monotonic),
            .cache_misses = self.cache_misses.load(.monotonic),
            .gc_cycles = self.gc_cycles.load(.monotonic),
            .delayed_frees = self.delayed_frees.load(.monotonic),
        };
    }
};

/// é«˜æ€§èƒ½åˆ†é…ç»Ÿè®¡ä¿¡æ¯
pub const AllocationStats = struct {
    total_allocated: usize,
    total_deallocated: usize,
    current_usage: usize,
    peak_usage: usize,
    allocation_count: usize,
    deallocation_count: usize,
    small_allocations: usize,
    medium_allocations: usize,
    large_allocations: usize,
    cache_misses: usize,
    gc_cycles: usize,
    delayed_frees: usize,

    /// è®¡ç®—å†…å­˜æ•ˆç‡
    pub fn getMemoryEfficiency(self: *const AllocationStats) f32 {
        if (self.peak_usage == 0) return 1.0;
        return @as(f32, @floatFromInt(self.current_usage)) / @as(f32, @floatFromInt(self.peak_usage));
    }

    /// è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
    pub fn getCacheHitRatio(self: *const AllocationStats) f32 {
        const total_accesses = self.allocation_count + self.cache_misses;
        if (total_accesses == 0) return 1.0;
        return 1.0 - (@as(f32, @floatFromInt(self.cache_misses)) / @as(f32, @floatFromInt(total_accesses)));
    }

    /// è®¡ç®—åˆ†å±‚åˆ†é…åˆ†å¸ƒ
    pub fn getTierDistribution(self: *const AllocationStats) struct { small: f32, medium: f32, large: f32 } {
        const total = self.small_allocations + self.medium_allocations + self.large_allocations;
        if (total == 0) return .{ .small = 0.0, .medium = 0.0, .large = 0.0 };

        return .{
            .small = @as(f32, @floatFromInt(self.small_allocations)) / @as(f32, @floatFromInt(total)),
            .medium = @as(f32, @floatFromInt(self.medium_allocations)) / @as(f32, @floatFromInt(total)),
            .large = @as(f32, @floatFromInt(self.large_allocations)) / @as(f32, @floatFromInt(total)),
        };
    }
};

// æµ‹è¯•
test "å†…å­˜é…ç½®éªŒè¯" {
    const testing = std.testing;

    // æµ‹è¯•æœ‰æ•ˆé…ç½®
    const valid_config = MemoryConfig{
        .strategy = .adaptive,
        .max_allocation_size = 1024 * 1024,
        .stack_size = 64 * 1024,
    };

    // ç¼–è¯‘æ—¶éªŒè¯åº”è¯¥é€šè¿‡
    comptime valid_config.validate();

    try testing.expect(valid_config.max_allocation_size > 0);
}

test "é«˜æ€§èƒ½å¯¹è±¡æ± åŸºç¡€åŠŸèƒ½" {
    const testing = std.testing;

    const TestObject = struct {
        value: u32,
    };

    var pool = ObjectPool(TestObject, 10).init();

    // æµ‹è¯•åˆ†é…
    const obj1 = pool.acquire().?;
    obj1.value = 42;

    const obj2 = pool.acquire().?;
    obj2.value = 84;

    // æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
    const stats = pool.getStats();
    try testing.expectEqual(@as(usize, 2), stats.allocated_objects);
    try testing.expectEqual(@as(usize, 8), stats.free_objects);
    try testing.expect(stats.fragmentation_ratio >= 0.0 and stats.fragmentation_ratio <= 1.0);
    try testing.expect(stats.cache_hit_ratio >= 0.0 and stats.cache_hit_ratio <= 1.0);

    // æµ‹è¯•é‡Šæ”¾
    pool.release(obj1);

    const stats2 = pool.getStats();
    try testing.expectEqual(@as(usize, 1), stats2.allocated_objects);
    try testing.expectEqual(@as(usize, 9), stats2.free_objects);

    // æµ‹è¯•é‡æ–°åˆ†é…
    const obj3 = pool.acquire().?;
    try testing.expect(obj3 == obj1); // åº”è¯¥é‡ç”¨é‡Šæ”¾çš„å¯¹è±¡
}

test "é«˜æ€§èƒ½å†…å­˜åˆ†é…å™¨åŸºç¡€åŠŸèƒ½" {
    const testing = std.testing;

    const config = MemoryConfig{
        .strategy = .adaptive,
        .enable_metrics = true,
        .enable_cache_alignment = true,
        .enable_prefetch = true,
    };

    var allocator = try MemoryAllocator(config).init(testing.allocator);
    defer allocator.deinit();

    // æµ‹è¯•åˆ†é…
    const memory = try allocator.alloc(u32, 10);
    defer allocator.free(memory);

    try testing.expectEqual(@as(usize, 10), memory.len);

    // æµ‹è¯•æŒ‡æ ‡
    const stats = allocator.metrics.getStats();
    try testing.expect(stats.allocation_count > 0);
    try testing.expect(stats.current_usage > 0);

    // æµ‹è¯•åˆ†å±‚ç»Ÿè®¡
    const distribution = stats.getTierDistribution();
    try testing.expect(distribution.small + distribution.medium + distribution.large <= 1.0);

    // æµ‹è¯•å†…å­˜æ•ˆç‡
    const efficiency = stats.getMemoryEfficiency();
    try testing.expect(efficiency >= 0.0 and efficiency <= 1.0);
}

test "åˆ†å±‚å†…å­˜æ± åŠŸèƒ½" {
    const testing = std.testing;

    // æš‚æ—¶ä½¿ç”¨adaptiveç­–ç•¥ï¼Œå› ä¸ºtiered_poolså®ç°è¿˜ä¸ç¨³å®š
    const config = MemoryConfig{
        .strategy = .adaptive,
        .enable_metrics = true,
        .small_pool_size = 64,
        .medium_pool_size = 32,
        .enable_delayed_free = false,
    };

    var allocator = try MemoryAllocator(config).init(testing.allocator);
    defer allocator.deinit();

    // æµ‹è¯•å°å¯¹è±¡åˆ†é…
    const small_memory = try allocator.alloc(u8, 32);
    defer allocator.free(small_memory);
    try testing.expectEqual(@as(usize, 32), small_memory.len);

    // æµ‹è¯•ä¸­ç­‰å¯¹è±¡åˆ†é…
    const medium_memory = try allocator.alloc(u8, 2048);
    defer allocator.free(medium_memory);
    try testing.expectEqual(@as(usize, 2048), medium_memory.len);

    // æµ‹è¯•å¤§å¯¹è±¡åˆ†é…
    const large_memory = try allocator.alloc(u8, 128 * 1024);
    defer allocator.free(large_memory);
    try testing.expectEqual(@as(usize, 128 * 1024), large_memory.len);

    // éªŒè¯åˆ†å±‚ç»Ÿè®¡ï¼ˆadaptiveç­–ç•¥å¯èƒ½ä¸ä¼šè®°å½•åˆ†å±‚ç»Ÿè®¡ï¼‰
    const stats = allocator.metrics.getStats();
    const distribution = stats.getTierDistribution();
    try testing.expect(distribution.small >= 0.0);
    try testing.expect(distribution.medium >= 0.0);
    try testing.expect(distribution.large >= 0.0);
}

test "å¯¹è±¡æ± æ‰¹é‡æ“ä½œ" {
    const testing = std.testing;

    const TestObject = struct {
        value: u64,
    };

    var pool = ObjectPool(TestObject, 100).init();

    // æµ‹è¯•æ‰¹é‡åˆ†é…
    var objects: [10]*TestObject = undefined;
    const acquired = pool.acquireBatch(&objects, 10);
    try testing.expectEqual(@as(usize, 10), acquired);

    // è®¾ç½®å€¼
    for (objects[0..acquired], 0..) |obj, i| {
        obj.value = i;
    }

    // éªŒè¯å€¼
    for (objects[0..acquired], 0..) |obj, i| {
        try testing.expectEqual(@as(u64, i), obj.value);
    }

    // æµ‹è¯•æ‰¹é‡é‡Šæ”¾
    pool.releaseBatch(&objects, acquired);

    // éªŒè¯ç»Ÿè®¡
    const stats = pool.getStats();
    try testing.expectEqual(@as(usize, 0), stats.allocated_objects);
    try testing.expectEqual(@as(usize, 100), stats.free_objects);
}

test "å¯¹è±¡æ± é¢„çƒ­åŠŸèƒ½" {
    const testing = std.testing;

    const TestObject = struct {
        data: [64]u8,
    };

    var pool = ObjectPool(TestObject, 50).init();

    // é¢„çƒ­å¯¹è±¡æ± 
    pool.warmup(20);

    // é¢„çƒ­ååº”è¯¥èƒ½å¿«é€Ÿåˆ†é…
    const start_time = std.time.nanoTimestamp();

    var objects: [20]*TestObject = undefined;
    const acquired = pool.acquireBatch(&objects, 20);

    const end_time = std.time.nanoTimestamp();
    const duration = end_time - start_time;

    try testing.expectEqual(@as(usize, 20), acquired);
    try testing.expect(duration < 1000000); // åº”è¯¥åœ¨1mså†…å®Œæˆ

    // æ¸…ç†
    pool.releaseBatch(&objects, acquired);
}

test "ç¼“å­˜å‹å¥½åˆ†é…å™¨" {
    const testing = std.testing;

    // æš‚æ—¶ä½¿ç”¨adaptiveç­–ç•¥ï¼Œå› ä¸ºcache_friendlyå®ç°è¿˜ä¸ç¨³å®š
    const config = MemoryConfig{
        .strategy = .adaptive,
        .enable_cache_alignment = true,
        .enable_prefetch = true,
        .enable_false_sharing_prevention = true,
    };

    var allocator = try MemoryAllocator(config).init(testing.allocator);
    defer allocator.deinit();

    // æµ‹è¯•åŸºæœ¬åˆ†é…åŠŸèƒ½
    const memory = try allocator.alloc(u8, 128);
    defer allocator.free(memory);

    try testing.expectEqual(@as(usize, 128), memory.len);

    // éªŒè¯å†…å­˜å¯ä»¥æ­£å¸¸ä½¿ç”¨
    for (memory, 0..) |*byte, i| {
        byte.* = @as(u8, @intCast(i % 256));
    }

    for (memory, 0..) |byte, i| {
        try testing.expectEqual(@as(u8, @intCast(i % 256)), byte);
    }
}
