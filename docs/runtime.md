# Zokio ç»Ÿä¸€å¼‚æ­¥è¿è¡Œæ—¶æ¶æ„è®¾è®¡

## ğŸ” å½“å‰ä»£ç åˆ†æ

### ç°æœ‰æ¶æ„é—®é¢˜

é€šè¿‡å¯¹æ•´ä¸ªä»£ç åº“çš„æ·±å…¥åˆ†æï¼Œå‘ç°ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š

1. **åŒè¿è¡Œæ—¶æ¶æ„æ··ä¹±**:
   - åŒæ—¶å­˜åœ¨`runtime.zig`çš„`ZokioRuntime`å’Œ`simple_runtime.zig`çš„`SimpleRuntime`
   - ä¸¤å¥—ä¸åŒçš„APIå’Œå®ç°è·¯å¾„
   - ç”¨æˆ·å›°æƒ‘äºé€‰æ‹©å“ªä¸ªè¿è¡Œæ—¶

2. **SimpleRuntimeçš„ä¼ªå¼‚æ­¥é—®é¢˜**:
   - `SimpleRuntime.blockOn()` åªæ˜¯ç®€å•çš„è½®è¯¢+sleep
   - æ²¡æœ‰çœŸæ­£çš„å·¥ä½œçº¿ç¨‹æ± å’Œä»»åŠ¡è°ƒåº¦å™¨
   - æ€§èƒ½æµ‹è¯•æ˜¾ç¤ºçš„é«˜æ€§èƒ½æ•°æ®æ˜¯Mockçš„

3. **I/Oç³»ç»Ÿæ¶æ„åˆ†æ•£**:
   - `io.zig`ä¸­æœ‰å¤šä¸ªåç«¯å®ç°ï¼ˆio_uringã€epollã€kqueueç­‰ï¼‰
   - ä½†éƒ½æ˜¯æ¨¡æ‹Ÿå®ç°ï¼Œæ²¡æœ‰çœŸæ­£çš„å¼‚æ­¥I/O
   - libxevé›†æˆä¸å®Œæ•´ï¼Œåªæ˜¯å¯é€‰ä¾èµ–

4. **APIä¸ä¸€è‡´**:
   - `lib.zig`ä¸­åŒæ—¶å¯¼å‡ºä¸¤å¥—è¿è¡Œæ—¶API
   - ç¤ºä¾‹å’Œæ–‡æ¡£ä¸­æ··ç”¨ä¸åŒçš„è¿è¡Œæ—¶
   - ç¼ºä¹ç»Ÿä¸€çš„ä½¿ç”¨æ¨¡å¼

## ğŸ¯ ç»Ÿä¸€è¿è¡Œæ—¶æ¶æ„è®¾è®¡

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **å•ä¸€è¿è¡Œæ—¶**: åˆ é™¤SimpleRuntimeï¼Œç»Ÿä¸€ä½¿ç”¨åŸºäºlibxevçš„çœŸå®å¼‚æ­¥è¿è¡Œæ—¶
2. **libxevä¼˜å…ˆ**: å®Œå…¨åŸºäºlibxevå®ç°I/Oï¼Œç§»é™¤æ‰€æœ‰è‡ªå®šä¹‰åç«¯
3. **Tokioå…¼å®¹**: APIå’Œæ¶æ„ä¸Tokioä¿æŒä¸€è‡´
4. **Zigä¼˜åŒ–**: å……åˆ†åˆ©ç”¨Zigçš„ç¼–è¯‘æ—¶ç‰¹æ€§

### ç»Ÿä¸€åçš„æ¶æ„

```zig
// å”¯ä¸€çš„è¿è¡Œæ—¶å…¥å£
pub const Runtime = struct {
    scheduler: MultiThreadScheduler,
    handle: Handle,
    io_driver: LibxevDriver,
    blocking_pool: BlockingPool,

    pub fn new() !Runtime {
        return Builder.new_multi_thread().enable_all().build();
    }

    pub fn spawn(self: *Runtime, future: anytype) JoinHandle(@TypeOf(future).Output) {
        return self.scheduler.spawn(future);
    }

    pub fn block_on(self: *Runtime, future: anytype) !@TypeOf(future).Output {
        return self.scheduler.block_on(&self.handle, future);
    }
};

// æ„å»ºå™¨æ¨¡å¼
pub const Builder = struct {
    worker_threads: ?usize = null,
    enable_io: bool = true,
    enable_time: bool = true,

    pub fn new_multi_thread() Builder;
    pub fn worker_threads(self: Builder, threads: usize) Builder;
    pub fn enable_all(self: Builder) Builder;
    pub fn build(self: Builder) !Runtime;
};
```

### libxevé›†æˆæ¶æ„

```zig
// ç»Ÿä¸€çš„I/Oé©±åŠ¨ - å®Œå…¨åŸºäºlibxev
pub const LibxevDriver = struct {
    loop: xev.Loop,
    thread_pool: xev.ThreadPool,
    completions: std.ArrayList(xev.Completion),

    pub fn init(allocator: Allocator) !LibxevDriver;
    pub fn submit_read(self: *LibxevDriver, fd: fd_t, buffer: []u8) !IoFuture;
    pub fn submit_write(self: *LibxevDriver, fd: fd_t, data: []const u8) !IoFuture;
    pub fn poll(self: *LibxevDriver, timeout_ms: u32) !u32;
};
```

## ğŸš€ ç»Ÿä¸€è¿è¡Œæ—¶æ”¹è¿›è®¡åˆ’

### é˜¶æ®µ1: åˆ é™¤SimpleRuntimeï¼Œç»Ÿä¸€è¿è¡Œæ—¶æ¶æ„

#### 1.1 ä»£ç é‡æ„è·¯å¾„

**åˆ é™¤çš„æ–‡ä»¶**:
- `src/runtime/simple_runtime.zig` - å®Œå…¨åˆ é™¤
- `src/io/io.zig`ä¸­çš„å¤šåç«¯å®ç° - ç®€åŒ–ä¸ºlibxevç»Ÿä¸€åç«¯

**é‡æ„çš„æ–‡ä»¶**:
- `src/runtime/runtime.zig` - é‡å‘½åä¸ºç»Ÿä¸€çš„Runtime
- `src/lib.zig` - ç§»é™¤SimpleRuntimeç›¸å…³å¯¼å‡º
- `build.zig` - ç§»é™¤SimpleRuntimeç›¸å…³é…ç½®

**æ–°å¢çš„æ–‡ä»¶**:
- `src/runtime/libxev_runtime.zig` - åŸºäºlibxevçš„ç»Ÿä¸€è¿è¡Œæ—¶
- `src/io/libxev_driver.zig` - libxev I/Oé©±åŠ¨
- `src/runtime/builder.zig` - è¿è¡Œæ—¶æ„å»ºå™¨

#### 1.2 ç»Ÿä¸€çš„Runtimeå®ç°

```zig
// src/runtime/runtime.zig - ç»Ÿä¸€çš„è¿è¡Œæ—¶å®ç°
const std = @import("std");
const xev = @import("libxev");

/// ç»Ÿä¸€çš„Zokioè¿è¡Œæ—¶ - æ›¿ä»£æ‰€æœ‰å…¶ä»–è¿è¡Œæ—¶å®ç°
pub const Runtime = struct {
    scheduler: MultiThreadScheduler,
    handle: Handle,
    io_driver: LibxevDriver,
    blocking_pool: BlockingPool,
    allocator: std.mem.Allocator,

    /// åˆ›å»ºæ–°çš„è¿è¡Œæ—¶å®ä¾‹
    pub fn new() !Runtime {
        return Builder.new_multi_thread().enable_all().build();
    }

    /// ç”Ÿæˆæ–°çš„å¼‚æ­¥ä»»åŠ¡
    pub fn spawn(self: *Runtime, future: anytype) JoinHandle(@TypeOf(future).Output) {
        return self.scheduler.spawn(future);
    }

    /// é˜»å¡æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡ç›´åˆ°å®Œæˆ
    pub fn block_on(self: *Runtime, future: anytype) !@TypeOf(future).Output {
        return self.scheduler.block_on(&self.handle, future);
    }

    /// å…³é—­è¿è¡Œæ—¶
    pub fn shutdown(self: *Runtime) void {
        self.scheduler.shutdown(&self.handle);
        self.io_driver.deinit();
        self.blocking_pool.shutdown();
    }

    /// è·å–è¿è¡Œæ—¶å¥æŸ„
    pub fn handle(self: *Runtime) *Handle {
        return &self.handle;
    }
};

/// è¿è¡Œæ—¶æ„å»ºå™¨
pub const Builder = struct {
    worker_threads: ?usize = null,
    enable_io: bool = true,
    enable_time: bool = true,
    thread_name: ?[]const u8 = null,
    thread_stack_size: ?usize = null,

    /// åˆ›å»ºå¤šçº¿ç¨‹è¿è¡Œæ—¶æ„å»ºå™¨
    pub fn new_multi_thread() Builder {
        return Builder{};
    }

    /// è®¾ç½®å·¥ä½œçº¿ç¨‹æ•°
    pub fn worker_threads(self: Builder, threads: usize) Builder {
        var new_self = self;
        new_self.worker_threads = threads;
        return new_self;
    }

    /// å¯ç”¨æ‰€æœ‰åŠŸèƒ½
    pub fn enable_all(self: Builder) Builder {
        var new_self = self;
        new_self.enable_io = true;
        new_self.enable_time = true;
        return new_self;
    }

    /// å¯ç”¨I/OåŠŸèƒ½
    pub fn enable_io(self: Builder) Builder {
        var new_self = self;
        new_self.enable_io = true;
        return new_self;
    }

    /// å¯ç”¨å®šæ—¶å™¨åŠŸèƒ½
    pub fn enable_time(self: Builder) Builder {
        var new_self = self;
        new_self.enable_time = true;
        return new_self;
    }

    /// è®¾ç½®çº¿ç¨‹åç§°
    pub fn thread_name(self: Builder, name: []const u8) Builder {
        var new_self = self;
        new_self.thread_name = name;
        return new_self;
    }

    /// æ„å»ºè¿è¡Œæ—¶å®ä¾‹
    pub fn build(self: Builder) !Runtime {
        const allocator = std.heap.page_allocator;

        // åˆ›å»ºlibxev I/Oé©±åŠ¨
        var io_driver = try LibxevDriver.init(allocator);

        // åˆ›å»ºé˜»å¡ä»»åŠ¡æ± 
        var blocking_pool = try BlockingPool.init(allocator, .{
            .max_threads = 512,
        });

        // åˆ›å»ºå¤šçº¿ç¨‹è°ƒåº¦å™¨
        const worker_count = self.worker_threads orelse std.Thread.getCpuCount() catch 4;
        var scheduler = try MultiThreadScheduler.new(
            worker_count,
            &io_driver,
            &blocking_pool,
            allocator
        );

        // åˆ›å»ºè¿è¡Œæ—¶å¥æŸ„
        var handle = Handle{
            .scheduler = &scheduler,
            .io_driver = &io_driver,
            .blocking_pool = &blocking_pool,
        };

        return Runtime{
            .scheduler = scheduler,
            .handle = handle,
            .io_driver = io_driver,
            .blocking_pool = blocking_pool,
            .allocator = allocator,
        };
    }
};

/// è¿è¡Œæ—¶å¥æŸ„ - ç”¨äºè·¨çº¿ç¨‹è®¿é—®è¿è¡Œæ—¶åŠŸèƒ½
pub const Handle = struct {
    scheduler: *MultiThreadScheduler,
    io_driver: *LibxevDriver,
    blocking_pool: *BlockingPool,

    /// ç”Ÿæˆæ–°ä»»åŠ¡
    pub fn spawn(self: *Handle, future: anytype) JoinHandle(@TypeOf(future).Output) {
        return self.scheduler.spawn(future);
    }

    /// ç”Ÿæˆé˜»å¡ä»»åŠ¡
    pub fn spawn_blocking(self: *Handle, func: anytype) JoinHandle(@TypeOf(func).ReturnType) {
        return self.blocking_pool.spawn(func);
    }
};
```

#### 1.2 å®ç°çœŸæ­£çš„å¤šçº¿ç¨‹è°ƒåº¦å™¨

```zig
// src/runtime/scheduler/multi_thread.zig - åŸºäºTokioæ¶æ„çš„è°ƒåº¦å™¨
pub const MultiThreadScheduler = struct {
    workers: []Worker,
    shared: *Shared,
    allocator: std.mem.Allocator,

    pub fn new(
        size: usize,
        io_driver: *LibxevDriver,
        blocking_pool: *BlockingPool,
        allocator: std.mem.Allocator
    ) !MultiThreadScheduler {
        // åˆ›å»ºå…±äº«çŠ¶æ€
        var shared = try allocator.create(Shared);
        shared.* = try Shared.init(size, io_driver, blocking_pool, allocator);

        // åˆ›å»ºå·¥ä½œçº¿ç¨‹
        var workers = try allocator.alloc(Worker, size);
        for (workers, 0..) |*worker, i| {
            worker.* = try Worker.init(i, shared, allocator);
        }

        // å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for (workers) |*worker| {
            try worker.start();
        }

        return MultiThreadScheduler{
            .workers = workers,
            .shared = shared,
            .allocator = allocator,
        };
    }

    pub fn spawn(self: *MultiThreadScheduler, future: anytype) JoinHandle(@TypeOf(future).Output) {
        const task = Task.new(future, self.allocator) catch unreachable;
        const join_handle = JoinHandle(@TypeOf(future).Output).new(task.id);

        // å°è¯•æ”¾å…¥å½“å‰å·¥ä½œçº¿ç¨‹çš„æœ¬åœ°é˜Ÿåˆ—
        if (self.getCurrentWorker()) |worker| {
            if (worker.core.local_queue.push(task)) {
                return join_handle;
            }
        }

        // æ”¾å…¥å…¨å±€æ³¨å…¥é˜Ÿåˆ—
        self.shared.inject_queue.push(task);
        self.shared.notify_work_available();

        return join_handle;
    }

    pub fn blockOn(self: *MultiThreadScheduler, handle: *Handle, future: anytype) !@TypeOf(future).Output {
        // è¿›å…¥è¿è¡Œæ—¶ä¸Šä¸‹æ–‡
        const _enter = handle.enter();

        // å¦‚æœåœ¨å·¥ä½œçº¿ç¨‹ä¸Šï¼Œä½¿ç”¨ç‰¹æ®Šçš„block_oné€»è¾‘
        if (self.getCurrentWorker()) |worker| {
            return worker.blockOnWorker(future);
        }

        // åœ¨éå·¥ä½œçº¿ç¨‹ä¸Šï¼Œåˆ›å»ºä¸€ä¸ªä¸“ç”¨çš„parker
        var parker = try Parker.new();
        defer parker.deinit();

        var fut = future;
        var waker = Waker.from_parker(&parker);
        var context = Context{ .waker = waker };

        while (true) {
            switch (fut.poll(&context)) {
                .ready => |result| return result,
                .pending => {
                    // è¿è¡Œä¸€äº›I/Oäº‹ä»¶
                    _ = handle.io_driver.poll(1) catch 0;

                    // åœæ³Šç­‰å¾…å”¤é†’
                    parker.park();
                },
            }
        }
    }

    pub fn shutdown(self: *MultiThreadScheduler, handle: *Handle) void {
        _ = handle;

        // å…³é—­æ³¨å…¥é˜Ÿåˆ—
        self.shared.inject_queue.close();

        // é€šçŸ¥æ‰€æœ‰å·¥ä½œçº¿ç¨‹å…³é—­
        for (self.workers) |*worker| {
            worker.shutdown();
        }

        // ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹ç»“æŸ
        for (self.workers) |*worker| {
            worker.join();
        }

        // æ¸…ç†èµ„æº
        self.shared.deinit();
        self.allocator.destroy(self.shared);
        self.allocator.free(self.workers);
    }

    fn getCurrentWorker(self: *MultiThreadScheduler) ?*Worker {
        const current_thread_id = std.Thread.getCurrentId();
        for (self.workers) |*worker| {
            if (worker.thread_id == current_thread_id) {
                return worker;
            }
        }
        return null;
    }
};
```

#### 1.3 å®ç°Workerå’ŒCore

```zig
// src/runtime/worker.zig - å·¥ä½œçº¿ç¨‹å®ç°
pub const Worker = struct {
    index: usize,
    shared: *Shared,
    core: Core,
    thread: ?std.Thread = null,
    thread_id: std.Thread.Id = undefined,
    running: std.atomic.Value(bool),
    allocator: std.mem.Allocator,

    pub fn init(index: usize, shared: *Shared, allocator: std.mem.Allocator) !Worker {
        return Worker{
            .index = index,
            .shared = shared,
            .core = try Core.init(allocator),
            .running = std.atomic.Value(bool).init(false),
            .allocator = allocator,
        };
    }

    pub fn start(self: *Worker) !void {
        self.running.store(true, .release);
        self.thread = try std.Thread.spawn(.{}, workerMain, .{self});
        self.thread_id = self.thread.?.getId();
    }

    pub fn shutdown(self: *Worker) void {
        self.running.store(false, .release);
    }

    pub fn join(self: *Worker) void {
        if (self.thread) |thread| {
            thread.join();
        }
    }

    pub fn blockOnWorker(self: *Worker, future: anytype) !@TypeOf(future).Output {
        var fut = future;
        var waker = Waker.noop(); // åœ¨å·¥ä½œçº¿ç¨‹ä¸Šä¸éœ€è¦çœŸæ­£çš„waker
        var context = Context{ .waker = waker };

        while (true) {
            switch (fut.poll(&context)) {
                .ready => |result| return result,
                .pending => {
                    // è¿è¡Œå…¶ä»–ä»»åŠ¡
                    if (self.runSomeTasks()) {
                        continue; // è¿è¡Œäº†ä¸€äº›ä»»åŠ¡ï¼Œå†æ¬¡å°è¯•
                    }

                    // å°è¯•çªƒå–å·¥ä½œ
                    if (self.stealWork()) {
                        continue;
                    }

                    // è¿è¡ŒI/Oäº‹ä»¶
                    _ = self.shared.io_driver.poll(1) catch 0;
                },
            }
        }
    }

    fn workerMain(self: *Worker) void {
        while (self.running.load(.acquire)) {
            // 1. è¿è¡ŒLIFOæ§½ä¸­çš„ä»»åŠ¡
            if (self.core.lifo_slot) |task| {
                self.core.lifo_slot = null;
                self.runTask(task);
                continue;
            }

            // 2. è¿è¡Œæœ¬åœ°é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
            if (self.core.local_queue.pop()) |task| {
                self.runTask(task);
                continue;
            }

            // 3. ä»å…¨å±€é˜Ÿåˆ—è·å–ä»»åŠ¡
            if (self.shared.inject_queue.pop()) |task| {
                self.runTask(task);
                continue;
            }

            // 4. å°è¯•çªƒå–å…¶ä»–å·¥ä½œçº¿ç¨‹çš„ä»»åŠ¡
            if (self.stealWork()) {
                continue;
            }

            // 5. è¿è¡ŒI/Oäº‹ä»¶
            const io_events = self.shared.io_driver.poll(1) catch 0;
            if (io_events > 0) {
                continue;
            }

            // 6. åœæ³Šç­‰å¾…å·¥ä½œ
            self.park();
        }
    }

    fn runTask(self: *Worker, task: *Task) void {
        // è®¾ç½®å½“å‰å·¥ä½œçº¿ç¨‹ä¸Šä¸‹æ–‡
        const old_worker = current_worker;
        current_worker = self;
        defer current_worker = old_worker;

        // è¿è¡Œä»»åŠ¡
        task.run();

        // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.core.stats.tasks_completed += 1;
    }

    fn runSomeTasks(self: *Worker) bool {
        var ran_any = false;
        var count: u32 = 0;

        // æœ€å¤šè¿è¡Œ16ä¸ªä»»åŠ¡
        while (count < 16) {
            if (self.core.local_queue.pop()) |task| {
                self.runTask(task);
                ran_any = true;
                count += 1;
            } else {
                break;
            }
        }

        return ran_any;
    }

    fn stealWork(self: *Worker) bool {
        // éšæœºé€‰æ‹©ä¸€ä¸ªå…¶ä»–å·¥ä½œçº¿ç¨‹è¿›è¡Œçªƒå–
        const target_index = self.core.rand.range(self.shared.workers.len);
        if (target_index == self.index) {
            return false;
        }

        const target_worker = &self.shared.workers[target_index];
        if (target_worker.core.local_queue.steal()) |task| {
            self.runTask(task);
            self.core.stats.tasks_stolen += 1;
            return true;
        }

        return false;
    }

    fn park(self: *Worker) void {
        // ç®€å•çš„åœæ³Šå®ç°ï¼šçŸ­æš‚ä¼‘çœ 
        std.time.sleep(1 * std.time.ns_per_ms);
    }
};

pub const Core = struct {
    tick: u32 = 0,
    lifo_slot: ?*Task = null,
    lifo_enabled: bool = true,
    local_queue: WorkStealingQueue(*Task),
    is_searching: bool = false,
    is_shutdown: bool = false,
    stats: WorkerStats,
    rand: std.rand.DefaultPrng,

    pub fn init(allocator: std.mem.Allocator) !Core {
        return Core{
            .local_queue = try WorkStealingQueue(*Task).init(allocator, 256),
            .stats = WorkerStats{},
            .rand = std.rand.DefaultPrng.init(@intCast(std.time.timestamp())),
        };
    }

    pub fn deinit(self: *Core) void {
        self.local_queue.deinit();
    }
};

pub const WorkerStats = struct {
    tasks_completed: u64 = 0,
    tasks_stolen: u64 = 0,
    park_count: u64 = 0,
    steal_attempts: u64 = 0,
};

// çº¿ç¨‹æœ¬åœ°å­˜å‚¨å½“å‰å·¥ä½œçº¿ç¨‹
threadlocal var current_worker: ?*Worker = null;

pub fn getCurrentWorker() ?*Worker {
    return current_worker;
}
```

### é˜¶æ®µ2: å®Œå…¨åŸºäºlibxevçš„I/Oé©±åŠ¨

#### 2.1 å®Œå…¨ç§»é™¤ç°æœ‰I/Oåç«¯ï¼Œç»Ÿä¸€ä½¿ç”¨libxev

libxevæ˜¯è·¨å¹³å°çš„é«˜æ€§èƒ½äº‹ä»¶å¾ªç¯åº“ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³åç«¯ï¼š
- Linux: io_uring + epoll
- macOS: kqueue
- Windows: IOCP
- å…¶ä»–: poll/select

```zig
// src/io/libxev_driver.zig - å®Œå…¨åŸºäºlibxevçš„I/Oé©±åŠ¨
const xev = @import("libxev");

pub const LibxevDriver = struct {
    loop: xev.Loop,
    thread_pool: xev.ThreadPool,
    completions: std.ArrayList(xev.Completion),
    waker_map: std.HashMap(u64, Waker),
    next_id: std.atomic.Value(u64),

    pub fn init(allocator: Allocator, config: IoConfig) !LibxevDriver {
        return LibxevDriver{
            .loop = try xev.Loop.init(.{}),
            .thread_pool = try xev.ThreadPool.init(.{ .max_threads = config.io_threads }),
            .completions = std.ArrayList(xev.Completion).init(allocator),
            .waker_map = std.HashMap(u64, Waker).init(allocator),
            .next_id = std.atomic.Value(u64).init(1),
        };
    }

    pub fn deinit(self: *LibxevDriver) void {
        self.thread_pool.deinit();
        self.loop.deinit();
        self.completions.deinit();
        self.waker_map.deinit();
    }

    /// è¿è¡Œäº‹ä»¶å¾ªç¯ï¼ˆéé˜»å¡ï¼‰
    pub fn poll(self: *LibxevDriver, timeout_ms: u32) !u32 {
        const run_mode: xev.RunMode = if (timeout_ms == 0) .no_wait else .once;
        try self.loop.run(run_mode);

        // å¤„ç†å®Œæˆçš„æ“ä½œ
        var completed: u32 = 0;
        while (self.completions.popOrNull()) |completion| {
            self.handleCompletion(completion);
            completed += 1;
        }

        return completed;
    }

    /// æäº¤å¼‚æ­¥è¯»æ“ä½œ
    pub fn submitRead(self: *LibxevDriver, fd: std.posix.fd_t, buffer: []u8, waker: Waker) !u64 {
        const id = self.next_id.fetchAdd(1, .monotonic);
        try self.waker_map.put(id, waker);

        var completion = xev.Completion{
            .op = .{
                .read = .{
                    .fd = fd,
                    .buffer = .{ .slice = buffer },
                },
            },
            .userdata = id,
            .callback = readCallback,
        };

        self.loop.add(&completion);
        return id;
    }

    /// æäº¤å¼‚æ­¥å†™æ“ä½œ
    pub fn submitWrite(self: *LibxevDriver, fd: std.posix.fd_t, buffer: []const u8, waker: Waker) !u64 {
        const id = self.next_id.fetchAdd(1, .monotonic);
        try self.waker_map.put(id, waker);

        var completion = xev.Completion{
            .op = .{
                .write = .{
                    .fd = fd,
                    .buffer = .{ .slice = @constCast(buffer) },
                },
            },
            .userdata = id,
            .callback = writeCallback,
        };

        self.loop.add(&completion);
        return id;
    }

    /// æäº¤å¼‚æ­¥è¿æ¥æ“ä½œ
    pub fn submitConnect(self: *LibxevDriver, fd: std.posix.fd_t, addr: std.net.Address, waker: Waker) !u64 {
        const id = self.next_id.fetchAdd(1, .monotonic);
        try self.waker_map.put(id, waker);

        var completion = xev.Completion{
            .op = .{
                .connect = .{
                    .fd = fd,
                    .addr = addr,
                },
            },
            .userdata = id,
            .callback = connectCallback,
        };

        self.loop.add(&completion);
        return id;
    }

    /// æäº¤å¼‚æ­¥æ¥å—æ“ä½œ
    pub fn submitAccept(self: *LibxevDriver, fd: std.posix.fd_t, waker: Waker) !u64 {
        const id = self.next_id.fetchAdd(1, .monotonic);
        try self.waker_map.put(id, waker);

        var completion = xev.Completion{
            .op = .{
                .accept = .{
                    .fd = fd,
                },
            },
            .userdata = id,
            .callback = acceptCallback,
        };

        self.loop.add(&completion);
        return id;
    }

    fn handleCompletion(self: *LibxevDriver, completion: xev.Completion) void {
        const id = completion.userdata;
        if (self.waker_map.get(id)) |waker| {
            waker.wake();
            _ = self.waker_map.remove(id);
        }
    }

    fn readCallback(userdata: ?*anyopaque, loop: *xev.Loop, completion: *xev.Completion, result: xev.Result) xev.CallbackAction {
        _ = userdata;
        _ = loop;
        _ = result;

        const driver: *LibxevDriver = @ptrFromInt(completion.userdata);
        driver.completions.append(completion.*) catch {};

        return .disarm;
    }

    fn writeCallback(userdata: ?*anyopaque, loop: *xev.Loop, completion: *xev.Completion, result: xev.Result) xev.CallbackAction {
        _ = userdata;
        _ = loop;
        _ = result;

        const driver: *LibxevDriver = @ptrFromInt(completion.userdata);
        driver.completions.append(completion.*) catch {};

        return .disarm;
    }

    fn connectCallback(userdata: ?*anyopaque, loop: *xev.Loop, completion: *xev.Completion, result: xev.Result) xev.CallbackAction {
        _ = userdata;
        _ = loop;
        _ = result;

        const driver: *LibxevDriver = @ptrFromInt(completion.userdata);
        driver.completions.append(completion.*) catch {};

        return .disarm;
    }

    fn acceptCallback(userdata: ?*anyopaque, loop: *xev.Loop, completion: *xev.Completion, result: xev.Result) xev.CallbackAction {
        _ = userdata;
        _ = loop;
        _ = result;

        const driver: *LibxevDriver = @ptrFromInt(completion.userdata);
        driver.completions.append(completion.*) catch {};

        return .disarm;
    }
};
```

#### 2.2 åŸºäºlibxevçš„ç½‘ç»œæŠ½è±¡

```zig
// src/net/tcp_stream.zig - å®Œå…¨åŸºäºlibxevçš„TCPæµ
const xev = @import("libxev");

pub const TcpStream = struct {
    fd: std.posix.fd_t,
    driver: *LibxevDriver,
    local_addr: ?std.net.Address = null,
    remote_addr: ?std.net.Address = null,

    pub fn connect(addr: std.net.Address, driver: *LibxevDriver) ConnectFuture {
        return ConnectFuture.init(addr, driver);
    }

    pub fn read(self: *TcpStream, buffer: []u8) ReadFuture {
        return ReadFuture.init(self.fd, buffer, self.driver);
    }

    pub fn write(self: *TcpStream, data: []const u8) WriteFuture {
        return WriteFuture.init(self.fd, data, self.driver);
    }

    pub fn close(self: *TcpStream) void {
        std.posix.close(self.fd);
    }

    pub fn setNodelay(self: *TcpStream, enable: bool) !void {
        const value: c_int = if (enable) 1 else 0;
        try std.posix.setsockopt(self.fd, std.posix.IPPROTO.TCP, std.posix.TCP.NODELAY, std.mem.asBytes(&value));
    }
};

pub const ConnectFuture = struct {
    addr: std.net.Address,
    driver: *LibxevDriver,
    state: State = .initial,
    fd: std.posix.fd_t = -1,
    operation_id: ?u64 = null,

    const State = enum { initial, connecting, completed, failed };

    pub fn init(addr: std.net.Address, driver: *LibxevDriver) ConnectFuture {
        return ConnectFuture{
            .addr = addr,
            .driver = driver,
        };
    }

    pub fn poll(self: *ConnectFuture, ctx: *Context) Poll(TcpStream) {
        switch (self.state) {
            .initial => {
                // åˆ›å»ºsocket
                self.fd = std.posix.socket(self.addr.any.family, std.posix.SOCK.STREAM, 0) catch {
                    self.state = .failed;
                    return .{ .ready = error.SocketCreationFailed };
                };

                // è®¾ç½®éé˜»å¡
                const flags = std.posix.fcntl(self.fd, std.posix.F.GETFL, 0) catch 0;
                _ = std.posix.fcntl(self.fd, std.posix.F.SETFL, flags | std.posix.O.NONBLOCK) catch {};

                // æäº¤è¿æ¥æ“ä½œ
                const waker = ctx.createWaker();
                self.operation_id = self.driver.submitConnect(self.fd, self.addr, waker) catch {
                    std.posix.close(self.fd);
                    self.state = .failed;
                    return .{ .ready = error.ConnectSubmitFailed };
                };

                self.state = .connecting;
                return .pending;
            },
            .connecting => {
                return .pending;
            },
            .completed => {
                const stream = TcpStream{
                    .fd = self.fd,
                    .driver = self.driver,
                    .remote_addr = self.addr,
                };
                return .{ .ready = stream };
            },
            .failed => {
                return .{ .ready = error.ConnectFailed };
            },
        }
    }
};

pub const ReadFuture = struct {
    fd: std.posix.fd_t,
    buffer: []u8,
    driver: *LibxevDriver,
    state: State = .initial,
    operation_id: ?u64 = null,
    bytes_read: usize = 0,

    const State = enum { initial, reading, completed, failed };

    pub fn init(fd: std.posix.fd_t, buffer: []u8, driver: *LibxevDriver) ReadFuture {
        return ReadFuture{
            .fd = fd,
            .buffer = buffer,
            .driver = driver,
        };
    }

    pub fn poll(self: *ReadFuture, ctx: *Context) Poll(usize) {
        switch (self.state) {
            .initial => {
                const waker = ctx.createWaker();
                self.operation_id = self.driver.submitRead(self.fd, self.buffer, waker) catch {
                    self.state = .failed;
                    return .{ .ready = error.ReadSubmitFailed };
                };

                self.state = .reading;
                return .pending;
            },
            .reading => {
                return .pending;
            },
            .completed => {
                return .{ .ready = self.bytes_read };
            },
            .failed => {
                return .{ .ready = error.ReadFailed };
            },
        }
    }
};

pub const WriteFuture = struct {
    fd: std.posix.fd_t,
    data: []const u8,
    driver: *LibxevDriver,
    state: State = .initial,
    operation_id: ?u64 = null,
    bytes_written: usize = 0,

    const State = enum { initial, writing, completed, failed };

    pub fn init(fd: std.posix.fd_t, data: []const u8, driver: *LibxevDriver) WriteFuture {
        return WriteFuture{
            .fd = fd,
            .data = data,
            .driver = driver,
        };
    }

    pub fn poll(self: *WriteFuture, ctx: *Context) Poll(usize) {
        switch (self.state) {
            .initial => {
                const waker = ctx.createWaker();
                self.operation_id = self.driver.submitWrite(self.fd, self.data, waker) catch {
                    self.state = .failed;
                    return .{ .ready = error.WriteSubmitFailed };
                };

                self.state = .writing;
                return .pending;
            },
            .writing => {
                return .pending;
            },
            .completed => {
                return .{ .ready = self.bytes_written };
            },
            .failed => {
                return .{ .ready = error.WriteFailed };
            },
        }
    }
};
```

#### 2.3 åŸºäºlibxevçš„æ–‡ä»¶I/O

```zig
// src/fs/file.zig - å®Œå…¨åŸºäºlibxevçš„æ–‡ä»¶I/O
pub const File = struct {
    fd: std.posix.fd_t,
    driver: *LibxevDriver,

    pub fn open(path: []const u8, flags: std.fs.File.OpenFlags, driver: *LibxevDriver) !File {
        const fd = try std.posix.open(path, flags, 0o644);
        return File{
            .fd = fd,
            .driver = driver,
        };
    }

    pub fn read(self: *File, buffer: []u8) ReadFuture {
        return ReadFuture.init(self.fd, buffer, self.driver);
    }

    pub fn write(self: *File, data: []const u8) WriteFuture {
        return WriteFuture.init(self.fd, data, self.driver);
    }

    pub fn close(self: *File) void {
        std.posix.close(self.fd);
    }
};
```

### é˜¶æ®µ3: çœŸå®çš„ä»»åŠ¡ç³»ç»Ÿå’Œå…±äº«çŠ¶æ€

#### 3.1 å…±äº«çŠ¶æ€å®ç°

```zig
// src/runtime/shared.zig - å·¥ä½œçº¿ç¨‹é—´çš„å…±äº«çŠ¶æ€
pub const Shared = struct {
    workers: []Worker,
    inject_queue: InjectQueue,
    idle_workers: IdleWorkers,
    owned_tasks: OwnedTasks,
    io_driver: *LibxevDriver,
    blocking_pool: *BlockingPool,
    allocator: std.mem.Allocator,
    shutdown_signal: std.atomic.Value(bool),

    pub fn init(
        worker_count: usize,
        io_driver: *LibxevDriver,
        blocking_pool: *BlockingPool,
        allocator: std.mem.Allocator
    ) !Shared {
        return Shared{
            .workers = undefined, // ç¨åè®¾ç½®
            .inject_queue = try InjectQueue.init(allocator),
            .idle_workers = try IdleWorkers.init(worker_count, allocator),
            .owned_tasks = try OwnedTasks.init(allocator),
            .io_driver = io_driver,
            .blocking_pool = blocking_pool,
            .allocator = allocator,
            .shutdown_signal = std.atomic.Value(bool).init(false),
        };
    }

    pub fn deinit(self: *Shared) void {
        self.inject_queue.deinit();
        self.idle_workers.deinit();
        self.owned_tasks.deinit();
    }

    pub fn notify_work_available(self: *Shared) void {
        // å”¤é†’ä¸€ä¸ªç©ºé—²çš„å·¥ä½œçº¿ç¨‹
        if (self.idle_workers.wake_one()) {
            // æˆåŠŸå”¤é†’äº†ä¸€ä¸ªå·¥ä½œçº¿ç¨‹
        }
    }

    pub fn is_shutdown(self: *const Shared) bool {
        return self.shutdown_signal.load(.acquire);
    }
};

/// å…¨å±€æ³¨å…¥é˜Ÿåˆ— - ç”¨äºè·¨çº¿ç¨‹ä»»åŠ¡æäº¤
pub const InjectQueue = struct {
    queue: std.atomic.Queue(*Task),
    is_closed: std.atomic.Value(bool),

    pub fn init(allocator: std.mem.Allocator) !InjectQueue {
        return InjectQueue{
            .queue = std.atomic.Queue(*Task).init(),
            .is_closed = std.atomic.Value(bool).init(false),
        };
    }

    pub fn deinit(self: *InjectQueue) void {
        // æ¸…ç†å‰©ä½™ä»»åŠ¡
        while (self.queue.get()) |node| {
            const task = node.data;
            task.cancel();
        }
    }

    pub fn push(self: *InjectQueue, task: *Task) bool {
        if (self.is_closed.load(.acquire)) {
            return false;
        }

        const node = task.queue_node();
        self.queue.put(node);
        return true;
    }

    pub fn pop(self: *InjectQueue) ?*Task {
        if (self.queue.get()) |node| {
            return node.data;
        }
        return null;
    }

    pub fn close(self: *InjectQueue) void {
        self.is_closed.store(true, .release);
    }
};

/// ç©ºé—²å·¥ä½œçº¿ç¨‹ç®¡ç†
pub const IdleWorkers = struct {
    parkers: []Parker,
    idle_mask: std.atomic.Value(u64),
    allocator: std.mem.Allocator,

    pub fn init(worker_count: usize, allocator: std.mem.Allocator) !IdleWorkers {
        const parkers = try allocator.alloc(Parker, worker_count);
        for (parkers) |*parker| {
            parker.* = try Parker.new();
        }

        return IdleWorkers{
            .parkers = parkers,
            .idle_mask = std.atomic.Value(u64).init(0),
            .allocator = allocator,
        };
    }

    pub fn deinit(self: *IdleWorkers) void {
        for (self.parkers) |*parker| {
            parker.deinit();
        }
        self.allocator.free(self.parkers);
    }

    pub fn park_worker(self: *IdleWorkers, worker_index: usize) void {
        // æ ‡è®°å·¥ä½œçº¿ç¨‹ä¸ºç©ºé—²
        const mask = @as(u64, 1) << @intCast(worker_index);
        _ = self.idle_mask.fetchOr(mask, .acq_rel);

        // åœæ³Šå·¥ä½œçº¿ç¨‹
        self.parkers[worker_index].park();

        // å–æ¶ˆç©ºé—²æ ‡è®°
        _ = self.idle_mask.fetchAnd(~mask, .acq_rel);
    }

    pub fn wake_one(self: *IdleWorkers) bool {
        const mask = self.idle_mask.load(.acquire);
        if (mask == 0) {
            return false; // æ²¡æœ‰ç©ºé—²å·¥ä½œçº¿ç¨‹
        }

        // æ‰¾åˆ°ç¬¬ä¸€ä¸ªç©ºé—²çš„å·¥ä½œçº¿ç¨‹
        const worker_index = @ctz(mask);
        self.parkers[worker_index].unpark();
        return true;
    }
};

/// ä»»åŠ¡æ‰€æœ‰æƒç®¡ç†
pub const OwnedTasks = struct {
    tasks: std.HashMap(TaskId, *Task),
    next_id: std.atomic.Value(u64),
    mutex: std.Thread.Mutex,

    pub fn init(allocator: std.mem.Allocator) !OwnedTasks {
        return OwnedTasks{
            .tasks = std.HashMap(TaskId, *Task).init(allocator),
            .next_id = std.atomic.Value(u64).init(1),
            .mutex = std.Thread.Mutex{},
        };
    }

    pub fn deinit(self: *OwnedTasks) void {
        self.mutex.lock();
        defer self.mutex.unlock();

        // å–æ¶ˆæ‰€æœ‰å‰©ä½™ä»»åŠ¡
        var iterator = self.tasks.iterator();
        while (iterator.next()) |entry| {
            entry.value_ptr.*.cancel();
        }

        self.tasks.deinit();
    }

    pub fn insert(self: *OwnedTasks, task: *Task) TaskId {
        const id = TaskId{ .id = self.next_id.fetchAdd(1, .monotonic) };
        task.id = id;

        self.mutex.lock();
        defer self.mutex.unlock();

        self.tasks.put(id, task) catch unreachable;
        return id;
    }

    pub fn remove(self: *OwnedTasks, id: TaskId) ?*Task {
        self.mutex.lock();
        defer self.mutex.unlock();

        return self.tasks.fetchRemove(id);
    }
};
```

#### 3.2 é‡æ„Taskç³»ç»Ÿ

```zig
// src/future/task.zig - çœŸå®çš„ä»»åŠ¡å®ç°
pub const TaskId = struct {
    id: u64,

    pub fn eql(self: TaskId, other: TaskId) bool {
        return self.id == other.id;
    }
};

pub const Task = struct {
    id: TaskId = TaskId{ .id = 0 },
    header: TaskHeader,
    future: *anyopaque,  // ç±»å‹æ“¦é™¤çš„Future
    vtable: *const TaskVTable,
    queue_node_storage: std.atomic.Queue(*Task).Node,

    pub fn new(comptime future: anytype, allocator: std.mem.Allocator) !*Task {
        const FutureType = @TypeOf(future);
        const TaskImpl = TaskImplFor(FutureType);

        const task_impl = try allocator.create(TaskImpl);
        task_impl.* = TaskImpl{
            .task = Task{
                .header = TaskHeader{},
                .future = &task_impl.future_storage,
                .vtable = &TaskImpl.vtable,
                .queue_node_storage = std.atomic.Queue(*Task).Node{ .data = undefined },
            },
            .future_storage = future,
        };

        task_impl.task.queue_node_storage.data = &task_impl.task;
        return &task_impl.task;
    }

    pub fn poll(self: *Task, ctx: *Context) Poll(void) {
        return self.vtable.poll(self.future, ctx);
    }

    pub fn wake(self: *Task) void {
        self.vtable.wake(self.future);
    }

    pub fn cancel(self: *Task) void {
        self.vtable.cancel(self.future);
    }

    pub fn drop(self: *Task, allocator: std.mem.Allocator) void {
        self.vtable.drop(self.future, allocator);
    }

    pub fn queue_node(self: *Task) *std.atomic.Queue(*Task).Node {
        return &self.queue_node_storage;
    }

    pub fn run(self: *Task) void {
        // åˆ›å»ºä¸Šä¸‹æ–‡
        var waker = Waker.from_task(self);
        var context = Context{ .waker = waker };

        // è½®è¯¢ä»»åŠ¡
        switch (self.poll(&context)) {
            .ready => {
                // ä»»åŠ¡å®Œæˆï¼Œä»owned_tasksä¸­ç§»é™¤
                if (getCurrentWorker()) |worker| {
                    _ = worker.shared.owned_tasks.remove(self.id);
                }

                // é€šçŸ¥JoinHandle
                self.vtable.complete(self.future);
            },
            .pending => {
                // ä»»åŠ¡æœªå®Œæˆï¼Œç­‰å¾…ä¸‹æ¬¡è°ƒåº¦
            },
        }
    }
};

pub const TaskHeader = struct {
    state: TaskState = .ready,

    const TaskState = enum {
        ready,
        running,
        completed,
        cancelled,
    };
};

pub const TaskVTable = struct {
    poll: *const fn(*anyopaque, *Context) Poll(void),
    wake: *const fn(*anyopaque) void,
    cancel: *const fn(*anyopaque) void,
    complete: *const fn(*anyopaque) void,
    drop: *const fn(*anyopaque, std.mem.Allocator) void,
};

fn TaskImplFor(comptime FutureType: type) type {
    return struct {
        const Self = @This();

        task: Task,
        future_storage: FutureType,
        join_handle_waker: ?Waker = null,
        result: ?FutureType.Output = null,

        const vtable = TaskVTable{
            .poll = poll,
            .wake = wake,
            .cancel = cancel,
            .complete = complete,
            .drop = drop,
        };

        fn poll(future_ptr: *anyopaque, ctx: *Context) Poll(void) {
            const self: *Self = @ptrCast(@alignCast(future_ptr));

            switch (self.future_storage.poll(ctx)) {
                .ready => |result| {
                    self.result = result;
                    return .ready;
                },
                .pending => return .pending,
            }
        }

        fn wake(future_ptr: *anyopaque) void {
            const self: *Self = @ptrCast(@alignCast(future_ptr));

            // å°†ä»»åŠ¡é‡æ–°è°ƒåº¦
            if (getCurrentWorker()) |worker| {
                if (worker.core.lifo_enabled and worker.core.lifo_slot == null) {
                    worker.core.lifo_slot = &self.task;
                } else {
                    _ = worker.core.local_queue.push(&self.task);
                }
            } else {
                // ä¸åœ¨å·¥ä½œçº¿ç¨‹ä¸Šï¼Œæ”¾å…¥å…¨å±€é˜Ÿåˆ—
                // è¿™éœ€è¦è®¿é—®å…¨å±€è¿è¡Œæ—¶å®ä¾‹
            }
        }

        fn cancel(future_ptr: *anyopaque) void {
            const self: *Self = @ptrCast(@alignCast(future_ptr));
            self.task.header.state = .cancelled;
        }

        fn complete(future_ptr: *anyopaque) void {
            const self: *Self = @ptrCast(@alignCast(future_ptr));
            self.task.header.state = .completed;

            // å”¤é†’JoinHandle
            if (self.join_handle_waker) |waker| {
                waker.wake();
            }
        }

        fn drop(future_ptr: *anyopaque, allocator: std.mem.Allocator) void {
            const self: *Self = @ptrCast(@alignCast(future_ptr));
            allocator.destroy(self);
        }
    };
}
```

#### 3.3 å®ç°JoinHandle

```zig
// src/future/join_handle.zig - ä»»åŠ¡å¥æŸ„
pub fn JoinHandle(comptime T: type) type {
    return struct {
        const Self = @This();

        task_id: TaskId,
        shared: *Shared,
        state: State = .running,
        result: ?T = null,

        const State = enum { running, completed, cancelled };

        pub fn new(task_id: TaskId, shared: *Shared) Self {
            return Self{
                .task_id = task_id,
                .shared = shared,
            };
        }

        pub fn poll(self: *Self, ctx: *Context) Poll(T) {
            switch (self.state) {
                .running => {
                    // æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
                    if (self.shared.owned_tasks.get(self.task_id)) |task| {
                        if (task.header.state == .completed) {
                            // è·å–ç»“æœ
                            const task_impl: *TaskImplFor(anytype) = @ptrCast(@alignCast(task.future));
                            if (task_impl.result) |result| {
                                self.result = result;
                                self.state = .completed;
                                return .{ .ready = result };
                            }
                        }
                    } else {
                        // ä»»åŠ¡å·²è¢«ç§»é™¤ï¼Œå¯èƒ½å·²å®Œæˆ
                        if (self.result) |result| {
                            return .{ .ready = result };
                        }
                    }

                    return .pending;
                },
                .completed => {
                    if (self.result) |result| {
                        return .{ .ready = result };
                    } else {
                        return .{ .ready = error.TaskCompletedWithoutResult };
                    }
                },
                .cancelled => {
                    return .{ .ready = error.TaskCancelled };
                },
            }
        }

        pub fn abort(self: *Self) void {
            if (self.shared.owned_tasks.get(self.task_id)) |task| {
                task.cancel();
                self.state = .cancelled;
            }
        }

        pub fn is_finished(self: *const Self) bool {
            return self.state != .running;
        }
    };
}
```

### é˜¶æ®µ4: å·¥ä½œçªƒå–é˜Ÿåˆ—

#### 4.1 æ”¹è¿›ç°æœ‰çš„WorkStealingQueue

åŸºäº`src/scheduler/work_stealing_queue.zig`ï¼š

```zig
// src/scheduler/work_stealing_queue.zig - æ”¹è¿›ç‰ˆæœ¬
pub fn WorkStealingQueue(comptime T: type) type {
    return struct {
        buffer: []AtomicPtr(T),
        head: AtomicUsize,
        tail: AtomicUsize,
        
        // æœ¬åœ°æ“ä½œï¼ˆæ— é”ï¼‰
        pub fn pushBack(self: *Self, item: T) bool;
        pub fn popBack(self: *Self) ?T;
        
        // è¿œç¨‹æ“ä½œï¼ˆå¯èƒ½æœ‰é”ï¼‰
        pub fn popFront(self: *Self) ?T;  // ç”¨äºçªƒå–
        pub fn steal(self: *Self) ?T;
    };
}
```

#### 4.2 å®ç°å…¨å±€æ³¨å…¥é˜Ÿåˆ—

```zig
// src/scheduler/inject_queue.zig
pub const InjectQueue = struct {
    queue: MpscQueue(Task),
    is_closed: AtomicBool,
    
    pub fn push(self: *InjectQueue, task: Task) bool;
    pub fn pop(self: *InjectQueue) ?Task;
    pub fn close(self: *InjectQueue) void;
};
```

### é˜¶æ®µ5: çœŸå®çš„å¼‚æ­¥åŸè¯­

#### 5.1 é‡æ„async_fnå’Œawait_fn

```zig
// src/future/async_fn.zig - çœŸå®ç‰ˆæœ¬
pub fn async_fn(comptime func: anytype) type {
    return struct {
        state: State = .initial,
        waker: ?Waker = null,
        
        pub fn poll(self: *Self, ctx: *Context) Poll(ReturnType) {
            switch (self.state) {
                .initial => {
                    // åˆ›å»ºçœŸå®çš„ä»»åŠ¡å¹¶è°ƒåº¦åˆ°è¿è¡Œæ—¶
                    const task = Task.new(func);
                    ctx.runtime.schedule(task);
                    self.state = .running;
                    return .pending;
                },
                .running => return .pending,
                .completed => return .{ .ready = self.result },
            }
        }
    };
}
```

#### 5.2 çœŸå®çš„await_fn

```zig
// src/future/await_fn.zig
pub fn await_fn(future: anytype) @TypeOf(future).Output {
    // åªèƒ½åœ¨async contextä¸­è°ƒç”¨
    const ctx = Context.current() orelse @panic("await_fn outside async context");
    
    var fut = future;
    while (true) {
        switch (fut.poll(ctx)) {
            .ready => |result| return result,
            .pending => {
                // çœŸæ­£çš„yieldï¼šå°†å½“å‰ä»»åŠ¡æ ‡è®°ä¸ºpendingå¹¶è®©å‡ºæ‰§è¡Œ
                ctx.yield();
            },
        }
    }
}
```

## ğŸ“Š æ€§èƒ½ç›®æ ‡

### åŸºäºçœŸå®æµ‹è¯•çš„ç›®æ ‡

æ ¹æ®çœŸå®å‹æµ‹ç»“æœï¼Œè®¾å®šåˆç†ç›®æ ‡ï¼š

| æŒ‡æ ‡ | å½“å‰æ€§èƒ½ | ç›®æ ‡æ€§èƒ½ | æ”¹è¿›å€æ•° |
|------|----------|----------|----------|
| æ–‡ä»¶I/O | 16.6K ops/sec | 50K ops/sec | 3x |
| ç½‘ç»œI/O | 159 ops/sec | 10K ops/sec | 63x |
| CPUå¯†é›†å‹ | 74M ops/sec | 100M ops/sec | 1.35x |
| æ··åˆè´Ÿè½½ | 465 ops/sec | 5K ops/sec | 11x |

### è°ƒåº¦å™¨æ€§èƒ½ç›®æ ‡

- **ä»»åŠ¡è°ƒåº¦å»¶è¿Ÿ**: < 10Î¼s
- **å·¥ä½œçªƒå–æ•ˆç‡**: > 90%
- **è´Ÿè½½å‡è¡¡**: å·¥ä½œçº¿ç¨‹åˆ©ç”¨ç‡å·®å¼‚ < 5%
- **å†…å­˜æ•ˆç‡**: æ¯ä¸ªä»»åŠ¡å¼€é”€ < 64 bytes

## ğŸ› ï¸ å®æ–½æ­¥éª¤

### ç¬¬1å‘¨: åŸºç¡€æ¶æ„é‡æ„
1. **Day 1-2**: é‡æ„SimpleRuntimeä¸ºRealRuntime
   - åˆ›å»ºBuilderæ¨¡å¼çš„è¿è¡Œæ—¶æ„å»ºå™¨
   - å®ç°åŸºç¡€çš„Handleå’ŒContextç»“æ„
   - é›†æˆlibxevä½œä¸ºå”¯ä¸€I/Oåç«¯

2. **Day 3-4**: å®ç°å¤šçº¿ç¨‹è°ƒåº¦å™¨æ¡†æ¶
   - åˆ›å»ºMultiThreadScheduleråŸºç¡€ç»“æ„
   - å®ç°Sharedå…±äº«çŠ¶æ€
   - åˆ›å»ºWorkeråŸºç¡€æ¡†æ¶

3. **Day 5-7**: å®ç°å·¥ä½œçº¿ç¨‹å’Œæ ¸å¿ƒè°ƒåº¦é€»è¾‘
   - å®ŒæˆWorkerçš„å·¥ä½œå¾ªç¯
   - å®ç°Coreçš„æœ¬åœ°é˜Ÿåˆ—ç®¡ç†
   - å®ç°åŸºç¡€çš„ä»»åŠ¡è°ƒåº¦

### ç¬¬2å‘¨: libxev I/Oé›†æˆ
1. **Day 1-2**: å®Œå…¨é›†æˆlibxev
   - å®ç°LibxevDriverä½œä¸ºå”¯ä¸€I/Oé©±åŠ¨
   - ç§»é™¤æ‰€æœ‰å…¶ä»–I/Oåç«¯ä»£ç 
   - å®ç°åŸºç¡€çš„å¼‚æ­¥I/Oæ“ä½œ

2. **Day 3-4**: å®ç°ç½‘ç»œæŠ½è±¡å±‚
   - åŸºäºlibxevå®ç°TcpStream
   - å®ç°TcpListener
   - å®ç°UDPæ”¯æŒ

3. **Day 5-7**: å®ç°æ–‡ä»¶I/Oå’Œå®šæ—¶å™¨
   - åŸºäºlibxevå®ç°å¼‚æ­¥æ–‡ä»¶I/O
   - å®ç°å®šæ—¶å™¨å’Œå»¶è¿ŸåŠŸèƒ½
   - å®ç°ä¿¡å·å¤„ç†

### ç¬¬3å‘¨: å·¥ä½œçªƒå–å’Œä»»åŠ¡ç³»ç»Ÿ
1. **Day 1-2**: å®Œå–„å·¥ä½œçªƒå–é˜Ÿåˆ—
   - ä¼˜åŒ–WorkStealingQueueå®ç°
   - å®ç°LIFOä¼˜åŒ–æ§½
   - å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡

2. **Day 3-4**: å®ç°å…¨å±€ä»»åŠ¡ç®¡ç†
   - å®ŒæˆInjectQueueå®ç°
   - å®ç°OwnedTasksä»»åŠ¡æ‰€æœ‰æƒ
   - å®ç°IdleWorkersç©ºé—²ç®¡ç†

3. **Day 5-7**: å®ç°çœŸå®çš„Taskç³»ç»Ÿ
   - å®Œæˆç±»å‹æ“¦é™¤çš„Taskå®ç°
   - å®ç°TaskVTableè™šå‡½æ•°è¡¨
   - å®ç°ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†

### ç¬¬4å‘¨: å¼‚æ­¥åŸè¯­å’Œä¸Šä¸‹æ–‡
1. **Day 1-2**: é‡æ„Futureç³»ç»Ÿ
   - å®ç°çœŸæ­£çš„async_fn
   - å®ç°çœŸæ­£çš„await_fn
   - å®ç°Contextå’ŒWakeræœºåˆ¶

2. **Day 3-4**: å®ç°JoinHandleå’Œé˜»å¡æ± 
   - å®ŒæˆJoinHandleå®ç°
   - å®ç°BlockingPoolé˜»å¡ä»»åŠ¡æ± 
   - å®ç°spawn_blockingåŠŸèƒ½

3. **Day 5-7**: å®ç°é«˜çº§å¼‚æ­¥åŸè¯­
   - å®ç°select!å®
   - å®ç°timeoutåŠŸèƒ½
   - å®ç°å¼‚æ­¥äº’æ–¥é”å’Œä¿¡å·é‡

### ç¬¬5å‘¨: ä¼˜åŒ–ã€æµ‹è¯•å’ŒéªŒè¯
1. **Day 1-2**: æ€§èƒ½ä¼˜åŒ–
   - å†…å­˜åˆ†é…ä¼˜åŒ–
   - ç¼“å­˜å‹å¥½çš„æ•°æ®ç»“æ„
   - å‡å°‘åŸå­æ“ä½œå¼€é”€

2. **Day 3-4**: å…¨é¢æµ‹è¯•
   - å•å…ƒæµ‹è¯•è¦†ç›–
   - é›†æˆæµ‹è¯•
   - å‹åŠ›æµ‹è¯•å’Œç¨³å®šæ€§æµ‹è¯•

3. **Day 5-7**: æ€§èƒ½éªŒè¯å’Œæ–‡æ¡£
   - ä¸ç›®æ ‡æ€§èƒ½å¯¹æ¯”
   - æ€§èƒ½å›å½’æµ‹è¯•
   - å®Œå–„æ–‡æ¡£å’Œç¤ºä¾‹

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### libxevé›†æˆç­–ç•¥
- **ç»Ÿä¸€åç«¯**: å®Œå…¨ä¾èµ–libxevï¼Œç§»é™¤æ‰€æœ‰è‡ªå®šä¹‰I/Oåç«¯
- **è·¨å¹³å°æ”¯æŒ**: libxevè‡ªåŠ¨é€‰æ‹©æœ€ä½³åç«¯
  - Linux: io_uring (é¦–é€‰) + epoll (å›é€€)
  - macOS: kqueue
  - Windows: IOCP
  - FreeBSD/NetBSD: kqueue
  - å…¶ä»–: poll/select
- **é›¶é…ç½®**: ç”¨æˆ·æ— éœ€å…³å¿ƒåº•å±‚I/Oæœºåˆ¶

### å†…å­˜ç®¡ç†ç­–ç•¥
- **ä»»åŠ¡å¯¹è±¡æ± **: é¢„åˆ†é…Taskå¯¹è±¡ï¼Œå‡å°‘è¿è¡Œæ—¶åˆ†é…
- **LIFOä¼˜åŒ–**: åˆ©ç”¨CPUç¼“å­˜å±€éƒ¨æ€§ï¼Œä¼˜å…ˆè¿è¡Œæœ€è¿‘æäº¤çš„ä»»åŠ¡
- **é›¶æ‹·è´I/O**: ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„ç¼“å†²åŒºï¼Œé¿å…å†…å­˜æ‹·è´
- **åˆ†ä»£åƒåœ¾å›æ”¶**: å¯¹é•¿æœŸå­˜åœ¨çš„å¯¹è±¡ä½¿ç”¨ä¸åŒçš„åˆ†é…ç­–ç•¥

### å¹¶å‘å®‰å…¨è®¾è®¡
- **æ— é”å·¥ä½œçªƒå–**: åŸºäºChase-Levç®—æ³•çš„æ— é”åŒç«¯é˜Ÿåˆ—
- **åŸå­æ“ä½œä¼˜åŒ–**:
  - ä½¿ç”¨relaxed orderingå‡å°‘åŒæ­¥å¼€é”€
  - å…³é”®è·¯å¾„ä½¿ç”¨acquire-releaseè¯­ä¹‰
  - é¿å…ä¸å¿…è¦çš„å†…å­˜å±éšœ
- **çº¿ç¨‹æœ¬åœ°å­˜å‚¨**: å‡å°‘è·¨çº¿ç¨‹æ•°æ®è®¿é—®
- **æ‰¹é‡æ“ä½œ**: å‡å°‘åŸå­æ“ä½œé¢‘ç‡

### Tokioå…¼å®¹æ€§è®¾è®¡
- **APIå…¼å®¹**: æä¾›ä¸Tokioç±»ä¼¼çš„APIæ¥å£
- **è¡Œä¸ºå…¼å®¹**: ä¿æŒç›¸åŒçš„è°ƒåº¦è¯­ä¹‰å’Œæ€§èƒ½ç‰¹å¾
- **ç”Ÿæ€å…¼å®¹**: æ”¯æŒç±»ä¼¼çš„ä¸­é—´ä»¶å’Œæ‰©å±•æ¨¡å¼

### ç¼–è¯‘æ—¶ä¼˜åŒ–
- **é›¶æˆæœ¬æŠ½è±¡**: åˆ©ç”¨Zigçš„comptimeç‰¹æ€§å®ç°é›¶å¼€é”€æŠ½è±¡
- **å†…è”ä¼˜åŒ–**: å…³é”®è·¯å¾„å‡½æ•°å¼ºåˆ¶å†…è”
- **æ­»ä»£ç æ¶ˆé™¤**: ç¼–è¯‘æ—¶ç§»é™¤æœªä½¿ç”¨çš„åŠŸèƒ½
- **ç‰¹åŒ–ä¼˜åŒ–**: ä¸ºä¸åŒçš„Futureç±»å‹ç”Ÿæˆç‰¹åŒ–ä»£ç 

## ğŸ“ˆ éªŒè¯æ–¹æ³•

### åŠŸèƒ½éªŒè¯
1. **å•å…ƒæµ‹è¯•**: è¦†ç›–ç‡ > 95%
   - æ¯ä¸ªç»„ä»¶çš„ç‹¬ç«‹æµ‹è¯•
   - è¾¹ç•Œæ¡ä»¶å’Œé”™è¯¯å¤„ç†æµ‹è¯•
   - å¹¶å‘å®‰å…¨æ€§æµ‹è¯•

2. **é›†æˆæµ‹è¯•**: ç«¯åˆ°ç«¯åœºæ™¯éªŒè¯
   - çœŸå®åº”ç”¨åœºæ™¯æ¨¡æ‹Ÿ
   - å¤šç§I/Oæ¨¡å¼ç»„åˆæµ‹è¯•
   - è·¨å¹³å°å…¼å®¹æ€§æµ‹è¯•

3. **å‹åŠ›æµ‹è¯•**: æé™æ¡ä»¶éªŒè¯
   - é«˜å¹¶å‘ä»»åŠ¡è°ƒåº¦æµ‹è¯•
   - å†…å­˜å‹åŠ›æµ‹è¯•
   - é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•

### æ€§èƒ½éªŒè¯
1. **åŸºå‡†æµ‹è¯•**: ä¸çœŸå®æ•°æ®å¯¹æ¯”
   - ä½¿ç”¨çœŸå®çš„I/Oæ“ä½œï¼ˆæ–‡ä»¶ã€ç½‘ç»œã€æ•°æ®åº“ï¼‰
   - æµ‹è¯•ä¸åŒè´Ÿè½½æ¨¡å¼ï¼ˆCPUå¯†é›†å‹ã€I/Oå¯†é›†å‹ã€æ··åˆå‹ï¼‰
   - è®°å½•è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡

2. **å¯¹æ¯”æµ‹è¯•**: ä¸Tokioæ€§èƒ½å¯¹æ¯”
   - ç›¸åŒæµ‹è¯•åœºæ™¯ä¸‹çš„æ€§èƒ½å¯¹æ¯”
   - å†…å­˜ä½¿ç”¨æ•ˆç‡å¯¹æ¯”
   - å»¶è¿Ÿå’Œååé‡å¯¹æ¯”

3. **å›å½’æµ‹è¯•**: æ€§èƒ½å›å½’æ£€æµ‹
   - è‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•æµæ°´çº¿
   - æ€§èƒ½æŒ‡æ ‡è¶‹åŠ¿ç›‘æ§
   - æ€§èƒ½å›å½’è‡ªåŠ¨å‘Šè­¦

### æ­£ç¡®æ€§éªŒè¯
1. **å¹¶å‘æ­£ç¡®æ€§**:
   - ä½¿ç”¨ThreadSanitizeræ£€æµ‹ç«æ€æ¡ä»¶
   - ä½¿ç”¨AddressSanitizeræ£€æµ‹å†…å­˜é”™è¯¯
   - ä½¿ç”¨Valgrindæ£€æµ‹å†…å­˜æ³„æ¼

2. **å½¢å¼åŒ–éªŒè¯**:
   - å…³é”®ç®—æ³•çš„æ•°å­¦è¯æ˜
   - ä¸å˜é‡æ£€æŸ¥
   - çŠ¶æ€æœºéªŒè¯

3. **æ¨¡ç³Šæµ‹è¯•**:
   - éšæœºè¾“å…¥æµ‹è¯•
   - å¼‚å¸¸æƒ…å†µæ¨¡æ‹Ÿ
   - è¾¹ç•Œæ¡ä»¶æ¢ç´¢

### å®é™…åº”ç”¨éªŒè¯
1. **ç¤ºä¾‹åº”ç”¨**: æ„å»ºçœŸå®çš„åº”ç”¨ç¨‹åº
   - HTTPæœåŠ¡å™¨
   - æ•°æ®åº“è¿æ¥æ± 
   - æ¶ˆæ¯é˜Ÿåˆ—å®¢æˆ·ç«¯

2. **ç”Ÿäº§ç¯å¢ƒæµ‹è¯•**:
   - åœ¨çœŸå®è´Ÿè½½ä¸‹è¿è¡Œ
   - ç›‘æ§å…³é”®æŒ‡æ ‡
   - æ”¶é›†ç”¨æˆ·åé¦ˆ

## ğŸ¯ æˆåŠŸæ ‡å‡†

### åŠŸèƒ½å®Œæ•´æ€§ (æƒé‡: 25%)
- âœ… æ”¯æŒæ‰€æœ‰è®¡åˆ’çš„å¼‚æ­¥åŸè¯­ (async_fn, await_fn, spawn, join)
- âœ… å®Œæ•´çš„I/OæŠ½è±¡ (TCP, UDP, æ–‡ä»¶, å®šæ—¶å™¨)
- âœ… é«˜çº§å¹¶å‘åŸè¯­ (select, timeout, äº’æ–¥é”, ä¿¡å·é‡)
- âœ… é”™è¯¯å¤„ç†å’Œèµ„æºç®¡ç†

### æ€§èƒ½è¾¾æ ‡ (æƒé‡: 35%)
- âœ… **æ–‡ä»¶I/O**: ä»16.6Kæå‡åˆ°50K ops/sec (3xæ”¹è¿›)
- âœ… **ç½‘ç»œI/O**: ä»159æå‡åˆ°10K ops/sec (63xæ”¹è¿›)
- âœ… **CPUå¯†é›†å‹**: ä»74Mæå‡åˆ°100M ops/sec (1.35xæ”¹è¿›)
- âœ… **æ··åˆè´Ÿè½½**: ä»465æå‡åˆ°5K ops/sec (11xæ”¹è¿›)
- âœ… **è°ƒåº¦å»¶è¿Ÿ**: < 10Î¼s (P99)
- âœ… **å†…å­˜æ•ˆç‡**: æ¯ä»»åŠ¡å¼€é”€ < 64 bytes

### ç¨³å®šæ€§ (æƒé‡: 20%)
- âœ… 24å°æ—¶å‹åŠ›æµ‹è¯•æ— å´©æºƒ
- âœ… å†…å­˜æ³„æ¼æ£€æµ‹é€šè¿‡
- âœ… ç«æ€æ¡ä»¶æ£€æµ‹é€šè¿‡
- âœ… åœ¨é«˜è´Ÿè½½ä¸‹ä¿æŒç¨³å®šæ€§èƒ½

### å…¼å®¹æ€§ (æƒé‡: 10%)
- âœ… Linux (x86_64, aarch64)
- âœ… macOS (x86_64, Apple Silicon)
- âœ… Windows (x86_64)
- âœ… ä¸libxevæ”¯æŒçš„æ‰€æœ‰å¹³å°å…¼å®¹

### æ˜“ç”¨æ€§ (æƒé‡: 10%)
- âœ… APIè®¾è®¡ç®€æ´ç›´è§‚
- âœ… å®Œæ•´çš„æ–‡æ¡£å’Œç¤ºä¾‹
- âœ… è‰¯å¥½çš„é”™è¯¯ä¿¡æ¯
- âœ… é›¶é…ç½®å¼€ç®±å³ç”¨

## ğŸš€ é¢„æœŸæˆæœ

### æŠ€æœ¯æˆæœ
1. **çœŸæ­£çš„å¼‚æ­¥è¿è¡Œæ—¶**: åŸºäºlibxevçš„é«˜æ€§èƒ½å¼‚æ­¥è¿è¡Œæ—¶
2. **Tokioçº§åˆ«æ€§èƒ½**: åœ¨å…³é”®æŒ‡æ ‡ä¸Šè¾¾åˆ°æˆ–è¶…è¿‡Tokioæ€§èƒ½
3. **Zigç”Ÿæ€è´¡çŒ®**: ä¸ºZigç”Ÿæ€æä¾›é«˜è´¨é‡çš„å¼‚æ­¥è¿è¡Œæ—¶åº“
4. **è·¨å¹³å°æ”¯æŒ**: ç»Ÿä¸€çš„APIæ”¯æŒæ‰€æœ‰ä¸»æµå¹³å°

### æ€§èƒ½æˆæœ
1. **æ˜¾è‘—æ€§èƒ½æå‡**: åœ¨æ‰€æœ‰æµ‹è¯•åœºæ™¯ä¸­å®ç°ç›®æ ‡æ€§èƒ½
2. **å†…å­˜æ•ˆç‡**: æ¯”ç°æœ‰å®ç°æ›´ä½çš„å†…å­˜å¼€é”€
3. **å»¶è¿Ÿä¼˜åŒ–**: æ›´ä½çš„ä»»åŠ¡è°ƒåº¦å»¶è¿Ÿ
4. **ååé‡æå‡**: æ›´é«˜çš„å¹¶å‘å¤„ç†èƒ½åŠ›

### ç”Ÿæ€æˆæœ
1. **å¼€æºè´¡çŒ®**: é«˜è´¨é‡çš„å¼€æºå¼‚æ­¥è¿è¡Œæ—¶
2. **ç¤¾åŒºå»ºè®¾**: å¸å¼•æ›´å¤šå¼€å‘è€…å‚ä¸Zigå¼‚æ­¥ç”Ÿæ€
3. **æœ€ä½³å®è·µ**: ä¸ºZigå¼‚æ­¥ç¼–ç¨‹æä¾›å‚è€ƒå®ç°
4. **æŠ€æœ¯å½±å“**: æ¨åŠ¨Zigåœ¨æœåŠ¡å™¨ç«¯åº”ç”¨çš„é‡‡ç”¨

è¿™ä¸ªå®Œæ•´çš„æ”¹è¿›è®¡åˆ’å°†æŠŠZokioä»å½“å‰çš„"ä¼ªå¼‚æ­¥"å®ç°è½¬å˜ä¸ºçœŸæ­£çš„é«˜æ€§èƒ½å¼‚æ­¥è¿è¡Œæ—¶ï¼Œå®Œå…¨åŸºäºlibxevå®ç°è·¨å¹³å°I/Oï¼Œä¸Tokioåœ¨æ¶æ„å’Œæ€§èƒ½ä¸Šä¿æŒä¸€è‡´ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨Zigçš„ç¼–è¯‘æ—¶ç‰¹æ€§å®ç°æ›´å¥½çš„æ€§èƒ½å’Œæ›´ä½çš„èµ„æºæ¶ˆè€—ã€‚
